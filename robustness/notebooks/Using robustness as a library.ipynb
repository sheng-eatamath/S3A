{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barebones starter example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CIFAR\n",
    "import torch as ch\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CIFAR('/tmp/')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet18', dataset=ds)\n",
    "train_loader, val_loader = ds.make_loaders(batch_size=BATCH_SIZE, workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cox store for logging\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 5\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, CIFAR)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CIFAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crit = ch.nn.CrossEntropyLoss()\n",
    "def custom_train_loss(logits, targ):\n",
    "    probs = ch.ones_like(logits) * 0.5\n",
    "    logits_to_multiply = ch.bernoulli(probs) * 9 + 1\n",
    "    return train_crit(logits_to_multiply * logits, targ)\n",
    "\n",
    "adv_crit = ch.nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "def custom_adv_loss(model, inp, targ):\n",
    "    logits = model(inp)\n",
    "    probs = ch.ones_like(logits) * 0.5\n",
    "    logits_to_multiply = ch.bernoulli(probs) * 9 + 1\n",
    "    new_logits = logits_to_multiply * logits\n",
    "    return adv_crit(new_logits, targ), new_logits\n",
    "\n",
    "train_args.custom_train_loss = custom_train_loss\n",
    "train_args.custom_adv_loss = custom_adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LambdaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness.loaders import LambdaLoader\n",
    "\n",
    "def label_noiser(ims, labels):\n",
    "    label_noise = ch.randint_like(labels, high=9)\n",
    "    probs = ch.ones_like(label_noise) * 0.1\n",
    "    labels_to_noise = ch.bernoulli(probs.float()).long()\n",
    "    new_labels = (labels + label_noise * labels_to_noise) % 10\n",
    "    return ims, new_labels\n",
    "\n",
    "train_loader = LambdaLoader(train_loader, label_noiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TransformedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robustness.loaders import TransformedLoader\n",
    "from robustness.data_augmentation import TRAIN_TRANSFORMS_DEFAULT\n",
    "\n",
    "def make_rand_labels(ims, targs):\n",
    "    new_targs = ch.randint(0, high=10,size=targs.shape).long()\n",
    "    return ims, new_targs\n",
    "\n",
    "train_loader_transformed = TransformedLoader(train_loader,\n",
    "                                            make_rand_labels,\n",
    "                                            TRAIN_TRANSFORMS_DEFAULT(32),\n",
    "                                            workers=8,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            do_tqdm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom per-iteration logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SCHEMA = {'iteration': int, 'weight_norm': float }\n",
    "out_store.add_table('custom', CUSTOM_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import parameters_to_vector as flatten\n",
    "\n",
    "def log_norm(mod, it, loop_type, inp, targ):\n",
    "    if loop_type == 'train':\n",
    "        curr_params = flatten(mod.parameters())\n",
    "        log_info_custom = { 'iteration': it,\n",
    "                    'weight_norm': ch.norm(curr_params).detach().cpu().numpy() }\n",
    "        out_store['custom'].append_row(log_info_custom)\n",
    "    \n",
    "train_args.iteration_hook = log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from robustness.model_utils import make_and_restore_model\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # Must implement the num_classes argument\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 1000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        out = x.view(x.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        return self.fc2(out)\n",
    "\n",
    "new_model = MLP(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model, _ = make_and_restore_model(arch=new_model, dataset=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train_model(train_args, new_model, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
