{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f07577-50c4-4bef-9a42-f86bc4b512a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. specify `sys.path.append` as your project directory path\n",
    "\n",
    "2. change `Config.dataset` as different dataset names\n",
    "\n",
    "3. set `Config.arch='ViT-L/14'` and `f_classifier='./cache/vocabulary_classifier_L.pth'` for ViT-L architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe6430f-7a2c-4f05-9a04-7420cfc4b089",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_vocab in21k\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sheng-eatamath/S3A/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import model as clip\n",
    "from model import tokenize\n",
    "from data.build_dataset import build_transform\n",
    "from data.imagenet_datasets import get_datasets_rzsc\n",
    "from data.vocab import get_vocab, Vocab\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:1'\n",
    "    arch = 'ViT-B/16'\n",
    "    ### dataset name\n",
    "    dataset = 'make_entity13'\n",
    "    n_sampled_classes = 100 ### set num of sampled classes for ImageNet-100\n",
    "    input_size = 224\n",
    "    batch_size = 256\n",
    "    clip_checkpoint = None ### whether to use clip checkpoint\n",
    "    f_classifier = './cache/vocabulary_classifier.pth' ### precomputed 21k CLIP vocabulary classifier\n",
    "    templates_name = 'templates' ### CLIP template file name\n",
    "    seed = 0\n",
    "    image_mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "    image_std = (0.26862954, 0.26130258, 0.27577711)\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b6c3e7-a7cf-4d0b-b4f6-2c8f2ae6daef",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch, device=args.device)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_classifier(args, model, templates, vocab_classnames, parent_classnames=None):\n",
    "    batch_size = 64\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        assert parent_classnames is None\n",
    "        with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "            for classname_set in np.array_split(vocab_classnames, len(vocab_classnames)//batch_size):\n",
    "                texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "                texts = tokenize(texts).to(args.device) #tokenize\n",
    "                class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "                class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "                class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "                class_embedding = class_embeddings.mean(dim=1)\n",
    "                class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "                zeroshot_weights.append(class_embedding.cpu())\n",
    "                pbar.update(1)\n",
    "    classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e211023f-4b6a-48e8-8a0d-6db74a3db202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 334718\n",
      "===load nonjit===\n",
      "missing keys:\n",
      "['visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']\n",
      "Model parameters: 150,408,193\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [03:21<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset = get_datasets_rzsc(args, vocab, is_train=True, transform=transform_val, seed=0)\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "print('dataset size', len(dataset))\n",
    "model = load_clip2(args)\n",
    "classifier = build_classifier(args, model, templates, vocab.classnames)\n",
    "torch.save(classifier, f'{args.f_classifier}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sssa",
   "language": "python",
   "name": "sssa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
