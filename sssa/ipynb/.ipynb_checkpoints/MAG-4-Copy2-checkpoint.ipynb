{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9697f3d5-2ba7-469c-a775-6eeb71e15a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets\n",
    "from data.vocab import get_vocab_with_classnames\n",
    "# from data.imagenet_datasets import get_datasets_oszsl\n",
    "from data.imagenet_datasets_namevocab import get_datasets_oszsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b1eee8-530e-4b7b-92b7-94b1a53b7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = 'classifier_3d'\n",
    "    vocabname = 'concat2' ### ['in21k', 'concat3', 'concat3+lvis']\n",
    "    \n",
    "    device = 'cuda:3'\n",
    "    arch = 'ViT-B/16'\n",
    "    \n",
    "    dataset = 'make_entity13'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = -1\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    f_classifier = './cache/classifier_3d-concat2.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7765e06d-49e7-4b0b-b525-16860baf5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "templates = load_templates(args)\n",
    "\n",
    "def load_clip(args):\n",
    "    model, preprocess = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model, preprocess\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "def load_mixture_clip(args, decay=1.0):\n",
    "    model1 = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model1.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model1.to(args.device).eval()\n",
    "    model2 = clip.load(args.arch)\n",
    "    model2.to(args.device).eval()\n",
    "    with torch.no_grad():\n",
    "        msd = model1.state_dict()\n",
    "        for k, ema_v in model2.state_dict().items():\n",
    "            # if needs_module:\n",
    "            #     k = 'module.' + k\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * decay + (1. - decay) * model_v)\n",
    "    return model2\n",
    "\n",
    "def topk_acc(all_pred_voc_topk, all_gt_voc):\n",
    "    acc = []\n",
    "    ### topK accuracy\n",
    "    for i in range(all_pred_voc_topk.size(1)):\n",
    "        vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "        for j in range(i+1):\n",
    "            vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "        print(f'k={i} acc={vec.float().mean()}')\n",
    "        acc.append(vec.float().mean().item())\n",
    "    return acc\n",
    "\n",
    "def semantic_acc(y_pred, y_true, metrics={}):\n",
    "    \"\"\" compute soft semantic acc for @y_pred and @y_true \"\"\"\n",
    "    assert len(metrics)>0\n",
    "    assert y_pred.size(0)==y_true.size(0)\n",
    "    scores = {m:[] for m in metrics.keys()}\n",
    "    with tqdm(total=y_pred.size(0)) as pbar:\n",
    "        for i in range(y_pred.size(0)):\n",
    "            syn_pred = mapping_vocidx_to_synsets(y_pred[i].item(), vocab)\n",
    "            syn_true = mapping_vocidx_to_synsets(y_true[i].item(), vocab)\n",
    "            pairs = list(itertools.product(range(len(syn_pred)), range(len(syn_true))))\n",
    "            for m_name, m in metrics.items():\n",
    "                scores[m_name].append( max([ m(syn_pred[p[0]], syn_true[p[1]]) for p in pairs ]) )\n",
    "            pbar.update(1)\n",
    "    for m_name in metrics.keys():\n",
    "        scores[m_name] = np.array(scores[m_name]).mean()\n",
    "    return scores\n",
    "    \n",
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e31e8a-5026-4a3f-bdaa-7c2b62707b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_vocab concat2\n",
      "get dataset make_entity13\n",
      "dataset size 334718\n",
      "missing keys:\n",
      "['visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']\n",
      "Model parameters: 150,408,193\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "vocab = get_vocab_with_classnames(args.vocabname)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "print('get dataset', args.dataset)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_val, seed=0)\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "print('dataset size', len(dataset))\n",
    "\n",
    "# model, preprocess = load_clip(args)\n",
    "model = load_clip2(args)\n",
    "\n",
    "mapping_vocidx_to_synsets = lambda anchor, vocab: list(filter(lambda x: (x.name().split('.')[1]=='n') and (x.name().split('.')[0]==vocab.mapping_idx_names[anchor]), wn.synsets( vocab.mapping_idx_names[anchor] )))\n",
    "# mapping_vocidx_to_synsets = lambda anchor, vocab: list(filter(lambda x: (x.name().split('.')[1]=='n') and (x.name().split('.')[0] in vocab.mapping_names_idx.keys()), wn.synsets( vocab.mapping_idx_names[anchor] )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8da2e-1cc6-41f6-8ca8-d2d320517c6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f663af-6f48-4314-b08b-77e9b888b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_classifier(args, model, templates, vocab_classnames, parent_classnames=None):\n",
    "    batch_size = 64\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        assert parent_classnames is None\n",
    "        with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "            for classname_set in np.array_split(vocab_classnames, len(vocab_classnames)//batch_size):\n",
    "                texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "                texts = tokenize(texts).to(args.device) #tokenize\n",
    "                class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "                class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "                class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "                class_embedding = class_embeddings.mean(dim=1)\n",
    "                class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "                zeroshot_weights.append(class_embedding.cpu())\n",
    "                pbar.update(1)\n",
    "        # else:\n",
    "        #     with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "        #         for classname_set, parentname_set in zip(\n",
    "        #             np.array_split(vocab_classnames, len(vocab_classnames)//batch_size),\n",
    "        #             np.array_split(parent_classnames, len(parent_classnames)//batch_size),\n",
    "        #         ):\n",
    "        #             texts = [template.format(classname)+f' A type of {pname}.' for classname, pname in zip(classname_set, parentname_set) for template in templates] #format with class\n",
    "        #             texts = tokenize(texts).to(args.device) #tokenize\n",
    "        #             class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "        #             class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "        #             class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "        #             class_embedding = class_embeddings.mean(dim=1)\n",
    "        #             class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "        #             zeroshot_weights.append(class_embedding.cpu())\n",
    "        #             pbar.update(1)\n",
    "    classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ff2189-0380-48f4-bb3f-8d819c274688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [01:01<00:00,  5.12it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = build_classifier(args, model, templates, vocab.classnames)\n",
    "torch.save(classifier, f'./cache/{args.exp}-{args.vocabname}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd907a29-a14e-41fc-81b8-7756f2eabc34",
   "metadata": {},
   "source": [
    "### performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48060709-150f-490c-8967-e15adfbdf404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### naive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58b595-ee52-4ec2-979e-270d85d6c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "all_pred_voc_topk = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_pred_voc_topk.append(prob.topk(k=5, dim=-1).indices.cpu())\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_pred_voc_topk = torch.cat(all_pred_voc_topk, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f947de4f-bd2c-4d9b-b55c-40692f45cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.33346137404441833\n",
      "n_missing=2\n"
     ]
    }
   ],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')\n",
    "n_missing = len(set(all_gt_voc.unique().numpy()) - set(all_pred_voc.unique().numpy()))\n",
    "print(f'n_missing={n_missing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384c09-940e-4fc4-b6cb-28cb36b3fbea",
   "metadata": {},
   "source": [
    "#### SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19b10c10-78de-45c9-b436-1bd3da399979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(args.input_size, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=torch.tensor(mean),\n",
    "        std=torch.tensor(std))\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=0)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d0db35-1f20-463c-954d-c375b31039c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    n_count = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count.append( selected.sum().item() )\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected, :]).ravel(), return_counts=True)\n",
    "        # counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    n_count = torch.tensor(n_count).cpu()\n",
    "    \n",
    "    # all_clu_pred = setdiff_assignment(all_clu_pred)\n",
    "    \n",
    "    all_clu_pred = all_clu_pred/(n_count.view(-1, 1) + 1e-20)\n",
    "    \n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc, return_results=False):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    inst_acc = (label_voc_kmeans==all_gt_voc).float().mean().item()\n",
    "    print('instance label acc::', inst_acc)\n",
    "    if return_results:\n",
    "        return label_voc_kmeans, res_ass, inst_acc\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, model, classifier, device, \n",
    "                             all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                            prob = classifier_selected[similarity.softmax(-1)]\n",
    "                            cluster_ind.append(prob.cpu().argmax(dim=-1))\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    if classifier_selected is not None:\n",
    "        cluster_ind_voc = classifier_selected[cluster_ind]\n",
    "    else:\n",
    "        cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, \n",
    "                             preextracted_vfeatures=None):\n",
    "    \"\"\" given vocab label set @label_voc_kmeans, \n",
    "    Args:\n",
    "        label_voc_kmeans: cluster-assigned label on vocab\n",
    "        ...\n",
    "        preextracted_vfeatures: np.array([N x D])\n",
    "    Returns:\n",
    "        cluster_ind: tensor([N]): re-ordered cluster assignment\n",
    "        cluster_ind_voc: tensor([N]): cluster assignment indiced by vocab\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device).unique()\n",
    "    cluster_ind = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        if preextracted_vfeatures is not None:\n",
    "            N = len(loader_f.dataset)\n",
    "            batch_size = min(10000, N)\n",
    "            indices = np.array_split(np.arange(N), N//batch_size)\n",
    "            with torch.no_grad():\n",
    "                for group in indices:\n",
    "                    logits = torch.from_numpy(preextracted_vfeatures[group]).float()\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits@classifier.t().cpu()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    cluster_ind.append(prob[:, label_voc_kmeans.cpu()].argmax(dim=-1))\n",
    "        else:\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if preextracted_vfeatures is not None:\n",
    "                            logits = torch.from_numpy(preextracted_vfeatures[idx_img.cpu().numpy()]).float().to(device)\n",
    "                        else:\n",
    "                            logits = model.ema.extract_vfeatures(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        similarity = 100 * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "    cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def computation_reassign_by_pred_cluster(row, idx, args, model, classifier, candidate_classifier_ind):\n",
    "    \"\"\"\n",
    "    candidate_classifier_ind = label_voc_kmeans.unique().to(args.device)\n",
    "    \"\"\"\n",
    "    images, label_voc, label_clu, idx_img = row[:4]\n",
    "    images = images.to(args.device)\n",
    "    with amp_autocast():\n",
    "        vfeatures = model.visual(images).float()\n",
    "        # vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "    vfeatures = F.normalize(vfeatures, dim=-1)\n",
    "    batch_sim = 100*vfeatures@classifier[candidate_classifier_ind].t()\n",
    "    cluster_ind = batch_sim.argmax(dim=-1)\n",
    "    cluster_ind_voc = candidate_classifier_ind[cluster_ind].cpu()\n",
    "    return cluster_ind_voc\n",
    "\n",
    "def aggregation_reassign_by_pred_cluster(r, candidate_classifier_ind):\n",
    "    cluster_ind_voc = torch.cat(r, dim=0)\n",
    "    mapping_ind = dict(zip(cluster_ind_voc.unique().numpy(), torch.arange(cluster_ind_voc.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind_voc])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vfeatures(model, data_loader, device):\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(data_loader)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(data_loader):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(device)\n",
    "            with amp_autocast():\n",
    "                vfeatures = model.visual(images).float()\n",
    "            vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "            all_vfeatures.append(vfeatures.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    return all_vfeatures\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def loop_row_collect_results_nograd(obj_iter, computations={}, aggregations={}):\n",
    "    \"\"\" compute and aggregate results, looping over @obj_iter \n",
    "    func_computation(@row, @index_row)\n",
    "    aggregations(list(@results_computation))\n",
    "    \"\"\"\n",
    "    assert set(list(computations.keys())) == set(list(aggregations.keys()))\n",
    "    collector = { k:[] for k in computations }\n",
    "    with tqdm(total=len(obj_iter)) as pbar:\n",
    "        for i, row in enumerate(obj_iter):\n",
    "            ### apply computations\n",
    "            for k, func in computations.items():\n",
    "                collector[k].append(func(row, i))\n",
    "            pbar.update(1)\n",
    "    ### aggregate results\n",
    "    results = {}\n",
    "    for k, func_agg in aggregations.items():\n",
    "        results[k] = func_agg(collector[k])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da71399a-710f-449b-8849-f74ab4eda18d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [10:20<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 1\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                # logits = model.extract_vfeatures(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd664e9d-4102-447a-aaa0-4675aa1465d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_by_pred_cluster\n",
      "is mutex assignment:: False\n",
      "assignment collision num:: 26\n",
      "linear_assign\n",
      "assignment shape=(260, 20079)\n",
      "instance label acc:: 0.3285631537437439\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/654 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 133\n",
      "cluster acc 0.7108013312699049\n",
      "missing label:: 133\n",
      "iou voc:: 0.3231552162849873\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(260, 20079)\n",
      "instance label acc:: 0.40483033657073975\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/654 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 133\n",
      "cluster acc 0.71250126972556\n",
      "missing label:: 133\n",
      "iou voc:: 0.3231552162849873\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(260, 20079)\n",
      "instance label acc:: 0.4046749770641327\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/654 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 133\n",
      "cluster acc 0.71250126972556\n",
      "missing label:: 133\n",
      "iou voc:: 0.3231552162849873\n"
     ]
    }
   ],
   "source": [
    "# pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset}-train-clip.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/pred_kmeans_t.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/pred_kmeans_t.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "    n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "    iou_voc = n_inter/n_union\n",
    "    n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "    print('missing label::', n_missing_label)\n",
    "    print('iou voc::', iou_voc)\n",
    "    history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8d22636-fcf5-44a7-8701-9799446415b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'all_clu_pred': all_clu_pred,\n",
    "    'label_voc_kmeans': label_voc_kmeans,\n",
    "    'pred_kmeans_t': pred_kmeans_t,\n",
    "    'record_pred_kmeans_t': record_pred_kmeans_t,\n",
    "    'all_gt_voc': all_gt_voc,\n",
    "    'all_label_clu': all_label_clu,\n",
    "    'all_topk_voc': all_topk_voc,\n",
    "    'cluster_ind_voc': cluster_ind_voc,\n",
    "    'all_vfeatures': torch.from_numpy(all_vfeatures),\n",
    "}, f'./cache/scd/{args.exp}-{args.vocabname}-{args.dataset}-scd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a2a1b8-bf4a-4562-a114-066223571a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-scd.pth', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599b73d-bc0d-4fd9-bb1e-430123b880a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Multi Agent Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe60588-c815-4ea5-a6b7-2ee8667dcae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}, verbose=False):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(completion)\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    return result\n",
    "\n",
    "def openai_chatgpt_post_multirounds(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=content,\n",
    "    **parameters,\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "def save_results(res, fpath='test.pkl'):\n",
    "    with open(f'./cache/openai/MAG/{fpath}', 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "    return \n",
    "\n",
    "def load_results(fpath='test.pkl'):\n",
    "    with open(f'./cache/openai/MAG/{fpath}', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ed84f6-45a1-4d29-8038-9dbf8f1a232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@3 = 0.6653845906257629\n"
     ]
    }
   ],
   "source": [
    "k_1 = 3\n",
    "\n",
    "def generate_concepts(record_pred_kmeans_t, all_gt_voc, k_1=3):\n",
    "    all_clu_gt_voc = []\n",
    "    for c in record_pred_kmeans_t.unique():\n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "    all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "    topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "    cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "    for i in range(k_1):\n",
    "        cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "    print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')\n",
    "\n",
    "    \"\"\" gather concepts \"\"\"\n",
    "    to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "    cluster_row_synsets = []\n",
    "    for row in topk_all_clu_pred:\n",
    "        row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "        cluster_row_synsets.append(row_synsets)\n",
    "    return cluster_row_synsets, topk_all_clu_pred\n",
    "\n",
    "cluster_row_synsets, topk_all_clu_pred = generate_concepts(record_pred_kmeans_t, all_gt_voc, k_1=k_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8316f37-a85c-4266-92c0-285cad37cb2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "def format_concept_request_with_def(cluster_row_synsets):\n",
    "    concept_request = []\n",
    "    for row in cluster_row_synsets:\n",
    "        ccpts = reduce(lambda x, y: x+y, row)\n",
    "        names = list(map(lambda x: \"'\"+x.split(':')[0]+\"'\", ccpts))\n",
    "        ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "        ccpts = ', '.join(ccpts)\n",
    "        concept_request.append((', '.join(names), ccpts))\n",
    "    return concept_request\n",
    "\n",
    "def format_concept_request(cluster_row_synsets):\n",
    "    concept_request = []\n",
    "    for row in cluster_row_synsets:\n",
    "        row_names = []\n",
    "        row_names = list(map(lambda x: x[0].split('.')[0], row))\n",
    "        concept_request.append((', '.join(row_names), None))\n",
    "    return concept_request\n",
    "\n",
    "\n",
    "def clean_round_1(all_chatgpt_res):\n",
    "    invalid_inds = []\n",
    "    clean_all_chatgpt_res = [[] for _ in range(len(all_chatgpt_res))]\n",
    "    for i in range(len(all_chatgpt_res)):\n",
    "        for j, row in enumerate(all_chatgpt_res[i]):\n",
    "            lines = row.split('\\n')\n",
    "            if len(lines)<10:\n",
    "                invalid_inds.append((i,j))\n",
    "            for l in lines[:10]:\n",
    "                re_match_res = re.match('[0-9]{1,2}\\..*', l)\n",
    "                if re_match_res is None:\n",
    "                    invalid_inds.append((i,j))\n",
    "            clean_all_chatgpt_res[i].append(lines[:10])\n",
    "    invalid_inds = list(set(invalid_inds))\n",
    "    return clean_all_chatgpt_res, invalid_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308ab485-053f-4d13-a6f3-754025a22db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept_request = format_concept_request_with_def(cluster_row_synsets)\n",
    "concept_request = format_concept_request(cluster_row_synsets)\n",
    "n_repeat = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df8f85-31bb-4018-b509-aebb7cf80c41",
   "metadata": {},
   "source": [
    "#### round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9e03b02-c725-45fb-94b7-f72f780ce32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_round_1_with_def = lambda concepts, concepts_with_def: \"Let's play a game. You are given three category names (\" + concepts_with_def + \"). GOAL: to visually discriminate \" + concepts + \". Please ask ten questions to distinguish which category is presented in an imaginary image. Rule: you can only ask about their visual appearance, visual features, or visual characteristics. Please ask all questions at once and list each in a row sequentially.\"\n",
    "template_round_1 = lambda concepts, concepts_with_def: \"Let's play a game. You are given three category names (\" + concepts + \"). GOAL: to visually discriminate \" + concepts + \". Please ask ten questions to distinguish which category is presented in an imaginary image. Rule: you can only ask about their visual appearance, visual features, or visual characteristics. Please ask all questions at once and list each in a row sequentially.\"\n",
    "\n",
    "template_in_use = template_round_1\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(*row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0835d683-0e52-406a-8378-4d02c4855cd7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 149/780 [15:09<1:01:35,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4f898650511f0ccdc9b1c775d541a184 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 176/780 [18:30<1:03:27,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d930d5f3275470900a254eb70fa91917 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 180/780 [19:25<1:31:20,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7d2c5933dc38163e846226b976e0d669 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 190/780 [21:07<1:21:16,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 455953797d69d3ef5530956784c32d7f in your message.)\n",
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 07:56:50 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d68bfc4288ed2a7-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 239/780 [32:14<55:18,  6.13s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP code 502 from API (<html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 254/780 [33:59<1:02:12,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1ce97388b598898f37c8d0874915f6ce in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 286/780 [37:59<47:54,  5.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 835a23fe20db41cb24a31d8485ba8030 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 446/780 [55:35<33:52,  6.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7a2b7eda611e7b13016a5da7a3734c38 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 530/780 [1:05:15<26:17,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d99bf9c532b2cc331a95d84c92fea4aa in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 533/780 [1:06:04<44:30, 10.81s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4a14e352f247c4cd147d2d9a1051fb43 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 614/780 [1:15:05<18:27,  6.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 30a071ea69fd2702e82b90b750e89837 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 637/780 [1:18:02<14:49,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6b144562323dc351b5ebac8a34389fde in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 655/780 [1:20:29<14:27,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fa6b3a604ee569420cca18b40f4f0860 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 770/780 [1:33:28<01:01,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 57d358bbfcd98f10c8b8a9bbdee02cab in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [1:35:07<00:00,  7.32s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2404c5c9-7ad9-4568-8a90-c73dcfa27c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res, fpath=f'{args.dataset}-round=1-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d766bfbb-588e-4b62-ae24-ecf6b0fc1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chatgpt_res = load_results(f'{args.dataset}-round=1-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd69fc8-13c2-4f78-a4dc-3e4d0fae54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### repair_r1\n",
    "while 1:\n",
    "    all_chatgpt_res_clean, invalid_inds = clean_round_1(all_chatgpt_res)\n",
    "    if len(invalid_inds)==0:\n",
    "        break\n",
    "    else:\n",
    "        for item in invalid_inds:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[item[0]][item[1]] = openai_chatgpt_post(concept_templates[item[1]])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \n",
    "all_chatgpt_res = all_chatgpt_res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c54a95ef-e872-4bee-a6e7-e6d6eaf71f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res, fpath=f'{args.dataset}-round=1-no_def.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bcf55-f5ba-4f23-9538-89907b677abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ec1c9b-777d-4ac6-bd68-4ad72bcd7415",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 38/780 [09:58<3:17:09, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 61afe89402a5139230ae983340176161 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 47/780 [12:56<3:35:40, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1e9bdf96bcbef7c0bda19d79d5b55a37 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 86/780 [23:23<3:01:18, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 09:33:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d694db3feddd2a7-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 108/780 [34:17<2:37:12, 14.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d45ef43f33deee8c4a9aee06601a06a5 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 112/780 [35:45<3:13:28, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9e641716449aeef9ca4b3610400d4a75 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 258/780 [1:13:15<1:58:58, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 06fa5f9e4ca5d074ed636d5a99a28abe in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 314/780 [1:26:42<1:51:07, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f195f8cfa19882e9d3d1a212fe574031 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 329/780 [1:30:42<2:01:13, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1fb53023f3d1b5d3006cb686678e6242 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 332/780 [1:32:07<2:48:34, 22.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c79d706dc520a4013eb3d687c0267902 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 410/780 [1:51:51<2:04:05, 20.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4f9fc94a97409605cca1b9d9867d226d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 417/780 [1:54:43<2:04:51, 20.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f43b60875f5bd56ce09c32d99b7bfc4b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 435/780 [1:59:54<1:40:20, 17.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2dd3ebe87230b620e75f8f864468fca5 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 471/780 [2:10:14<1:30:13, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f36db77e8b1d303dedc62efc5121d247 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 652/780 [3:01:25<38:53, 18.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b77e389fcb22dbdb19897dda272e6c10 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 654/780 [3:02:25<49:44, 23.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8be901323e46c5f4e429ebe08c2541f6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 705/780 [3:17:36<22:34, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e9a86172e3485e76caca90e602fbb027 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 759/780 [3:33:38<04:31, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 076b1abf4723b3b31e534ad807d22d72 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [3:38:39<00:00, 16.82s/it]\n"
     ]
    }
   ],
   "source": [
    "template_round_2_with_def = lambda concepts, concepts_with_def: \"Let's play a game. GOAL: to visually discriminate \" + concepts + \". You are given three category names with definitions (\" + concepts_with_def + \"). I will give you a number of questions. Please answer these questions concisely and accurately for each category. Imagine you are given an imagenery image. For each category name, please answer all questions at once and list each in a row sequentially. I will give you the questions now.\"\n",
    "template_round_2 = lambda concepts, concepts_with_def: \"Let's play a game. GOAL: to visually discriminate \" + concepts + \". You are given three category names (\" + concepts + \"). I will give you a number of questions. Please answer these questions concisely and accurately for each category. Imagine you are given an imagenery image. For each category name, please answer all questions at once and list each in a row sequentially. I will give you the questions now.\"\n",
    "\n",
    "template_in_use_r2 = template_round_2\n",
    "concept_templates_r2 = [[] for _ in range(n_repeat)]\n",
    "all_chatgpt_res_r2 = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=n_repeat*len(concept_request)) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            ### prepare template\n",
    "            content = \\\n",
    "                [\n",
    "                    {'role': 'user', 'content': template_in_use_r2(*row)},\n",
    "                    {'role': 'system', 'content': \"Sure, I'm ready to play the game. Please go ahead and provide me with the questions\"},\n",
    "                    {'role': 'user', 'content': '\\n'.join(all_chatgpt_res[i][j]) + 'Please mention the category name before your listed answers.'}\n",
    "                ]\n",
    "            concept_templates_r2[i].append(content)\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### collect result\n",
    "                    all_chatgpt_res_r2[i].append(openai_chatgpt_post_multirounds(content)[\"choices\"][0].message.content)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            pbar.update(1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de53bd65-9c63-491a-b5ff-7e3c7deb4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r2, fpath=f'{args.dataset}-round=2-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07d2a85f-931b-4b99-9b21-f28bc44346b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chatgpt_res = np.array(all_chatgpt_res).reshape(3, -1, 10).tolist()\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    for j, row in enumerate(concept_request):\n",
    "        all_chatgpt_res[i][j] = '\\n'.join(all_chatgpt_res[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c6aff9-b322-4cc8-9dc3-221e9b113fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:18<?, ?it/s]\n",
      "  0%|          | 0/3 [00:14<?, ?it/s]\n",
      "  0%|          | 0/3 [00:15<?, ?it/s]\n",
      "  0%|          | 0/3 [00:24<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" check: missing concepts \"\"\"\n",
    "while 1:\n",
    "    ### check validity\n",
    "    invalid_inds = []\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            concepts = list(map(lambda x: x.strip('\\''), concept_request[j][0].split(', ')))\n",
    "            answers = all_chatgpt_res_r2[i][j].split('\\n\\n')[-len(concepts):]\n",
    "            if len(answers) not in [len(concepts), len(concepts)+1]:\n",
    "                invalid_inds.append((i,j))\n",
    "            # for k in range(len(concepts)):\n",
    "                # concepts[k] == answers[k][:len(concepts[k])]\n",
    "    ### request\n",
    "    if len(invalid_inds)==0:\n",
    "        break\n",
    "    with tqdm(total=len(invalid_inds)) as pbar:\n",
    "        for ind in invalid_inds:\n",
    "            row = concept_request[ind[1]]\n",
    "            ### prepare template\n",
    "            content = \\\n",
    "                [\n",
    "                    {'role': 'user', 'content': template_in_use_r2(*row)},\n",
    "                    {'role': 'system', 'content': \"Sure, I'm ready to play the game. Please go ahead and provide me with the questions\"},\n",
    "                    {'role': 'user', 'content': all_chatgpt_res[ind[0]][ind[1]] + 'Please mention the category name before your listed answers.'}\n",
    "                ]\n",
    "            concept_templates_r2[ind[0]][ind[1]] = content\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### collect result\n",
    "                    all_chatgpt_res_r2[ind[0]][ind[1]] = openai_chatgpt_post_multirounds(content)[\"choices\"][0].message.content\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64dc7464-2e7e-46a2-807e-450ea3450a16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:18<00:00, 14.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:35<00:00, 11.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:09<00:19,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 49550b1d4ba4e8efb5d15d1ba9973666 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:18<00:00, 13.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:42<00:00,  8.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:32<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:34<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:22<00:17,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 69de17d4638f63d225f43a84624708c6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:01<00:00, 12.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:44<00:00,  8.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:35<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:29<00:00,  7.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:30<00:00,  7.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:21<00:00,  5.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:24<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:29<00:00,  9.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:14<00:00,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:24<00:00,  8.32s/it]\n",
      "100%|██████████| 3/3 [00:18<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:09<00:09,  9.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4de002474d376034c5560dd0dea46dd2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:54<00:00, 27.00s/it]\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.58s/it]\n",
      "100%|██████████| 2/2 [00:15<00:00,  7.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:18<00:00,  9.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.16s/it]\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:17<00:00,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c1a11f31d11f4dbb9037c7eb7e6b4ed4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:47<00:00, 23.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.13s/it]\n",
      "100%|██████████| 2/2 [00:15<00:00,  7.88s/it]\n",
      "100%|██████████| 2/2 [00:17<00:00,  8.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:18<00:00,  9.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:13<00:00,  6.84s/it]\n",
      "100%|██████████| 2/2 [00:20<00:00, 10.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:18<00:00,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 86ecbe5569fccb8fd5fe537c67407051 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:58<00:00, 29.28s/it]\n",
      "100%|██████████| 2/2 [00:20<00:00, 10.30s/it]\n",
      "100%|██████████| 2/2 [00:19<00:00,  9.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:21<00:00, 10.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.47s/it]\n",
      "100%|██████████| 2/2 [00:16<00:00,  8.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.07s/it]\n",
      "100%|██████████| 2/2 [00:17<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:20<00:00, 10.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# template_round_2_with_def = lambda concepts, concepts_with_def: \"Let's play a game. GOAL: to visually discriminate \" + concepts + \". You are given three category names with definitions (\" + concepts_with_def + \"). I will give you a number of questions. Please answer all these questions concisely and accurately for each category based on your knowledge. Imagine you are given an imagenery image. For each category name, please answer all questions at once and list each in a row sequentially. I will give you the questions now.\"\n",
    "\"\"\" check: missing answer \"\"\"\n",
    "i_iter = 0\n",
    "while 1:\n",
    "    all_qa_pairs = [[] for _ in range(n_repeat)] ### N x R x C x P\n",
    "    invalid_inds = []\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            concepts = list(map(lambda x: x.strip('\\''), concept_request[j][0].split(', ')))\n",
    "            answers = all_chatgpt_res_r2[i][j].split('\\n\\n')[-len(concepts):]\n",
    "            answers = answers[-len(concepts): ]\n",
    "            names = [item.strip(\"'\") for item in row[0].split(', ')]\n",
    "            names_def = [item.strip(\"'\") for item in row[1].split(', ')] if row[1] is not None else [None]*len(row[0].split(', '))\n",
    "\n",
    "            qa_pairs = []\n",
    "            q = [' '.join(item.split(' ')[1:]) for item in all_chatgpt_res[i][j].split('\\n')]\n",
    "            for k in range(len(concepts)):\n",
    "                extract_lines = lambda x: list(filter(lambda y: len(y), x.split('\\n')))\n",
    "                extract_ans = lambda x: ' '.join(x.split(' ')[1:])\n",
    "                try:\n",
    "                    a = [extract_ans(item) for item in extract_lines(answers[k])[1:]]\n",
    "                    qa_pairs.append([names[k], names_def[k], q, a])\n",
    "                    if len(q)!=len(a):\n",
    "                        invalid_inds.append((i, j))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    invalid_inds.append((i, j))\n",
    "            all_qa_pairs[i].append(qa_pairs)\n",
    "    invalid_inds = list(set(invalid_inds))\n",
    "    if len(invalid_inds) == 0:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=len(invalid_inds)) as pbar:\n",
    "        for ind in invalid_inds:\n",
    "            row = concept_request[ind[1]]\n",
    "            ### prepare template\n",
    "            content = \\\n",
    "                [\n",
    "                    {'role': 'user', 'content': template_in_use_r2(*row)},\n",
    "                    {'role': 'system', 'content': \"Sure, I'm ready to play the game. Please go ahead and provide me with the questions\"},\n",
    "                    {'role': 'user', 'content': all_chatgpt_res[ind[0]][ind[1]] + 'Please mention the category name before your listed answers.'}\n",
    "                ]\n",
    "            ### update template\n",
    "            concept_templates_r2[ind[0]][ind[1]] = content\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### update result\n",
    "                    all_chatgpt_res_r2[ind[0]][ind[1]] = openai_chatgpt_post_multirounds(content)[\"choices\"][0].message.content\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            pbar.update(1)\n",
    "            \n",
    "    if i_iter>50:\n",
    "        for ind in invalid_inds:\n",
    "            key1 = random.choice(range(n_repeat))\n",
    "            all_chatgpt_res_r2[ind[0]][ind[1]] = all_chatgpt_res_r2[key1][ind[1]]\n",
    "            \n",
    "    i_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e516ab86-6169-4e1a-838e-05c8134e1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r2, fpath=f'{args.dataset}-round=2-no_def.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24c159-c1ff-4539-a689-add676bccb36",
   "metadata": {},
   "source": [
    "#### round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70baf483-6131-4224-abad-0d4fd4eafd5d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/23400 [00:35<8:19:31,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3069d289b3ee3b20bc845907fa54d0bb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 85/23400 [02:26<8:34:52,  1.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 551ecc0dde8d72fda05596bb672c2c78 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 341/23400 [08:29<7:00:48,  1.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6bba61bac92619ce20fbb2ed9ad7cc71 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 376/23400 [09:45<9:28:37,  1.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID afac762ec7a42bb85aab4fc27028bab2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 437/23400 [11:33<8:05:56,  1.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 41c1236719e4a683585dbc2f3bf648b9 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 450/23400 [12:19<8:55:46,  1.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b096759bc39d9e962dbe8e28b7e92f05 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 530/23400 [14:34<8:05:11,  1.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 27bc3065fcff49e132ae990bf84fe699 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 636/23400 [17:36<8:21:38,  1.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fbc83a1bc1481365b43258fc21103cc0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 666/23400 [18:49<8:12:18,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 250b1d8ce96cf7fe82aba10935c59b4e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 667/23400 [19:20<64:56:45, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d6d44cda3ef888d7f9a67babc9c4b155 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 776/23400 [22:14<9:35:43,  1.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bbe90be3009ac8c3e01071ddf333bd84 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 800/23400 [23:16<8:35:06,  1.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3c8a3ac06963090c48f5ad2b9168ea66 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 848/23400 [24:45<7:22:55,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID da7a4504aceaff51f762939997a37cd4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 864/23400 [25:40<10:49:02,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f410f36bb27757a980e0cf5fefda5694 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 876/23400 [26:26<8:09:36,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ce8d99795594b18e8bf1016089bd49a6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 962/23400 [28:44<7:37:12,  1.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID da9212dd2de515fe33ce3796bf6321d7 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1039/23400 [30:51<9:58:31,  1.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7143ec7088b0b3a8dd998800759126d1 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1040/23400 [31:22<65:56:01, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 46f5839b16b90b3b8eb3092d3064831e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1044/23400 [32:09<49:43:38,  8.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f4569f7cd1ea697b2a9fee736737d8d5 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1056/23400 [32:57<9:51:24,  1.59s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b5f7bf748959c87d23dba3d22fe64354 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1090/23400 [34:15<10:17:41,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ebc451cde967b380c497d2d59a636fee in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1096/23400 [34:53<18:09:08,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a441ef8c5e6691cb8ceefa0e34ec1529 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1122/23400 [36:09<8:32:47,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e905d11a05d004c12491ea4b9e7d97fb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1161/23400 [37:33<7:37:01,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 47082b5c874d4197a983e5e1bc692b9b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1185/23400 [38:34<6:57:05,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c7875558ef112ad4c2968f780e581c74 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1219/23400 [39:53<7:52:15,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6458667d2216b322c72e7380c475cb9 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1307/23400 [42:25<9:42:03,  1.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c541be64c6545844935ab846bd1880e8 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1310/23400 [43:00<37:03:41,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c84c813954f425f0c2bd8b898cb6b541 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1322/23400 [43:46<8:18:11,  1.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 609297943d88c91166b6e9284d54d070 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1328/23400 [44:26<19:09:04,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dc8074879a9f4f4dafa062727479022f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1341/23400 [45:14<9:59:36,  1.63s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 69baeb635b0027b9e9863926f58704d5 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1351/23400 [45:59<10:49:28,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0b8c596a25b5da2ad27d2f405283e749 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1367/23400 [46:50<7:55:49,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID be40e9541c142c56bf9846dd8ceacbfa in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1452/23400 [49:12<6:18:16,  1.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bf3e18d3cc8c9864b6667bd5518bfb2b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1491/23400 [50:30<7:27:08,  1.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID da7622923cbd40623112659e280bb443 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1597/23400 [53:19<6:39:03,  1.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b4e04c405846600690b478e8a7ac7d1d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1618/23400 [54:12<7:07:07,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 15c809ca9c32a7c51c87e2d4d24ab510 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1619/23400 [54:43<62:03:06, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bdb8a71b2f31ee12d2ef738525c51301 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1637/23400 [55:37<8:43:04,  1.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c8b879f34f4ca29100e065287d9e93c2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1661/23400 [56:37<6:55:52,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 121d71e269f6b6f6ac3baba2123cd6e9 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1891/23400 [1:02:14<7:29:48,  1.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d7a1051f4bfb4332fc62350a2951f80b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1912/23400 [1:03:07<6:23:31,  1.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dac9fbb0110e74cc5856d562d4e4c24a in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1921/23400 [1:03:49<10:37:39,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6f1f4ceb460fe0b42dc3a51ed37ecd31 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1936/23400 [1:04:36<8:02:18,  1.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9baaded8a75fca07def7120927b29df6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2068/23400 [1:07:44<7:00:16,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 56047d331f1a392e4f393bc96d1811e0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2096/23400 [1:08:51<8:09:07,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cd0894614249be0fe7d9c326388ea3a in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2158/23400 [1:10:51<9:01:59,  1.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8000504db021dbe4112ef340482a5c98 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2193/23400 [1:12:14<8:23:44,  1.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e827157a76851a5b1378aafb966628c2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2240/23400 [1:13:47<7:44:46,  1.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a558af8f30c09b201648163a063425ce in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2245/23400 [1:14:24<21:11:32,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID cacfea1474f640b2557f6890e38a5725 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2326/23400 [1:16:43<8:44:21,  1.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d0fe20270a1330cdb68b4dee8163ffc9 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2347/23400 [1:17:43<6:51:00,  1.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 14:38:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6b0b84bf072280-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2367/23400 [1:23:43<8:39:17,  1.48s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID aa0b1a5ee05028a1bf9d22236d82adab in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2444/23400 [1:25:54<7:29:31,  1.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d9af2574d7560c1969de4b1369706e20 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2512/23400 [1:28:06<6:52:26,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 949dddc75fb6e3417bb963889249f1e4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2603/23400 [1:30:54<11:26:45,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5bc7acff8b38a757da1e8d7b4e6754ff in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2622/23400 [1:31:49<7:39:58,  1.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b4075c57ac9a555494445093b0966889 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2679/23400 [1:33:35<5:49:55,  1.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7fb23ac0f369d878886209971cca40ef in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2906/23400 [1:39:08<6:42:51,  1.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6ba0ef618a8a2de7932472b459aacb29 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2921/23400 [1:39:56<6:52:58,  1.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bea63efa2d8c02ace1a547f99f825184 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3044/23400 [1:42:58<6:47:46,  1.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b769e49f3356110ed7995c8cd1cb9353 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3055/23400 [1:43:44<9:24:02,  1.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 961d7ee6cb5df3dfae0b4b450e9cf077 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3080/23400 [1:44:47<7:41:19,  1.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 21ff179c8a2a8414d29a530f953713df in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3175/23400 [1:47:31<9:26:04,  1.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 239c24105940b4baa3e5720b0cf49e3f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3247/23400 [1:50:00<6:46:06,  1.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fd0ea1a04c3b54d5e745102d58c94761 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3281/23400 [1:51:18<9:32:04,  1.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 07359287a6043ea19f8c06001cb5aaa2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3283/23400 [1:51:51<43:51:27,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 80da075141d3040cf44382f1864a8adb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3290/23400 [1:52:33<18:21:09,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID abdeeb416e23de202a7bc8daf5a41457 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3294/23400 [1:53:09<28:28:03,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ad30842b8eacb31fdf7110e5b83f23dd in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3349/23400 [1:54:55<7:18:08,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 22a72b6ad45bed5dad0f4431f8480e04 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3369/23400 [1:55:50<7:41:16,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f9144a83fc5fbcf29bfdbbd9405a1bed in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3411/23400 [1:57:17<7:25:13,  1.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 75129e06da3b231fba741c10c539092b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3429/23400 [1:58:14<8:44:13,  1.57s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3caff1a350bed2a1c1500f3afefa7fdb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3430/23400 [1:58:46<58:59:37, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8cb5bbc69f4a30fb5d972af06896cfc8 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3448/23400 [1:59:43<9:12:20,  1.66s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b2caefcbfce8aa43ca2b57eecc988a1a in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3468/23400 [2:00:38<6:01:25,  1.09s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bc299da7a6c1dfa6e40bc38dad26d3f6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 3474/23400 [2:01:16<14:37:39,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8663cd4af94462c99add5433ee148c7c in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3512/23400 [2:02:37<7:04:51,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID df3ba9150113255f9f7ee8e37cf60f96 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3529/23400 [2:03:29<6:49:12,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b7b24be8e9774caed40882edf882ae74 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3541/23400 [2:04:15<8:25:07,  1.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 77354a2cdc263cd02bf283fa392682a6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3597/23400 [2:05:52<8:00:48,  1.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5395adea188bb35372635376f44de67c in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3606/23400 [2:06:35<11:27:20,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2cb6cb92fb66e0fa1afa57b535f3043d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3780/23400 [2:11:45<6:22:28,  1.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4b7ea91258323b765c3dd169ce545f09 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 3845/23400 [2:13:41<7:56:32,  1.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 883c112243346eef1e06fbaab9f664b4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 3859/23400 [2:14:33<9:48:56,  1.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8b1a169ab26f6022eb524e9e9f150d15 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3867/23400 [2:15:17<13:18:44,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 876a61e5ecffe292392ab7c40f6f482f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3948/23400 [2:17:38<6:26:33,  1.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 20544799472f37a9adeefad10a43a045 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3951/23400 [2:18:11<30:09:44,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 15:38:27 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6b641bdd8dd536-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3955/23400 [2:23:27<185:19:42, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 13794832edc0040c695e9d492aed29c0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3966/23400 [2:24:11<11:35:24,  2.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 717ce5fe40b0f847b0b59e2b3f096e34 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4119/23400 [2:28:00<6:56:16,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 15:48:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6b72793dd7d536-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4207/23400 [2:35:14<6:10:28,  1.16s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 896e1d986fa90bb9b2ebb63362b42a02 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4245/23400 [2:36:37<7:37:28,  1.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 15:56:53 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6b7f17f8240413-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4261/23400 [2:42:11<8:55:26,  1.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fdc211674c7eeaa593d5073dafb4575e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4266/23400 [2:42:46<17:38:44,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 22b83c044a7aa2818b53442bed2f611d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4298/23400 [2:44:00<9:38:55,  1.82s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 15b62ee654f34f029fb8042b2ada4fe9 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 4330/23400 [2:45:14<7:19:58,  1.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9bdee59271aefb2720cb6ddd4b529096 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 4354/23400 [2:46:16<6:32:58,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 16:06:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6b8d3f0be70413-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4451/23400 [2:53:43<8:26:12,  1.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dfdd2fff08546c0a0ee075526539c356 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4457/23400 [2:54:21<15:15:51,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fddde56756c931f33bbd1f98ca1130cb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4481/23400 [2:55:26<6:49:47,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 25bc2596ad5b495a90339258454fb1c6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4523/23400 [2:56:56<6:52:24,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4c6874544bf396701d65e9c288336077 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4540/23400 [2:57:47<5:59:32,  1.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0591c6bfeed736266ec917ba939d9471 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 4606/23400 [2:59:47<6:33:18,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c644faae54e42927e4f9902168839d36 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4698/23400 [3:02:18<5:32:47,  1.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID da2963fcc4386f49c740c267550ddcd3 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4703/23400 [3:02:53<16:42:38,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 56bf18d7f8584c9dc1bb36329bf3d64c in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4709/23400 [3:03:30<15:11:28,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dfd3f5ca66239f6bbfe792490a2ab2c8 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4710/23400 [3:04:01<59:20:19, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 145d96bb9256f8c3258a14ff4c89bfda in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4714/23400 [3:04:37<35:51:35,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a594c7d155e16d1ed203b335fcb8ab8a in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4733/23400 [3:05:37<8:02:58,  1.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f158e1711abfc344c93d4cd9ff83bcb4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4754/23400 [3:06:35<5:46:21,  1.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b402f38d2265141a64807d9e16c8f718 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4770/23400 [3:07:27<8:43:05,  1.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f599db9824c4d274ca710caba0dfd653 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4774/23400 [3:08:02<22:34:53,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ed3e7d30df0a5ef764850e894dbfb1ab in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4793/23400 [3:09:02<7:59:30,  1.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c1566d3cdb602955003a9e55946b21c0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4849/23400 [3:10:43<5:28:39,  1.06s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 16:30:59 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6bb10bece10413-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4891/23400 [3:16:51<7:07:42,  1.39s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID adba724ee9859d0ab2b9c4739fe41df0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4931/23400 [3:18:18<5:39:42,  1.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1d1e6cfc41c9d512e54b97e8f6046095 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4939/23400 [3:18:59<10:21:14,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ec91bbb6cdd891bd8451eb33a0e7f599 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 5021/23400 [3:21:16<5:27:43,  1.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6a9aa9344309bf8c2aa654fab139bd29 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5092/23400 [3:23:32<6:06:22,  1.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 210c42521dfd930a45c27513e54038d3 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5105/23400 [3:24:21<9:30:13,  1.87s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 41eeefde75eefd69661cb4d18015d872 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5170/23400 [3:26:12<6:10:23,  1.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b18eb60b2083b197b789139a0e5e4a38 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5196/23400 [3:30:40<8:24:17,  1.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1b24ab6d42998639938dc6efd8a966e1 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5227/23400 [3:31:52<6:46:55,  1.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5338784010abc692d7c9a108a54b3bcf in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5231/23400 [3:32:28<22:09:36,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 98db59b3f0c113759894db504ee1887b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5267/23400 [3:33:54<8:40:15,  1.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5a880eb49ec6976f0da09a8e26b56a1f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5278/23400 [3:34:43<10:12:34,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID da0a932f24f200963f06df6d25f2a5d4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5292/23400 [3:35:35<7:32:56,  1.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 19579bb8730909522c6c480eace61354 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5326/23400 [3:36:53<8:03:35,  1.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ced852c9fab9b1f3034f907f5ee9057d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5406/23400 [3:39:13<6:21:26,  1.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 91d279190420bdaa775ad2f4ffcf7d6e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5434/23400 [3:40:30<6:30:00,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8a7d281d481edf47083fa7a122caa1e3 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5492/23400 [3:42:22<6:15:29,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c13e9398db1cf5b73654283a3e5d9105 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 5551/23400 [3:44:09<6:42:16,  1.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a26455d25db07109a07c5cb7082d8b69 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 5553/23400 [3:44:42<37:50:06,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 17:04:58 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6be2d4ed810214-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5596/23400 [3:50:52<7:37:32,  1.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 512ee2c498b9f299ff697554cc45c5fa in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5626/23400 [3:52:00<5:43:14,  1.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 95a5ed0ba91167b58f3eb8d8168bfa3b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5680/23400 [3:53:36<5:32:23,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 73768b8eead338c444235dd605de26bb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5815/23400 [3:57:05<6:10:41,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c5438ada23064f1c50ac45552c655a7e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6027/23400 [4:02:11<4:54:56,  1.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6037/23400 [4:12:22<39:57:15,  8.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fb333aeea87c33ba7827bcbb8712d300 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6038/23400 [4:12:54<73:33:53, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID fd85eb0dc8a5fbdb8fcccaf62c1612e5 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6064/23400 [4:13:55<6:10:39,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2d795a100d560a0f102d682bc379cc1b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6089/23400 [4:14:58<6:51:02,  1.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0f59e8ba299e81c661c9eff217c6bb98 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6242/23400 [4:18:44<6:15:26,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dd73112be906ff4d97c4a522871c9a5e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6289/23400 [4:24:07<6:05:53,  1.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 04f0d8f0bdc116a943cfebde7afdf78c in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6305/23400 [4:24:56<5:33:00,  1.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f6ac27f09dcb586552df06463d6614c3 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6394/23400 [4:27:18<5:19:36,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9843ebbd298486090152cdccd202f0fe in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 6574/23400 [4:31:42<6:10:26,  1.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e37f85b27b2c94adba773209ebff8533 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 6637/23400 [4:33:28<5:32:42,  1.19s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6fea7ff156204c453a9dd8861826f7d8 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 6668/23400 [4:34:37<4:19:24,  1.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09fcc5839d03456f63d3aeb4ae244b68 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6679/23400 [4:35:19<6:07:53,  1.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ecbaeb4d1da64e9c0bc091767c65af36 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6688/23400 [4:36:00<7:29:35,  1.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a971f6162aa9da9bb9c97f9285d34a63 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6689/23400 [4:36:32<49:18:54, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 60f6bf47ae0ff128910f68adc79bdbac in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6708/23400 [4:37:23<5:10:16,  1.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b99483a31d101d2230222a73e4eee52b in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 6784/23400 [4:40:10<7:22:43,  1.60s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 823ba221fb02db8878bd39f27ccccd82 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 6812/23400 [4:41:20<5:33:28,  1.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c69be78df3af3155ffc8803659983603 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 6818/23400 [4:41:57<12:02:31,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1f2b85db9cda148048c0c1cf97b9ccbe in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 6875/23400 [4:43:40<5:12:18,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 33b61dee53f46ec454ea9b09db8761ca in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6907/23400 [4:44:50<5:12:59,  1.14s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6937aaed905edef234efc693cc55c1cc in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6962/23400 [4:47:28<32:40:09,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 65143603a9ca5ca4cdbb254ec93ed739 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 7008/23400 [4:49:00<9:07:37,  2.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 18:09:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6c41063923d532-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 7013/23400 [4:54:17<107:56:14, 23.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4b27db58ed3952188ac22b54c5cf4171 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7055/23400 [4:55:39<6:02:04,  1.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID bbc45194a5fa2a18333cc5af0cf129c1 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7096/23400 [4:57:07<5:30:45,  1.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7269cce267f02dd75acd06f3cab46298 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 7292/23400 [5:01:52<5:43:23,  1.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 43cc258ea2f55af66b973fac8ed5539b in your message.)\n",
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a424f3ac34051cdf8fce88ebc4c29a00 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7411/23400 [5:05:11<5:33:44,  1.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 587daea0c5397b7ae447236ca2885423 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7431/23400 [5:06:03<5:48:34,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 141fe5690f1496d634197a28ff3a07f4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7499/23400 [5:07:58<5:35:57,  1.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8d4fd12a5fec9d67a2b7f48fe7451940 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7500/23400 [5:08:30<45:39:31, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 20a1976216137773b34efbb83e95d790 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7513/23400 [5:09:14<6:01:01,  1.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a7a36808716b4f757c4d03654ec9502c in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7641/23400 [5:14:35<6:07:16,  1.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 85637b7140a677926bb666cd7dc4133f in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7750/23400 [5:17:33<4:54:26,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 157876c9fa0fdfad9aa06719ce8135ba in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7772/23400 [5:18:32<5:20:34,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 18:38:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6c6c4b3b64d532-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7781/23400 [5:23:55<28:47:35,  6.64s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b2eb5322c8d3ac61ab51a31621e4304e in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7796/23400 [5:24:43<5:38:54,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 175c07430e97676ad0cd3e18c275a067 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7961/23400 [5:31:06<5:27:08,  1.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID de1354665149d0627d3af7d00d79111c in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7982/23400 [5:32:02<5:16:47,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 04b4bd165349c48dca5ff9ae626cfdf2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 8007/23400 [5:33:01<4:29:36,  1.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 5b606464dcd3425a485a64c58518b054 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8079/23400 [5:34:59<5:46:15,  1.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ac16453ee804bb357e15ede0ee36bcd1 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8084/23400 [5:35:35<14:21:30,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 00a597b7f4e8e0086cb3d31f592dbb65 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8110/23400 [5:36:39<5:20:44,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9848d261b129fa21c11b63253a52286a in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8136/23400 [5:37:46<4:51:46,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 18:58:02 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6c8873ede7f19a-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8140/23400 [5:43:01<140:22:09, 33.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Tue, 13 Jun 2023 19:03:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7d6c90278fabf19a-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8165/23400 [5:48:40<4:33:44,  1.08s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2d936117dbd2f370aa1dd2a0db3b8b04 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 8273/23400 [5:51:23<4:44:44,  1.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6e9f0ff9c6225f6565e7b98d563a5124 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 8294/23400 [5:52:17<4:48:33,  1.15s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1d15409335ebd4ef5d2c856beadd7be7 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 8310/23400 [5:53:05<4:28:48,  1.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8efb477f6bf5ec7889cdd4d99da584fd in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 8325/23400 [5:53:55<5:11:34,  1.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 09ab6a0e5d0a83838a6bed95324f02f7 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 8382/23400 [5:55:34<4:05:26,  1.02it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a8919756143018a3a37157287de24b6 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 8464/23400 [5:57:49<4:22:00,  1.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 95daf109f2cc431c1f7df91fc7b18065 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 8479/23400 [5:58:38<5:31:37,  1.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID eb3e48b3f97fe5d8ec0f566bc44317ab in your message.)\n",
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 66db54798f892fa80544caa07be290c2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8499/23400 [6:00:05<5:31:41,  1.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 03332d4c1395296fe0be75e92be8203d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8578/23400 [6:02:05<5:32:53,  1.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4a11250551e91940b3bd832ad74f37bb in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8622/23400 [6:03:30<5:30:45,  1.34s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ac4617799c40f3d51143bbc9c27d4998 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8685/23400 [6:05:21<5:56:26,  1.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 97a680836e5eb1fb78430c35a50809e7 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8830/23400 [6:08:47<5:28:52,  1.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b6f6b7106a7ed2080d68ea70f8885603 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8838/23400 [6:09:28<8:08:46,  2.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c3806d2537722ae3541455c038b6a5ea in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8859/23400 [6:10:22<5:02:26,  1.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7fe747cdf8905398079d0c049f06c4b0 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8908/23400 [6:11:57<4:57:09,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1494d3eeddc3582124336424a17efbeb in your message.)\n",
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e92f6a6f54522e5a40d1ce861dbcdf88 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8963/23400 [6:14:10<4:43:47,  1.18s/it] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template_round_3_with_def = lambda concepts, concepts_with_def, qa, query: \"GOAL: to visually discriminate \" + concepts + \". Their definitions are given as (\" + concepts_with_def + \"). Please generate a consise descriptive image caption for \" + query + \" only based on the information of this Q&A \" + qa + \". Please answer in template \\\"caption: {caption}\\\".\"\n",
    "template_round_3 = lambda concepts, concepts_with_def, qa, query: \"GOAL: to visually discriminate \" + concepts + \". Please generate a consise descriptive image caption for \" + query + \" only based on the information of this Q&A \" + qa + \". Please answer in template \\\"caption: {caption}\\\".\"\n",
    "synthesize_qa = lambda q, a: [item_q + ' ' + item_a for item_q, item_a in zip(q, a)]\n",
    "\n",
    "template_in_use_r3 = template_round_3\n",
    "concept_templates_r3 = [[[] for _ in range(len(concept_request))] for _ in range(n_repeat)]\n",
    "all_chatgpt_res_r3 = [[[] for _ in range(len(concept_request))] for _ in range(n_repeat)]\n",
    "with tqdm(total=n_repeat*len(concept_request)*10*len(concepts)) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            concepts = list(map(lambda x: x.strip(\"'\"), concept_request[j][0].split(', ')))\n",
    "            concept_templates_r3[i][j] = [ [] for _ in range(len(concepts)) ]\n",
    "            all_chatgpt_res_r3[i][j] = [ [] for _ in range(len(concepts)) ]\n",
    "            for k in range(len(concepts)):\n",
    "                qas = synthesize_qa(*all_qa_pairs[i][j][k][-2:])\n",
    "                for n in range(10):\n",
    "                    ### prepare template\n",
    "                    content = template_in_use_r3(row[0], row[1], qas[n], all_qa_pairs[i][j][k][0])\n",
    "                    concept_templates_r3[i][j][k].append(content)\n",
    "                    ### make request\n",
    "                    while 1:\n",
    "                        try:\n",
    "                            ### collect result\n",
    "                            all_chatgpt_res_r3[i][j][k].append(openai_chatgpt_post(content, verbose=False))\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    pbar.update(1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2d0944f-468c-425f-95e2-29d7bdaadb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r3, fpath=f'{args.dataset}-round=3-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14f4ea1f-8c6c-4688-8036-639ba8ca56bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### validate round 3\n",
    "all_qa_captions = deepcopy(all_chatgpt_res_r3)\n",
    "# all_qa_captions = np.empty([len(all_chatgpt_res_r3[0]), len(all_chatgpt_res_r3[0][0]), len(all_chatgpt_res_r3), len(all_chatgpt_res_r3[0][0][0][0])]).tolist()\n",
    "while 1:\n",
    "    invalid_inds = []\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(len(concept_request)):\n",
    "            concepts = list(map(lambda x: x.strip(\"'\"), concept_request[j][0].split(', ')))\n",
    "            for k in range(len(concepts)):\n",
    "                for i_c, cap in enumerate(all_chatgpt_res_r3[i][j][k]):\n",
    "                    if (not cap[:len('caption:')].lower() == 'caption:') or ('sorry' in cap):\n",
    "                        invalid_inds.append((i,j,k,i_c))\n",
    "                    else:\n",
    "                        extract_caption = lambda x: x.lower().split('caption: ')[-1].strip('{}\\\"')\n",
    "                        all_qa_captions[i][j][k][i_c] = extract_caption(all_chatgpt_res_r3[i][j][k][i_c])\n",
    "                        # all_qa_captions[j][k][i][i_c] = extract_caption(all_chatgpt_res_r3[i][j][k][i_c])\n",
    "    if len(invalid_inds)==0:\n",
    "        break\n",
    "        \n",
    "    with tqdm(total=len(invalid_inds)) as pbar:\n",
    "        for row in invalid_inds:\n",
    "            i, j, k, i_cap = row\n",
    "            qas = synthesize_qa(*all_qa_pairs[i][j][k][-2:])\n",
    "            content = template_in_use_r3(concept_request[j][0], concept_request[j][1], qas[i_cap], all_qa_pairs[i][j][k][0])\n",
    "            concept_templates_r3[i][j][k][i_cap] = content\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### collect result\n",
    "                    all_chatgpt_res_r3[i][j][k][i_cap] = openai_chatgpt_post(content, verbose=False)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6390b15c-5163-4f64-9ae9-3bce71810b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r3, fpath=f'{args.dataset}-round=3-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc5a6d60-3e94-4a0e-8b06-79be9c7b9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qa_captions = np.transpose(np.array(all_qa_captions), (1,2,0,3))\n",
    "all_qa_captions = all_qa_captions.reshape(all_qa_captions.shape[0], all_qa_captions.shape[1], np.prod(all_qa_captions.shape[-2:])).tolist() ### row x [concept x repeat x caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8932a8db-5ce0-458f-a49c-4498e0be3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_classifier_qa_captions(all_qa_captions, model, all_row_key_name=None):\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_qa_captions)) as pbar:\n",
    "        for idx, row in enumerate(all_qa_captions):\n",
    "            shape_row = np.array(row).shape ### 3 x 30\n",
    "            row = np.array(row).ravel().tolist()\n",
    "            if all_row_key_name is not None:\n",
    "                pass\n",
    "            row_t = tokenize(row).to(args.device)\n",
    "            features = model.encode_text(row_t)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a25f05c-61d1-4130-a3e3-8bc4c9466b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:10<00:00, 25.13it/s]\n"
     ]
    }
   ],
   "source": [
    "qa_cap_classifiers = build_classifier_qa_captions(all_qa_captions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9fbb121-21e1-4b5a-b147-17ad8987a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_inds_qa_cap = [torch.arange(90).int().div(30, rounding_mode='floor') for _ in range(len(qa_cap_classifiers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4804a-06f4-4c94-808c-e28b67db0d14",
   "metadata": {},
   "source": [
    "#### naive ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6914315-c82a-49db-8f2d-aae1407d7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeatures = all_vfeatures\n",
    "k_2 = 1\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "all_clu_pred_qa_cap = torch.zeros_like(all_clu_pred[:, 0])\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "for c in range(len(qa_cap_classifiers)):\n",
    "    ### selection \n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = qa_cap_classifiers[c]\n",
    "    ### prediction \n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    ### counting\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each ind\n",
    "    count_smask = []\n",
    "    smask = np.array(candidate_inds_qa_cap[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        # if enable_weight:\n",
    "        #     row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "        #     row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "        #     row_weight /= row_weight.sum()\n",
    "        #     count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        count_smask.append(count_names[smask==s].sum().item())\n",
    "    prediction = torch.tensor(count_smask).argmax(dim=-1)\n",
    "    instance_pred_voc[select] = topk_all_clu_pred[c, prediction]\n",
    "    all_clu_pred_qa_cap[c] = topk_all_clu_pred[c, prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0bb4778d-367e-47c6-bb88-9ec61cad8880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4842)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(instance_pred_voc == all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e86a-2f86-4ff9-8aa1-732ec9681f8d",
   "metadata": {},
   "source": [
    "#### logical ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bfdac131-0278-4bb4-9694-a8ad4f494c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row x [(concept x repeat x caption)] -> row x [caption x (concept x repeat)]\n",
    "dim_concept, dim_repeat, dim_caption, dim_feature = 3, 3, 10, 512\n",
    "qa_cap_classifiers = [x.view(dim_concept, dim_repeat, dim_caption, dim_feature).permute(2,0,1,3).view(dim_caption, -1, dim_feature) for x in qa_cap_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9a2a3c8b-e599-47e5-b357-c4c14341b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_inds_qa_cap = [torch.arange(9).int().div(3, rounding_mode='floor') for _ in range(len(qa_cap_classifiers))] ### row x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc8fa68c-193e-4c57-80c6-3d080a6b8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeatures = all_vfeatures\n",
    "k_2 = 2\n",
    "N = record_pred_kmeans_t.shape[0]\n",
    "R = all_clu_pred.shape[0]\n",
    "instance_pred_voc = torch.zeros(dim_caption, N)\n",
    "all_clu_pred_qa_cap = torch.zeros(dim_caption, R)\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "for i_cap in range(dim_caption):\n",
    "    for c in range(len(qa_cap_classifiers)):\n",
    "        ### selection \n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        row_classifier = qa_cap_classifiers[c][i_cap]\n",
    "        ### prediction \n",
    "        sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "        sim_topk = sim.topk(k=k_2)\n",
    "        ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "        ### counting\n",
    "        count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "        count_names[ind] = val ### count of each ind\n",
    "        count_smask = []\n",
    "        smask = np.array(candidate_inds_qa_cap[c]) ### partition mask\n",
    "        for s in np.unique(smask):\n",
    "            count_smask.append(count_names[smask==s].sum().item())\n",
    "        prediction = torch.tensor(count_smask).argmax(dim=-1)\n",
    "        instance_pred_voc[i_cap, select] = topk_all_clu_pred[c, prediction]\n",
    "        all_clu_pred_qa_cap[i_cap, c] = topk_all_clu_pred[c, prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de7d7600-2703-4695-8127-c964a855df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_pred_voc = instance_pred_voc.mode(dim=0).values.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c6ff3fa-7a6e-44d5-8608-d7fb1b502688",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clu_pred_qa_cap = all_clu_pred_qa_cap.mode(dim=0).values.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9cb06f96-2ea7-46c9-86e6-b8ccbe487e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4870)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(instance_pred_voc == all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a4644d5e-8ab4-4cf3-b242-b70f1ce213ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64dec99c-d61b-4121-a428-ff51186c7113",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHATGPT request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97155be-e602-4509-a3fa-e91d4a7a5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    # completion = openai.Completion.create(\n",
    "    #     model=\"text-davinci-003\",\n",
    "    #     prompt=content,  \n",
    "    #     temperature=0.7,\n",
    "    #     max_tokens=256,\n",
    "    #     top_p=1,\n",
    "    #     frequency_penalty=0,\n",
    "    #     presence_penalty=0,\n",
    "    # )\n",
    "    # result = completion['choices'][0]['text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c210a91-0cb2-429f-a936-87914e2ccc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clu_gt_voc = []\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "k_1 = 3\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "for i in range(k_1):\n",
    "    cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad4dbd-f755-4709-812c-f48f96e95801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46253372-83e5-4b5b-bb2b-d813d1795e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/OSZSL/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_13 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible adjective phrases of visual descriptions for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible visual descriptive phrases for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9_1\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16e8ec-4a49-4493-81d2-925d7a5347cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20293320-5aca-4921-aec9-58806f30cc45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)\n",
    "    \n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-iter=1.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(data, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'rb') as f:\n",
    "#     all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14a08e2c-498e-449e-b972-3e9e670aba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-uk{args.estimate_k}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb5cfa-2ec9-4824-b743-90b31b59a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-iter=1.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "878b68f2-a270-439c-bc5e-6e23c4f7ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "191fef52-2b54-4946-b3eb-c0a1e1db7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-uk{args.estimate_k}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96acd2a-7e85-41ef-afe9-2d937289fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/VDE/{args.exp}-{args.vocabname}-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba2f3d-8fb1-4e49-af3a-f7a61f79fced",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "while 1:\n",
    "    \"\"\" integrity check \"\"\"\n",
    "    while 1:\n",
    "        invalid_res = []\n",
    "        for i in range(n_repeat):\n",
    "            for j, row in enumerate(all_chatgpt_res[i]):\n",
    "                extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "                remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "                synsets = extract_synsetid(remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n')))\n",
    "                gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "                try:\n",
    "                    start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "                    synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "                    assert set(synsets)==set(gt_synsets)\n",
    "                except Exception as e:\n",
    "                    print(i, j)\n",
    "                    print(synsets, gt_synsets)\n",
    "                    invalid_res.append((i,j))\n",
    "\n",
    "        if len(invalid_res)==0:\n",
    "            break\n",
    "        else:\n",
    "            for i,j in invalid_res:\n",
    "                print(f'repair {(i,j)}')\n",
    "                content = concept_templates[j]\n",
    "                while 1:\n",
    "                    try:\n",
    "                        res = openai_chatgpt_post(content)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                all_chatgpt_res[i][j] = res\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" extract key-value-list from @chatgpt-res \"\"\"\n",
    "    extracted_chatgpt_res = []\n",
    "    for j, row in enumerate(all_chatgpt_res[0]):\n",
    "        # all_chatgpt_res[0][j] = \n",
    "        chatgpt_row_res = {}\n",
    "        extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "        remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "        extract_synnames = lambda r: list(map(lambda x: x.split(': ')[1].split('; '), r))\n",
    "        for i in range(n_repeat):\n",
    "            row = all_chatgpt_res[i][j]\n",
    "            row_data = remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n'))\n",
    "            synsets = extract_synsetid(row_data)\n",
    "            synnames = extract_synnames(row_data)\n",
    "            gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "            start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "            synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "            for idx_s, s in enumerate(synsets):\n",
    "                chatgpt_row_res.setdefault(s, [])\n",
    "                chatgpt_row_res[s].append( remove_space(synnames[idx_s]) )\n",
    "        extracted_chatgpt_res.append(chatgpt_row_res)\n",
    "\n",
    "    \"\"\" deduplication \"\"\"\n",
    "    use_dedup = True\n",
    "    all_candidates = []\n",
    "    all_candidates_set = []\n",
    "    for i, row in enumerate(extracted_chatgpt_res):\n",
    "        ### flatten multiple results\n",
    "        row_all_synset_names = list(map(lambda x: x.split('.')[0], row.keys()))\n",
    "        row_candidates = {}\n",
    "        row_candidates_set = {}\n",
    "        for k, v in row.items():\n",
    "            candidates = list(reduce(lambda x, y: x+y, v))\n",
    "            candidates = [c for c in candidates if c not in row_all_synset_names] ### remove competing synset names\n",
    "            set_candidates = set(candidates)\n",
    "            k = k.split('.')[0] ### key synset name\n",
    "            row_candidates.setdefault(k, [])\n",
    "            row_candidates_set.setdefault(k, set([]))\n",
    "            row_candidates[k].extend(candidates)\n",
    "            row_candidates_set[k] |= set_candidates\n",
    "        ### collect duplicates\n",
    "        duplicates = set()\n",
    "        for k1, v1 in row.items():\n",
    "            k1 = k1.split('.')[0]\n",
    "            for k2, v2 in row.items():\n",
    "                k2 = k2.split('.')[0]\n",
    "                if k1!=k2:\n",
    "                    duplicates |= row_candidates_set[k1]&row_candidates_set[k2]\n",
    "        ### remove duplication with synset-names (keys)\n",
    "        row_candidates_update = {}\n",
    "        row_candidates_set_update = {}\n",
    "        for k1, v1 in row.items():\n",
    "            k1 = k1.split('.')[0]\n",
    "            for k2, v2 in row.items():\n",
    "                k2 = k2.split('.')[0]\n",
    "            row_candidates_set_update[k1] = row_candidates_set[k1] - duplicates if use_dedup else row_candidates_set[k1]\n",
    "            row_candidates_update[k1] = [item for item in row_candidates[k1] if item not in duplicates ] if row_candidates_set[k1] else row_candidates[k1]\n",
    "\n",
    "        all_candidates.append(row_candidates_update)\n",
    "        all_candidates_set.append(row_candidates_set_update)\n",
    "\n",
    "\n",
    "    ### check non-empty\n",
    "    empty_list = []\n",
    "    for i, line in enumerate(all_candidates_set):\n",
    "        for k, v in line.items():\n",
    "            if len(v)==0:\n",
    "                for j in range(n_repeat):\n",
    "                    empty_list.append(j)\n",
    "                    print(f'repair {i} {j}')\n",
    "                    while 1:\n",
    "                        try:\n",
    "                            res = openai_chatgpt_post(concept_templates[i])\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    all_chatgpt_res[j][i] = res\n",
    "\n",
    "    if len(empty_list)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695fa28-601f-465e-8862-e405d825c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "{\n",
    "    'all_candidates': all_candidates,\n",
    "    'all_candidates_set': all_candidates_set,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053c196-dfb3-4491-9ad6-e6d400f4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" counter sorting \"\"\"\n",
    "all_candidates = data['all_candidates']\n",
    "all_counter_candidates = []\n",
    "all_number_candidates = []\n",
    "for row in all_candidates:\n",
    "    row_counter = {}\n",
    "    total_num = 0\n",
    "    for k, v in row.items():\n",
    "        ct = Counter(v)\n",
    "        row_counter[k] = OrderedDict(sorted(ct.items())) ### order key\n",
    "        total_num += sum(ct.values())\n",
    "    all_counter_candidates.append(OrderedDict(sorted(row_counter.items()))) ### order key\n",
    "    all_number_candidates.append(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc250a1-2d48-41ef-a166-649d1662be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### flatten\n",
    "all_row_mapping_idx_synset_name = []\n",
    "all_row_chatgpt_names = []\n",
    "all_row_i_syn = []\n",
    "all_row_weight = []\n",
    "all_row_key_name = []\n",
    "for i in range(len(all_counter_candidates)):\n",
    "    row_synset_names = all_counter_candidates[i].keys()\n",
    "    row_mapping_idx_synset_name = dict(zip(range(len(row_synset_names)), row_synset_names))\n",
    "    row_i_syn = []\n",
    "    row_chatgpt_names = []\n",
    "    row_weight = []\n",
    "    for i_syn, syn in enumerate(row_synset_names):\n",
    "        row_i_syn.extend([i_syn for _ in range(len(all_counter_candidates[i][syn]))])\n",
    "        row_chatgpt_names.extend(list(all_counter_candidates[i][syn]))\n",
    "        row_weight.extend(list(all_counter_candidates[i][syn].values()))\n",
    "    \n",
    "    all_row_mapping_idx_synset_name.append(row_mapping_idx_synset_name)\n",
    "    all_row_chatgpt_names.append(row_chatgpt_names)\n",
    "    all_row_i_syn.append(row_i_syn)\n",
    "    all_row_weight.append(row_weight)\n",
    "    all_row_key_name.append(list(map(lambda x: row_mapping_idx_synset_name[x], row_i_syn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a821d8-1775-4645-a77a-0269bc30fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=None):\n",
    "    \"\"\" build classifier for chatgpt\n",
    "    Args:\n",
    "        all_row_chatgpt_names: [[names]]\n",
    "    \"\"\"\n",
    "    if all_row_key_name is None: ### single name\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 1\n",
    "            templates = json.load(f)['imagenet']\n",
    "    else:\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 2\n",
    "            templates = json.load(f)[f'{args.dataset}-parent-3']\n",
    "            \n",
    "    len_t = len(templates)\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_row_chatgpt_names)) as pbar:\n",
    "        for idx, row in enumerate(all_row_chatgpt_names):\n",
    "            len_row = len(row)\n",
    "            if all_row_key_name is None:\n",
    "                row_t = [ t.format(name) for name in row for t in templates ]\n",
    "            else:\n",
    "                row_t = [ t.format(pname, name) for pname, name in zip(all_row_key_name[idx], row) for t in templates ]\n",
    "            row_t = tokenize(row_t).to(args.device)\n",
    "            features = model.encode_text(row_t)\n",
    "            features = features.view(len_row, len_t, -1).float()\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            features = features.mean(dim=1)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145633a-4d2e-4ab6-93dd-1d24d31ee023",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_classifier = build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=all_row_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61b61a-000c-43ba-bfa2-d652de176881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/vfeatures-{args.dataset}.npy')\n",
    "# is_correct = []\n",
    "# k_2 = 1\n",
    "# enable_weight = True\n",
    "# instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "# for c in range(len(all_row_classifier)):\n",
    "#     select = (record_pred_kmeans_t==c)\n",
    "#     row_classifier = all_row_classifier[c]\n",
    "#     sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "#     sim_topk = sim.topk(k=k_2)\n",
    "#     # reliable_samples = sim_topk.values.flatten().topk(k=int(0.9*sim.size(0))).indices\n",
    "#     ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "#     count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "#     count_names[ind] = val ### count of each name\n",
    "#     count_smask = []\n",
    "#     smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "#     for s in np.unique(smask):\n",
    "#         if enable_weight:\n",
    "#             row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "#             # row_weight /= row_weight.sum()\n",
    "#             # row_weight = torch.ones(len(all_row_weight[c])).float()\n",
    "#             # row_weight[smask==s] = row_weight[smask==s] / row_weight[smask==s].sum()\n",
    "#             row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "#             row_weight /= row_weight.sum()\n",
    "#             count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "#         else:\n",
    "#             count_smask.append(count_names[smask==s].sum())\n",
    "#     name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "#     name_gt = all_gt_voc[select].mode().values\n",
    "#     name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "#     is_correct.append(name_pred==name_gt)\n",
    "#     instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73726305-326d-4047-b855-daed75f86105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/features/vfeatures-{args.dataset}.npy')\n",
    "vfeatures = all_vfeatures\n",
    "all_clu_pred_chatgpt = torch.zeros_like(all_clu_pred)\n",
    "is_correct = []\n",
    "k_2 = 3\n",
    "enable_weight = True\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "for c in range(len(all_row_classifier)):\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = all_row_classifier[c]\n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each name\n",
    "    count_smask = []\n",
    "    smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        if enable_weight:\n",
    "            row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "            row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "            row_weight /= row_weight.sum()\n",
    "            count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        else:\n",
    "            count_smask.append(count_names[smask==s].sum())\n",
    "    name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "    name_gt = all_gt_voc[select].mode().values\n",
    "    name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "    is_correct.append(name_pred==name_gt)\n",
    "    instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]\n",
    "    \n",
    "    val_count = torch.tensor(count_smask)\n",
    "    ind_count = [ all_row_mapping_idx_synset_name[c][ii] for ii in range(val_count.shape[0]) ]\n",
    "    ind_count = torch.tensor([vocab.mapping_names_idx[xx] for xx in ind_count])\n",
    "    all_clu_pred_chatgpt[c, ind_count] = val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058e853-7b70-4c67-a46d-d689cbc32012",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_acc = np.array(is_correct).mean().item()\n",
    "instance_acc = (instance_pred_voc==all_gt_voc).float().mean().item()\n",
    "missing = all_gt_voc.unique().size(0) - all_gt_voc[(instance_pred_voc==all_gt_voc)].unique().size(0)\n",
    "\n",
    "print(f'name_acc={name_acc}, instance_acc={instance_acc}, missing={missing}')\n",
    "instance_pred_voc.unique().shape, all_gt_voc.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8db0d60-45de-46db-82b8-9cf10ee6c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_voc_clu = dict(zip(instance_pred_voc.unique().numpy().tolist(), range(len(instance_pred_voc))))\n",
    "# r_pred_kmeans_t = np.array([mapping_voc_clu[item.item()] for item in instance_pred_voc])\n",
    "\n",
    "# np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt-uk{args.estimate_k}.pth', r_pred_kmeans_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfd56e80-c545-4fd7-aa76-033e3b51d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "\n",
    "# # with open(f'./cache/openai/inov-cluster_visual_chatgpt-repeat={n_repeat}-k_1={k_1}-dataset={args.dataset}.pkl', 'wb') as f:\n",
    "# #     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# instance_acc = ((instance_pred_voc==all_gt_voc) | (cluster_ind_voc.cpu()==all_gt_voc)).float().mean().item()\n",
    "# print(instance_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f541e79a-8854-4c18-8d3d-26ddba25bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "a, res_ass = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "r_pred_kmeans_t, r_cluster_ind_voc = reassign_by_pred_cluster(a, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fba0b7de-3708-4a81-ab75-8ef96501a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 46\n",
      "iou voc:: 0.3698630136986301\n",
      "cluster acc 0.794941588325532\n"
     ]
    }
   ],
   "source": [
    "set_pred = set(res_ass[1].tolist())\n",
    "set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "iou_voc = n_inter/n_union\n",
    "n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "print('missing label::', n_missing_label)\n",
    "print('iou voc::', iou_voc)\n",
    "print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=r_pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85160855-c829-41dc-82ce-cb07dbcfead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/VDE/{args.exp}-{args.vocabname}-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e95f8569-6111-46f5-aa44-73f94a022318",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt-uk206.pth', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddece1d5-371e-4ea2-b057-45e6a1c52f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt.pth', r_pred_kmeans_t.cpu().numpy())\n",
    "# np.save(f'/home/sheng/sssa/ipynb/cache/cluster/cache-inov-{args.dataset}-clip-chatgpt-iter=1.pth', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f089427d-e49d-42d3-b96b-8ec64be6b327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([167, 167, 167, ..., 232, 232, 232])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f'/home/sheng/sssa/ipynb/cache/cluster/cache-inov-{args.dataset}-clip-chatgpt.pth.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158387f-c808-4a68-bb83-a887f9cc62b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### vocab ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f40302f9-1061-44b9-8651-50ba1e4128a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 3\n",
    "topk_all_clu_pred = (classifier@classifier.t()).topk(k=k_1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d905939-2990-4c9f-88c8-21c341c325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86ecafa0-5463-40b1-9e6a-d577a29377bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/sssa/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0d2d4-4343-40bc-be8c-f944383e7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60/20071 [06:52<38:59:28,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8c6d3b0b746aa518c8842e4e54019d28 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 126/20071 [15:12<47:16:45,  8.53s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b6404-bfff-49b5-a283-313d7efbd14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
