{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9697f3d5-2ba7-469c-a775-6eeb71e15a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import pandas as pd\n",
    "from wordnet_utils import *\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import get_datasets_oszsl, build_transform, get_hier_datasets, Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b1eee8-530e-4b7b-92b7-94b1a53b7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cuda:0'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset_name = 'make_entity13'\n",
    "    n_sampled_classes = 1000\n",
    "    input_size = 224\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99258ee0-367e-4f99-8eff-f0a2c118c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3955f8-fecd-44cb-86ce-fba688a93077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" build entire wn-graph \"\"\"\n",
    "from nxgraph_model import *\n",
    "\n",
    "with open('/home/sheng/dataset/wordnet_nouns_with_synset.pkl', 'rb') as f:\n",
    "    entire_vocab = pickle.load(f)\n",
    "    \n",
    "G = create_graph([wn.synset(x) for x in entire_vocab['synsets']], entire_vocab['ids'], entire_vocab['names'], entire_vocab['def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e31e8a-5026-4a3f-bdaa-7c2b62707b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 334718\n",
      "missing keys:\n",
      "[]\n",
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_val, seed=1)\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "print('dataset size', len(dataset))\n",
    "\n",
    "model = load_clip2(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cafe5f-0418-40a3-8460-5b0cacb29677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### subsample in21k graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606f9c1-993c-41d1-b11e-a2182ebccc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute parent subgraph with hierarchical closure \"\"\"\n",
    "### compute hierarchical closure of @classes\n",
    "classes_closure = classes\n",
    "for c in classes:\n",
    "    classes_closure = classes_closure | predecessor_set(G, source=c)\n",
    "\n",
    "subgraph = G.subgraph(list(classes_closure))\n",
    "\n",
    "parent_classes = set()\n",
    "for c in classes:\n",
    "    parent_classes = parent_classes | predecessor_set(G, source=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e80709bf-c668-4c33-9418-3dec71d39722",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "1. compute parent classes at K level\n",
    "2. build classifier per name\n",
    "\"\"\"\n",
    "k_hier = 6\n",
    "parent_classes_at_k = list(filter(lambda x: G.nodes[x]['depth']==k_hier, parent_classes))\n",
    "# print(len(parent_classes_at_k))\n",
    "# pprint(parent_classes_at_k, compact=True)\n",
    "parent_classes_at_k = sorted(list(set(map(lambda x: x.split('.')[0], parent_classes_at_k))))\n",
    "classifier = build_classifier(args, model, templates, parent_classes_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e734847b-cc92-4033-bb52-e5121d214a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_synset_to_parents = {k: predecessor_set(G, source=k) for k in classes}\n",
    "vfeatures = np.load(f'./cache/vfeatures-{args.dataset_name}.npy')\n",
    "\n",
    "sim = torch.from_numpy(vfeatures)@classifier.cpu().t()\n",
    "pred_topk = sim.topk(k=5).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "962e45c9-5881-4af0-99a4-0560a08643f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topk = np.array(parent_classes_at_k)[pred_topk.flatten()].reshape(-1, 5)\n",
    "to_name = lambda r: list(map(lambda x: x.split('.')[0], r))\n",
    "instance_to_parents = [\n",
    "    to_name(reduce(lambda x,y: x|y, list(map(lambda x: mapping_synset_to_parents[x.name()], mapping_vocidx_to_synsets(x.item(), vocab)))))\n",
    "    for x in all_gt_voc\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c2d0c9d2-0151-406d-85a2-10b741316a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" visualization: adding `type` causes confusion of classifier \"\"\"\n",
    "# parent_classes_at_k = list(filter(lambda x: G.nodes[x]['depth']==k_hier, parent_classes))\n",
    "# parent_classes_at_k = sorted(list(parent_classes_at_k))\n",
    "# name_parent_classes_at_k = list(map(lambda x: x.split('.')[0], parent_classes_at_k))\n",
    "# parent_parent_classes_at_k = [ ', '.join([pp.name().split('.')[0] for pp in wn.synset(p).hypernyms()]) for p in parent_classes_at_k]\n",
    "# classifier_p = build_classifier(args, model, templates, name_parent_classes_at_k, parent_parent_classes_at_k)\n",
    "\n",
    "# sim = classifier@classifier.t()\n",
    "# sns.distplot(sim.topk(k=10).values[:, 1].cpu().numpy(), bins=100)\n",
    "\n",
    "# sim = classifier_p@classifier_p.t()\n",
    "# sns.distplot(sim.topk(k=10).values[:, 1].cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8da2e-1cc6-41f6-8ca8-d2d320517c6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f663af-6f48-4314-b08b-77e9b888b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_classifier(args, model, templates, vocab_classnames, parent_classnames=None):\n",
    "    batch_size = 64\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        assert parent_classnames is None\n",
    "        with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "            for classname_set in np.array_split(vocab_classnames, len(vocab_classnames)//batch_size):\n",
    "                texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "                texts = tokenize(texts).to(args.device) #tokenize\n",
    "                class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "                class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "                class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "                class_embedding = class_embeddings.mean(dim=1)\n",
    "                class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "                zeroshot_weights.append(class_embedding.cpu())\n",
    "                pbar.update(1)\n",
    "        # else:\n",
    "        #     with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "        #         for classname_set, parentname_set in zip(\n",
    "        #             np.array_split(vocab_classnames, len(vocab_classnames)//batch_size),\n",
    "        #             np.array_split(parent_classnames, len(parent_classnames)//batch_size),\n",
    "        #         ):\n",
    "        #             texts = [template.format(classname)+f' A type of {pname}.' for classname, pname in zip(classname_set, parentname_set) for template in templates] #format with class\n",
    "        #             texts = tokenize(texts).to(args.device) #tokenize\n",
    "        #             class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "        #             class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "        #             class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "        #             class_embedding = class_embeddings.mean(dim=1)\n",
    "        #             class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "        #             zeroshot_weights.append(class_embedding.cpu())\n",
    "        #             pbar.update(1)\n",
    "    classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1ff2189-0380-48f4-bb3f-8d819c274688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:41<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = build_classifier(args, model, templates, vocab.classnames)\n",
    "torch.save(classifier, './cache/wordnet_classifier_in21k_word.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd907a29-a14e-41fc-81b8-7756f2eabc34",
   "metadata": {},
   "source": [
    "### performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3751c-e1d4-4c63-84d1-8827f2dff0ce",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1adcb05-b0e4-4806-93c4-142f00f333c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip(args):\n",
    "    model, preprocess = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model, preprocess\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "def topk_acc(all_pred_voc_topk, all_gt_voc):\n",
    "    acc = []\n",
    "    ### topK accuracy\n",
    "    for i in range(all_pred_voc_topk.size(1)):\n",
    "        vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "        for j in range(i+1):\n",
    "            vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "        print(f'k={i} acc={vec.float().mean()}')\n",
    "        acc.append(vec.float().mean().item())\n",
    "    return acc\n",
    "\n",
    "def semantic_acc(y_pred, y_true, metrics={}):\n",
    "    \"\"\" compute soft semantic acc for @y_pred and @y_true \"\"\"\n",
    "    assert len(metrics)>0\n",
    "    assert y_pred.size(0)==y_true.size(0)\n",
    "    scores = {m:[] for m in metrics.keys()}\n",
    "    with tqdm(total=y_pred.size(0)) as pbar:\n",
    "        for i in range(y_pred.size(0)):\n",
    "            syn_pred = mapping_vocidx_to_synsets(y_pred[i].item(), vocab)\n",
    "            syn_true = mapping_vocidx_to_synsets(y_true[i].item(), vocab)\n",
    "            pairs = list(itertools.product(range(len(syn_pred)), range(len(syn_true))))\n",
    "            for m_name, m in metrics.items():\n",
    "                scores[m_name].append( max([ m(syn_pred[p[0]], syn_true[p[1]]) for p in pairs ]) )\n",
    "            pbar.update(1)\n",
    "    for m_name in metrics.keys():\n",
    "        scores[m_name] = np.array(scores[m_name]).mean()\n",
    "    return scores\n",
    "    \n",
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48060709-150f-490c-8967-e15adfbdf404",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### naive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df58b595-ee52-4ec2-979e-270d85d6c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [02:49<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "all_pred_voc_topk = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_pred_voc_topk.append(prob.topk(k=5, dim=-1).indices.cpu())\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_pred_voc_topk = torch.cat(all_pred_voc_topk, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1308109a-fab9-4126-9c44-6309127f9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/features/vfeatures-{args.dataset_name}.npy', all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f947de4f-bd2c-4d9b-b55c-40692f45cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.33346137404441833\n",
      "n_missing=2\n"
     ]
    }
   ],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')\n",
    "n_missing = len(set(all_gt_voc.unique().numpy()) - set(all_pred_voc.unique().numpy()))\n",
    "print(f'n_missing={n_missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1711f146-82ca-480d-a9b7-f686da5525f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_baseline = semantic_acc(all_pred_voc, all_gt_voc, \n",
    "                              metrics={'wup_similarity': wn.wup_similarity, 'lch_similarity': wn.lch_similarity})\n",
    "print(score_baseline)\n",
    "\n",
    "topk_acc(all_pred_voc_topk, all_gt_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0bfb5-f07e-4d7e-baa0-f9e9d2a865de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/vfeatures-{args.dataset_name}.npy')\n",
    "\n",
    "\n",
    "# ind, val = all_pred_voc.unique(return_counts=True)\n",
    "# sns.distplot(val.cpu().numpy(), bins=100)\n",
    "\n",
    "# (val<2).sum()/ind.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384c09-940e-4fc4-b6cb-28cb36b3fbea",
   "metadata": {},
   "source": [
    "#### SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b10c10-78de-45c9-b436-1bd3da399979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(args.input_size, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=torch.tensor(mean),\n",
    "        std=torch.tensor(std))\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=0)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d0db35-1f20-463c-954d-c375b31039c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    n_count = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count.append( selected.sum().item() )\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected]).ravel(), return_counts=True)\n",
    "        # counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    n_count = torch.tensor(n_count).cpu()\n",
    "    \n",
    "    # all_clu_pred = setdiff_assignment(all_clu_pred)\n",
    "    \n",
    "    all_clu_pred = all_clu_pred/(n_count.view(-1, 1) + 1e-20)\n",
    "    \n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc, return_results=False):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    inst_acc = (label_voc_kmeans==all_gt_voc).float().mean().item()\n",
    "    print('instance label acc::', inst_acc)\n",
    "    if return_results:\n",
    "        return label_voc_kmeans, res_ass, inst_acc\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, model, classifier, device, \n",
    "                             all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                            prob = classifier_selected[similarity.softmax(-1)]\n",
    "                            cluster_ind.append(prob.cpu().argmax(dim=-1))\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    if classifier_selected is not None:\n",
    "        cluster_ind_voc = classifier_selected[cluster_ind]\n",
    "    else:\n",
    "        cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, \n",
    "                             preextracted_vfeatures=None):\n",
    "    \"\"\" given vocab label set @label_voc_kmeans, \n",
    "    Args:\n",
    "        label_voc_kmeans: cluster-assigned label on vocab\n",
    "        ...\n",
    "        preextracted_vfeatures: np.array([N x D])\n",
    "    Returns:\n",
    "        cluster_ind: tensor([N]): re-ordered cluster assignment\n",
    "        cluster_ind_voc: tensor([N]): cluster assignment indiced by vocab\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device).unique()\n",
    "    cluster_ind = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        if preextracted_vfeatures is not None:\n",
    "            N = len(loader_f.dataset)\n",
    "            batch_size = 10000\n",
    "            indices = np.array_split(np.arange(N), N//batch_size)\n",
    "            with torch.no_grad():\n",
    "                for group in indices:\n",
    "                    logits = torch.from_numpy(preextracted_vfeatures[group]).float()\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits@classifier.t().cpu()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    cluster_ind.append(prob[:, label_voc_kmeans.cpu()].argmax(dim=-1))\n",
    "        else:\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if preextracted_vfeatures is not None:\n",
    "                            logits = torch.from_numpy(preextracted_vfeatures[idx_img.cpu().numpy()]).float().to(device)\n",
    "                        else:\n",
    "                            logits = model.ema.extract_vfeatures(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        similarity = 100 * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "    cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def computation_reassign_by_pred_cluster(row, idx, args, model, classifier, candidate_classifier_ind):\n",
    "    \"\"\"\n",
    "    candidate_classifier_ind = label_voc_kmeans.unique().to(args.device)\n",
    "    \"\"\"\n",
    "    images, label_voc, label_clu, idx_img = row[:4]\n",
    "    images = images.to(args.device)\n",
    "    with amp_autocast():\n",
    "        vfeatures = model.visual(images).float()\n",
    "        # vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "    vfeatures = F.normalize(vfeatures, dim=-1)\n",
    "    batch_sim = 100*vfeatures@classifier[candidate_classifier_ind].t()\n",
    "    cluster_ind = batch_sim.argmax(dim=-1)\n",
    "    cluster_ind_voc = candidate_classifier_ind[cluster_ind].cpu()\n",
    "    return cluster_ind_voc\n",
    "\n",
    "def aggregation_reassign_by_pred_cluster(r, candidate_classifier_ind):\n",
    "    cluster_ind_voc = torch.cat(r, dim=0)\n",
    "    mapping_ind = dict(zip(cluster_ind_voc.unique().numpy(), torch.arange(cluster_ind_voc.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind_voc])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vfeatures(model, data_loader, device):\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(data_loader)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(data_loader):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(device)\n",
    "            with amp_autocast():\n",
    "                vfeatures = model.visual(images).float()\n",
    "            vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "            all_vfeatures.append(vfeatures.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    return all_vfeatures\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def loop_row_collect_results_nograd(obj_iter, computations={}, aggregations={}):\n",
    "    \"\"\" compute and aggregate results, looping over @obj_iter \n",
    "    func_computation(@row, @index_row)\n",
    "    aggregations(list(@results_computation))\n",
    "    \"\"\"\n",
    "    assert set(list(computations.keys())) == set(list(aggregations.keys()))\n",
    "    collector = { k:[] for k in computations }\n",
    "    with tqdm(total=len(obj_iter)) as pbar:\n",
    "        for i, row in enumerate(obj_iter):\n",
    "            ### apply computations\n",
    "            for k, func in computations.items():\n",
    "                collector[k].append(func(row, i))\n",
    "            pbar.update(1)\n",
    "    ### aggregate results\n",
    "    results = {}\n",
    "    for k, func_agg in aggregations.items():\n",
    "        results[k] = func_agg(collector[k])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbf7bb34-74cb-4c02-8345-0c14d1de9cb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [13:56<00:00,  1.28s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     40\u001b[0m     record_pred_kmeans_t \u001b[38;5;241m=\u001b[39m pred_kmeans_t\n\u001b[0;32m---> 41\u001b[0m     all_clu_pred \u001b[38;5;241m=\u001b[39m agg_by_pred_cluster(args, \u001b[43mpred_kmeans_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m(), all_topk_voc, voc_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_voc)\n\u001b[1;32m     42\u001b[0m     label_voc_kmeans, res_ass \u001b[38;5;241m=\u001b[39m linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)\n",
    "\n",
    "# pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-clip.npy'))\n",
    "pred_kmeans_t = np.load(f'/home/sheng/sssa/ipynb/cache/cluster/kmeans-{args.dataset_name}.npy')\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    # pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    # results = \\\n",
    "    # loop_row_collect_results_nograd(loader_f, \n",
    "    #                                 computations={\n",
    "    #                                     'reassign_by_pred_cluster': partial(\n",
    "    #                                         computation_reassign_by_pred_cluster, \n",
    "    #                                         args=args, \n",
    "    #                                         model=model, \n",
    "    #                                         classifier=classifier, \n",
    "    #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "    #                                     ),\n",
    "    #                                 }, \n",
    "    #                                 aggregations={\n",
    "    #                                     'reassign_by_pred_cluster': partial(\n",
    "    #                                         aggregation_reassign_by_pred_cluster,\n",
    "    #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "    #                                     ),\n",
    "    #                                 },\n",
    "    #                                )\n",
    "    # pred_kmeans_t, cluster_ind_voc = results['reassign_by_pred_cluster']\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29ab4f-640c-499b-ae58-1b41bdfbc5aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/MUST-output/make_entity13/debug-scd-vproto=1.0-w_st=0.0-entmin_conf=0.0-w_entmin=0.25-pv_conf=0.5-w_pv=0.25-no_calib-update=5-lr=1e-3-B224-E10/clustering.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    # pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    # results = \\\n",
    "    # loop_row_collect_results_nograd(loader_f, \n",
    "    #                                 computations={\n",
    "    #                                     'reassign_by_pred_cluster': partial(\n",
    "    #                                         computation_reassign_by_pred_cluster, \n",
    "    #                                         args=args, \n",
    "    #                                         model=model, \n",
    "    #                                         classifier=classifier, \n",
    "    #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "    #                                     ),\n",
    "    #                                 }, \n",
    "    #                                 aggregations={\n",
    "    #                                     'reassign_by_pred_cluster': partial(\n",
    "    #                                         aggregation_reassign_by_pred_cluster,\n",
    "    #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "    #                                     ),\n",
    "    #                                 },\n",
    "    #                                )\n",
    "    # pred_kmeans_t, cluster_ind_voc = results['reassign_by_pred_cluster']\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1913536-5442-4636-8d16-fa8e283be7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'all_clu_pred': all_clu_pred,\n",
    "    'label_voc_kmeans': label_voc_kmeans,\n",
    "    'pred_kmeans_t': pred_kmeans_t,\n",
    "    'cluster_ind_voc': cluster_ind_voc,\n",
    "    'record_pred_kmeans_t': record_pred_kmeans_t,\n",
    "    'all_gt_voc': all_gt_voc,\n",
    "    'all_label_clu': all_label_clu,\n",
    "    'all_topk_voc': all_topk_voc,\n",
    "}, f'./cache-inov-{args.dataset_name}-clip.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7dfb287-384e-4c0f-aaf7-bfa31e2257f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(f'./cache-inov-{args.dataset_name}-clip.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']\n",
    "all_topk_voc = res['all_topk_voc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468da9a9-ac21-487e-91f6-61ff4cdd9929",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-clip.npy'))\n",
    "pred_kmeans_t = all_label_clu\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, model, classifier, args.device)\n",
    "    # results = \\\n",
    "    # loop_row_collect_results_nograd(loader_f, \n",
    "    #                                 computations={\n",
    "    #                                     'reassign_by_pred_cluster': partial(\n",
    "    #                                         computation_reassign_by_pred_cluster, \n",
    "    #                                         args=args, \n",
    "    #                                         model=model, \n",
    "    #                                         classifier=classifier, \n",
    "    #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "    #                                     ),\n",
    "    #                                 }, \n",
    "    #                                 aggregations={\n",
    "    #                                     'reassign_by_pred_cluster': partial(\n",
    "    #                                         aggregation_reassign_by_pred_cluster,\n",
    "    #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "    #                                     ),\n",
    "    #                                 },\n",
    "    #                                )\n",
    "    # pred_kmeans_t, cluster_ind_voc = results['reassign_by_pred_cluster']\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dec99c-d61b-4121-a428-ff51186c7113",
   "metadata": {},
   "source": [
    "#### CHATGPT request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97155be-e602-4509-a3fa-e91d4a7a5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    # completion = openai.Completion.create(\n",
    "    #     model=\"text-davinci-003\",\n",
    "    #     prompt=content,  \n",
    "    #     temperature=0.7,\n",
    "    #     max_tokens=256,\n",
    "    #     top_p=1,\n",
    "    #     frequency_penalty=0,\n",
    "    #     presence_penalty=0,\n",
    "    # )\n",
    "    # result = completion['choices'][0]['text']\n",
    "    return result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=None):\n",
    "    \"\"\" build classifier for chatgpt\n",
    "    Args:\n",
    "        all_row_chatgpt_names: [[names]]\n",
    "    \"\"\"\n",
    "    if all_row_key_name is None: ### single name\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 1\n",
    "            templates = json.load(f)['imagenet']\n",
    "    else:\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 2\n",
    "            templates = json.load(f)['imagenet-parent-3']\n",
    "            \n",
    "    len_t = len(templates)\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_row_chatgpt_names)) as pbar:\n",
    "        for idx, row in enumerate(all_row_chatgpt_names):\n",
    "            len_row = len(row)\n",
    "            if all_row_key_name is None:\n",
    "                row_t = [ t.format(name) for name in row for t in templates ]\n",
    "            else:\n",
    "                row_t = [ t.format(name, pname) for pname, name in zip(all_row_key_name[idx], row) for t in templates ]\n",
    "            row_t = tokenize(row_t).to(args.device)\n",
    "            features = model.encode_text(row_t)\n",
    "            features = features.view(len_row, len_t, -1).float()\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            features = features.mean(dim=1)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c210a91-0cb2-429f-a936-87914e2ccc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clu_gt_voc = []\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "k_1 = 3\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "for i in range(k_1):\n",
    "    cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad4dbd-f755-4709-812c-f48f96e95801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46253372-83e5-4b5b-bb2b-d813d1795e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/OSZSL/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "be16e8ec-4a49-4493-81d2-925d7a5347cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [1:04:40<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "20293320-5aca-4921-aec9-58806f30cc45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(data, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'rb') as f:\n",
    "#     all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb5cfa-2ec9-4824-b743-90b31b59a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset_name}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2dd17f-1d88-4455-8491-d1097dae6fa8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" integrity check \"\"\"\n",
    "while 1:\n",
    "    invalid_res = []\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(all_chatgpt_res[i]):\n",
    "            extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "            remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "            synsets = extract_synsetid(remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n')))\n",
    "            gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "            try:\n",
    "                start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "                synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "                assert set(synsets)==set(gt_synsets)\n",
    "            except Exception as e:\n",
    "                print(i, j)\n",
    "                print(synsets, gt_synsets)\n",
    "                invalid_res.append((i,j))\n",
    "            \n",
    "    if len(invalid_res)==0:\n",
    "        break\n",
    "    else:\n",
    "        for i,j in invalid_res:\n",
    "            print(f'repair {(i,j)}')\n",
    "            content = concept_templates[j]\n",
    "            while 1:\n",
    "                try:\n",
    "                    res = openai_chatgpt_post(content)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            all_chatgpt_res[i][j] = res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff74fe8-2d15-4c91-8067-f95a5dd713c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" extract key-value-list from @chatgpt-res \"\"\"\n",
    "extracted_chatgpt_res = []\n",
    "for j, row in enumerate(all_chatgpt_res[0]):\n",
    "    # all_chatgpt_res[0][j] = \n",
    "    chatgpt_row_res = {}\n",
    "    extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "    remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "    extract_synnames = lambda r: list(map(lambda x: x.split(': ')[1].split('; '), r))\n",
    "    for i in range(n_repeat):\n",
    "        row = all_chatgpt_res[i][j]\n",
    "        row_data = remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n'))\n",
    "        synsets = extract_synsetid(row_data)\n",
    "        synnames = extract_synnames(row_data)\n",
    "        gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "        start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "        synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "        for idx_s, s in enumerate(synsets):\n",
    "            chatgpt_row_res.setdefault(s, [])\n",
    "            chatgpt_row_res[s].append( remove_space(synnames[idx_s]) )\n",
    "    extracted_chatgpt_res.append(chatgpt_row_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372e283-d454-40dd-96cb-dfc908398322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" deduplication \"\"\"\n",
    "use_dedup = True\n",
    "all_candidates = []\n",
    "all_candidates_set = []\n",
    "for i, row in enumerate(extracted_chatgpt_res):\n",
    "    ### flatten multiple results\n",
    "    row_all_synset_names = list(map(lambda x: x.split('.')[0], row.keys()))\n",
    "    row_candidates = {}\n",
    "    row_candidates_set = {}\n",
    "    for k, v in row.items():\n",
    "        candidates = list(reduce(lambda x, y: x+y, v))\n",
    "        candidates = [c for c in candidates if c not in row_all_synset_names] ### remove competing synset names\n",
    "        set_candidates = set(candidates)\n",
    "        k = k.split('.')[0] ### key synset name\n",
    "        row_candidates.setdefault(k, [])\n",
    "        row_candidates_set.setdefault(k, set([]))\n",
    "        row_candidates[k].extend(candidates)\n",
    "        row_candidates_set[k] |= set_candidates\n",
    "    ### collect duplicates\n",
    "    duplicates = set()\n",
    "    for k1, v1 in row.items():\n",
    "        k1 = k1.split('.')[0]\n",
    "        for k2, v2 in row.items():\n",
    "            k2 = k2.split('.')[0]\n",
    "            if k1!=k2:\n",
    "                duplicates |= row_candidates_set[k1]&row_candidates_set[k2]\n",
    "    ### remove duplication with synset-names (keys)\n",
    "    row_candidates_update = {}\n",
    "    row_candidates_set_update = {}\n",
    "    for k1, v1 in row.items():\n",
    "        k1 = k1.split('.')[0]\n",
    "        for k2, v2 in row.items():\n",
    "            k2 = k2.split('.')[0]\n",
    "        row_candidates_set_update[k1] = row_candidates_set[k1] - duplicates if use_dedup else row_candidates_set[k1]\n",
    "        row_candidates_update[k1] = [item for item in row_candidates[k1] if item not in duplicates ] if row_candidates_set[k1] else row_candidates[k1]\n",
    "    \n",
    "    all_candidates.append(row_candidates_update)\n",
    "    all_candidates_set.append(row_candidates_set_update)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc526f40-ac1e-46e9-8eb9-8c1236e9135a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### check non-empty\n",
    "for i, line in enumerate(all_candidates_set):\n",
    "    for k, v in line.items():\n",
    "        if len(v)==0:\n",
    "            for j in range(n_repeat):\n",
    "                print(f'repair {i} {j}')\n",
    "                while 1:\n",
    "                    try:\n",
    "                        res = openai_chatgpt_post(concept_templates[i])\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                all_chatgpt_res[j][i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695fa28-601f-465e-8862-e405d825c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "{\n",
    "    'all_candidates': all_candidates,\n",
    "    'all_candidates_set': all_candidates_set,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053c196-dfb3-4491-9ad6-e6d400f4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" counter sorting \"\"\"\n",
    "all_candidates = data['all_candidates']\n",
    "all_counter_candidates = []\n",
    "all_number_candidates = []\n",
    "for row in all_candidates:\n",
    "    row_counter = {}\n",
    "    total_num = 0\n",
    "    for k, v in row.items():\n",
    "        ct = Counter(v)\n",
    "        row_counter[k] = OrderedDict(sorted(ct.items())) ### order key\n",
    "        total_num += sum(ct.values())\n",
    "    all_counter_candidates.append(OrderedDict(sorted(row_counter.items()))) ### order key\n",
    "    all_number_candidates.append(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc250a1-2d48-41ef-a166-649d1662be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### flatten\n",
    "all_row_mapping_idx_synset_name = []\n",
    "all_row_chatgpt_names = []\n",
    "all_row_i_syn = []\n",
    "all_row_weight = []\n",
    "all_row_key_name = []\n",
    "for i in range(len(all_counter_candidates)):\n",
    "    row_synset_names = all_counter_candidates[i].keys()\n",
    "    row_mapping_idx_synset_name = dict(zip(range(len(row_synset_names)), row_synset_names))\n",
    "    row_i_syn = []\n",
    "    row_chatgpt_names = []\n",
    "    row_weight = []\n",
    "    for i_syn, syn in enumerate(row_synset_names):\n",
    "        row_i_syn.extend([i_syn for _ in range(len(all_counter_candidates[i][syn]))])\n",
    "        row_chatgpt_names.extend(list(all_counter_candidates[i][syn]))\n",
    "        row_weight.extend(list(all_counter_candidates[i][syn].values()))\n",
    "    \n",
    "    all_row_mapping_idx_synset_name.append(row_mapping_idx_synset_name)\n",
    "    all_row_chatgpt_names.append(row_chatgpt_names)\n",
    "    all_row_i_syn.append(row_i_syn)\n",
    "    all_row_weight.append(row_weight)\n",
    "    all_row_key_name.append(list(map(lambda x: row_mapping_idx_synset_name[x], row_i_syn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145633a-4d2e-4ab6-93dd-1d24d31ee023",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_classifier = build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=all_row_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9a61b61a-000c-43ba-bfa2-d652de176881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vfeatures = np.load(f'./cache/features/vfeatures-{args.dataset_name}.npy')\n",
    "is_correct = []\n",
    "k_2 = 1\n",
    "enable_weight = True\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "for c in range(len(all_row_classifier)):\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = all_row_classifier[c]\n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    # reliable_samples = sim_topk.values.flatten().topk(k=int(0.9*sim.size(0))).indices\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each name\n",
    "    count_smask = []\n",
    "    smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        if enable_weight:\n",
    "            row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "            # row_weight /= row_weight.sum()\n",
    "            # row_weight = torch.ones(len(all_row_weight[c])).float()\n",
    "            # row_weight[smask==s] = row_weight[smask==s] / row_weight[smask==s].sum()\n",
    "            row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "            row_weight /= row_weight.sum()\n",
    "            count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        else:\n",
    "            count_smask.append(count_names[smask==s].sum())\n",
    "    name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "    name_gt = all_gt_voc[select].mode().values\n",
    "    name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "    is_correct.append(name_pred==name_gt)\n",
    "    instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73726305-326d-4047-b855-daed75f86105",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeatures = np.load(f'./cache/features/vfeatures-{args.dataset_name}.npy')\n",
    "all_clu_pred_chatgpt = torch.zeros_like(all_clu_pred)\n",
    "is_correct = []\n",
    "k_2 = 3\n",
    "enable_weight = True\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "for c in range(len(all_row_classifier)):\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = all_row_classifier[c]\n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each name\n",
    "    count_smask = []\n",
    "    smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        if enable_weight:\n",
    "            row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "            row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "            row_weight /= row_weight.sum()\n",
    "            count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        else:\n",
    "            count_smask.append(count_names[smask==s].sum())\n",
    "    name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "    name_gt = all_gt_voc[select].mode().values\n",
    "    name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "    is_correct.append(name_pred==name_gt)\n",
    "    instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]\n",
    "    \n",
    "    val_count = torch.tensor(count_smask)\n",
    "    ind_count = [ all_row_mapping_idx_synset_name[c][ii] for ii in range(k_1) ]\n",
    "    ind_count = torch.tensor([vocab.mapping_names_idx[xx] for xx in ind_count])\n",
    "    all_clu_pred_chatgpt[c, ind_count] = val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058e853-7b70-4c67-a46d-d689cbc32012",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_acc = np.array(is_correct).mean().item()\n",
    "instance_acc = (instance_pred_voc==all_gt_voc).float().mean().item()\n",
    "missing = all_gt_voc.unique().size(0) - all_gt_voc[(instance_pred_voc==all_gt_voc)].unique().size(0)\n",
    "\n",
    "print(f'name_acc={name_acc}, instance_acc={instance_acc}, missing={missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50583f8e-9be3-4deb-87e6-2e9be02c3523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_assign\n",
      "assignment shape=(260, 20071)\n",
      "instance label acc:: 0.4360088109970093\n"
     ]
    }
   ],
   "source": [
    "a, b = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "93218a5d-5871-49ba-b263-0e8705b8933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./cache/openai/inov-cluster_visual_chatgpt-repeat={n_repeat}-k_1={k_1}-dataset={args.dataset_name}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7749b23a-2744-49dd-b898-86c5f38360ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5370240807533264\n"
     ]
    }
   ],
   "source": [
    "instance_acc = ((instance_pred_voc==all_gt_voc) | (cluster_ind_voc.cpu()==all_gt_voc)).float().mean().item()\n",
    "print(instance_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f541e79a-8854-4c18-8d3d-26ddba25bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_assign\n",
      "assignment shape=(260, 20071)\n",
      "instance label acc:: 0.4360088109970093\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [17:37<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "a, res_ass = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "r_pred_kmeans_t, r_cluster_ind_voc = reassign_by_pred_cluster(a, model, classifier, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fba0b7de-3708-4a81-ab75-8ef96501a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 120\n",
      "cluster acc 0.7123250019419332\n"
     ]
    }
   ],
   "source": [
    "set_pred = set(res_ass[1].tolist())\n",
    "set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "print('missing label::', len(set_gt - set_pred))\n",
    "print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=r_pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddece1d5-371e-4ea2-b057-45e6a1c52f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-clip-chatgpt.npy', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f2678-f263-47eb-b962-022e0ab7fa71",
   "metadata": {},
   "source": [
    "### Vocab ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5820f7b0-9889-46aa-90c6-915c6f07fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-extract.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "extracted_chatgpt_synsets = data['synsets']\n",
    "extracted_chatgpt_synsets_counter = data['synsets_counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "485b9c6f-8621-402b-9260-384575d1a587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20071"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_chatgpt_synsets_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1c1bc8f4-33ec-424a-bfaa-23bbc4386d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(f'./cache-inov-{args.dataset_name}-clip.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']\n",
    "all_topk_voc = res['all_topk_voc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eb1804cb-00cf-4bcf-90ae-c7460263528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@3 = 0.7384615540504456\n"
     ]
    }
   ],
   "source": [
    "\"\"\" get topk candidates \"\"\"\n",
    "all_clu_gt_voc = []\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "k_1 = 3\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "for i in range(k_1):\n",
    "    cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ead4a04b-973f-4bb0-9aa8-25403b5793eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concept names \"\"\"\n",
    "to_name = lambda x: x[0].name().split('.')[0]\n",
    "cluster_row_names = []\n",
    "cluster_table = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_names = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    table = []\n",
    "    for rn in row_names:\n",
    "        for k, v in extracted_chatgpt_synsets_counter[rn].items():\n",
    "            table_row = [rn, k, v]\n",
    "            table.append(deepcopy(table_row))\n",
    "    cluster_row_names.append(row_names)\n",
    "    cluster_table.append(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "94d47cb2-78e4-4fbe-b020-0418a2012371",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_df = []\n",
    "cluster_names = []\n",
    "cluster_prompts = []\n",
    "cluster_weights = []\n",
    "all_row_i_syn = []\n",
    "all_row_mapping_idx_synset_name = []\n",
    "for row in cluster_table:\n",
    "    df = pd.DataFrame(row, columns=['concept', 'prompt', 'freq'])\n",
    "    cluster_df.append(df)\n",
    "    row_names = df['concept'].values.tolist()\n",
    "    cluster_names.append(row_names)\n",
    "    cluster_prompts.append(df['prompt'].values.tolist())\n",
    "    cluster_weights.append(df['freq'].values)\n",
    "    row_mapping_idx_synset = dict( zip(range(len(row_names)), list(set(row_names))) )\n",
    "    row_mapping_synset_idx = dict( zip(list(set(row_names)), range(len(row_names))) )\n",
    "    all_row_i_syn.append( np.array([row_mapping_synset_idx[x] for x in row_names]) )\n",
    "    all_row_mapping_idx_synset_name.append(row_mapping_idx_synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a05b2724-8f5f-425f-91af-208861421e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:34<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "all_row_classifier = build_classifier_chatgpt(\n",
    "    cluster_prompts, \n",
    "    model, \n",
    "    cluster_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "12cf8cb2-2719-4a82-9d24-b045ab41262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_weight = cluster_weights\n",
    "\n",
    "vfeatures = np.load(f'/home/sheng/sssa/ipynb/cache/features/vfeatures-{args.dataset_name}.npy')\n",
    "\n",
    "all_clu_pred_chatgpt = torch.zeros_like(all_clu_pred)\n",
    "is_correct = []\n",
    "k_2 = 3\n",
    "enable_weight = True\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "for c in range(len(all_row_classifier)):\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = all_row_classifier[c]\n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each name\n",
    "    count_smask = []\n",
    "    smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        if enable_weight:\n",
    "            row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "            row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "            row_weight /= row_weight.sum()\n",
    "            count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        else:\n",
    "            count_smask.append(count_names[smask==s].sum())\n",
    "    name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "    name_gt = all_gt_voc[select].mode().values\n",
    "    name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "    is_correct.append(name_pred==name_gt)\n",
    "    instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]\n",
    "    \n",
    "    val_count = torch.tensor(count_smask)\n",
    "    ind_count = [ all_row_mapping_idx_synset_name[c][ii] for ii in range(k_1) ]\n",
    "    ind_count = torch.tensor([vocab.mapping_names_idx[xx] for xx in ind_count])\n",
    "    all_clu_pred_chatgpt[c, ind_count] = val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "04f2bbf9-aa6b-4128-96a5-b83df2f58a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_acc=0.5, instance_acc=0.42655012011528015, missing=126\n"
     ]
    }
   ],
   "source": [
    "name_acc = np.array(is_correct).mean().item()\n",
    "instance_acc = (instance_pred_voc==all_gt_voc).float().mean().item()\n",
    "missing = all_gt_voc.unique().size(0) - all_gt_voc[(instance_pred_voc==all_gt_voc)].unique().size(0)\n",
    "\n",
    "print(f'name_acc={name_acc}, instance_acc={instance_acc}, missing={missing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158387f-c808-4a68-bb83-a887f9cc62b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### vocab ChatGPT request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1b6404-bfff-49b5-a283-313d7efbd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f40302f9-1061-44b9-8651-50ba1e4128a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 3\n",
    "topk_all_clu_pred = (classifier@classifier.t()).topk(k=k_1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d905939-2990-4c9f-88c8-21c341c325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86ecafa0-5463-40b1-9e6a-d577a29377bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/OSZSL/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0d2d4-4343-40bc-be8c-f944383e7988",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3912/10071 [6:09:47<6:43:36,  3.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Thu, 20 Apr 2023 23:10:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '7bb10ed41ba4b7a3-AMS'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4703/10071 [7:20:07<8:05:22,  5.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5217/10071 [8:07:01<6:39:37,  4.94s/it] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 84%|████████▍ | 8504/10071 [13:52:08<2:30:37,  5.77s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7f4473d7-845f-4899-83b5-a011d6518ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1.pkl', 'rb') as f:\n",
    "    d1 = pickle.load(f)\n",
    "    \n",
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-part=2.pkl', 'rb') as f:\n",
    "    d2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "61ee35e0-c401-4a6e-b13a-b9889b00d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chatgpt_res = d1[0][:10000] + d2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05295da6-49c5-4fa0-be39-1b5d11608e96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" integrity check \"\"\"\n",
    "n_repeat = 1\n",
    "repeated_request_record = {}\n",
    "while 1:\n",
    "    invalid_res = []\n",
    "    for j, row in enumerate(all_chatgpt_res):\n",
    "        extract_synsetid = lambda r: list(map(lambda x: x.strip('\"\\'').split(': ')[0], r))\n",
    "        remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "        synsets = extract_synsetid(remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n')))\n",
    "        gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "        try:\n",
    "            # start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "            # synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "            assert set(synsets)==set(gt_synsets)\n",
    "            # assert synsets == gt_synsets\n",
    "        except Exception as e:\n",
    "            print(j)\n",
    "            print(synsets, gt_synsets)\n",
    "            invalid_res.append(j)\n",
    "    if len(invalid_res)==0:\n",
    "        break\n",
    "    else:\n",
    "        print(len(invalid_res))\n",
    "        for j in invalid_res:\n",
    "            print(f'repair {j}')\n",
    "            repeated_request_record.setdefault(j, 0)\n",
    "            content = concept_templates[j]\n",
    "            while 1:\n",
    "                try:\n",
    "                    repeated_request_record[j] = repeated_request_record[j] + 1\n",
    "                    res = openai_chatgpt_post(content)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            all_chatgpt_res[j] = res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0a2a95d5-a134-478d-9377-f6ebb5b0b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chatgpt_res[8705] = 'hercules\\'-club.n.01: densely spiny; ornamental; southeastern United States; West Indies\\nservice_club.n.02: recreational center; servicemen\\nclass.n.02: body of students; taught together'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4425a63-f992-4a41-b46d-a597186e3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-clean-2.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3909b341-8154-4468-bf26-8d11907b9f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20071"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-clean-2.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)\n",
    "len(all_chatgpt_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d50d1-bc3f-4925-bdbf-157be13041db",
   "metadata": {},
   "source": [
    "concept name dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4529b447-0630-41e8-b0bc-0aa8ce779354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" extract key-value-list from @chatgpt-res \"\"\"\n",
    "extracted_chatgpt_res = []\n",
    "for j, row in enumerate(all_chatgpt_res):\n",
    "    # all_chatgpt_res[0][j] = \n",
    "    chatgpt_row_res = {}\n",
    "    extract_synsetid = lambda r: list(map(lambda x: x.strip('\"\\'').split(': ')[0], r))\n",
    "    remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "    extract_synnames = lambda r: list(map(lambda x: x.split(': ')[1].split('; '), r))\n",
    "    row = all_chatgpt_res[j]\n",
    "    row_data = remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n'))\n",
    "    synsets = extract_synsetid(row_data)\n",
    "    synnames = extract_synnames(row_data)\n",
    "    gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "    # start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "    # synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "    for idx_s, s in enumerate(synsets):\n",
    "        chatgpt_row_res.setdefault(s, [])\n",
    "        chatgpt_row_res[s] = remove_space(synnames[idx_s])\n",
    "    extracted_chatgpt_res.append(chatgpt_row_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ab6046b-e0d9-405a-ba1e-f02f864b9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_name = lambda x: x.split('.')[0]\n",
    "extracted_chatgpt_synsets = {}\n",
    "for row in extracted_chatgpt_res:\n",
    "    for k, v in row.items():\n",
    "        extracted_chatgpt_synsets.setdefault(k, [])\n",
    "        extracted_chatgpt_synsets[k].extend(v)\n",
    "\n",
    "extracted_chatgpt_synsets_counter = {}\n",
    "for k, v in extracted_chatgpt_synsets.items():\n",
    "    extracted_chatgpt_synsets_counter.setdefault(to_name(k), Counter())\n",
    "    extracted_chatgpt_synsets_counter[to_name(k)] = extracted_chatgpt_synsets_counter[to_name(k)] + Counter(v)\n",
    "    assert len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d9af138-0fea-4abf-ad3a-26c9c2df62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "{\n",
    "    'synsets': extracted_chatgpt_synsets,\n",
    "    'synsets_counter': extracted_chatgpt_synsets_counter,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78af13ea-b440-43b1-b123-fa45aebc29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-extract.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417f3e2c-1753-4223-af87-40ebe940b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-extract.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe968cd-382d-4e63-b4b2-3b4d64be4498",
   "metadata": {},
   "source": [
    "concept dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52a7ff3-95c3-40b9-bc40-f3bab7f463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" extract key-value-list from @chatgpt-res \"\"\"\n",
    "extracted_chatgpt_res = []\n",
    "for j, row in enumerate(all_chatgpt_res):\n",
    "    # all_chatgpt_res[0][j] = \n",
    "    chatgpt_row_res = {}\n",
    "    extract_synsetid = lambda r: list(map(lambda x: x.strip('\"\\'').split(': ')[0], r))\n",
    "    remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "    extract_synnames = lambda r: list(map(lambda x: x.split(': ')[1].split('; '), r))\n",
    "    row = all_chatgpt_res[j]\n",
    "    row_data = remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n'))\n",
    "    synsets = extract_synsetid(row_data)\n",
    "    synnames = extract_synnames(row_data)\n",
    "    gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "    # start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "    # synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "    for idx_s, s in enumerate(synsets):\n",
    "        chatgpt_row_res.setdefault(s, [])\n",
    "        chatgpt_row_res[s] = remove_space(synnames[idx_s])\n",
    "    extracted_chatgpt_res.append(chatgpt_row_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb2574d-6cab-4d7e-b917-5175be71113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_chatgpt_synsets = {}\n",
    "for row in extracted_chatgpt_res:\n",
    "    for k, v in row.items():\n",
    "        extracted_chatgpt_synsets.setdefault(k, [])\n",
    "        extracted_chatgpt_synsets[k].extend(v)\n",
    "\n",
    "extracted_chatgpt_synsets_counter = {}\n",
    "for k, v in extracted_chatgpt_synsets.items():\n",
    "    extracted_chatgpt_synsets_counter.setdefault(k, Counter())\n",
    "    extracted_chatgpt_synsets_counter[k] = extracted_chatgpt_synsets_counter[k] + Counter(v)\n",
    "    assert len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9905a7d-abbb-45c8-9c3d-7a4ccae5f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "{\n",
    "    'synsets': extracted_chatgpt_synsets,\n",
    "    'synsets_counter': extracted_chatgpt_synsets_counter,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55146352-fdef-4afb-8b01-94adee43d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/inov-vocab-template_9-n_repeat=1-extract_synsets.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
