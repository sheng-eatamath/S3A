nohup: ignoring input
/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
===load nonjit===
missing keys:
['visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']
Model parameters: 150,408,193
Input resolution: 224
Context length: 77
Vocab size: 49408
==============================
make_nonliving26
dataset size 132767

  0%|          | 0/519 [00:00<?, ?it/s]
  0%|          | 0/519 [00:04<?, ?it/s]
Traceback (most recent call last):
  File "ablation-cl/sssa/ng.py", line 161, in <module>
    logits = model.visual.extract_features(images)
  File "/home/sheng/MUST/clip/model.py", line 611, in extract_features
    x = self.transformer(x)                          
  File "/home/sheng/sssa/nda3/envs/gcd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sheng/MUST/clip/model.py", line 494, in forward
    x = block(x, attn_mask=attn_mask)    
  File "/home/sheng/sssa/nda3/envs/gcd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sheng/sssa/clip/model.py", line 458, in forward
    attn_output, attn_output_weights = self.attention(self.ln_1(x), need_weights=save_attn, attn_mask=attn_mask)
  File "/home/sheng/MUST/clip/model.py", line 449, in attention
    return self.attn(x, x, x, need_weights=need_weights, attn_mask=attn_mask)
  File "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/torch/nn/functional.py", line 5310, in multi_head_attention_forward
    v = v.contiguous().view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
RuntimeError: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 1; 47.54 GiB total capacity; 1.22 GiB already allocated; 20.62 MiB free; 1.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
