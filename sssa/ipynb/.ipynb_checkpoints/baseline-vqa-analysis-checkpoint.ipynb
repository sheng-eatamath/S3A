{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0705be-f973-4ee8-921c-305648207653",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:2'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'imagenet'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 256\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_entity13/sssa/checkpoint-current.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word_L.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n",
    "\n",
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70595758-cc67-4fbc-9dae-887127f6fdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'sdogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18fe19c3-b685-4318-aa0b-b4aec245ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/vqav2-{args.dataset}-train.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)\n",
    "\n",
    "with open(f'./cache/vlm-{args.dataset}-scd.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "topk_all_clu_pred = data['topk_all_clu_pred']\n",
    "pred_kmeans_t = data['pred_kmeans_t']\n",
    "all_clu_pred = data['all_clu_pred']\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def most_frequent(strings):\n",
    "    counts = Counter(strings)\n",
    "    return max(counts, key=counts.get)\n",
    "\n",
    "c_pred = res['c_pred']\n",
    "c_gt = np.array([' '.join(x.split('_')) for x in res['c_gt']])\n",
    "for c in pred_kmeans_t.unique():\n",
    "    select = (pred_kmeans_t==c).numpy()\n",
    "    item = most_frequent(c_pred[select])\n",
    "    c_pred[select] = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2decbdb3-5724-4b68-a4ce-13a30d3ce0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluate import measure_similarity_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a0e0c1-fa6b-4697-85ae-7b9ce06d6262",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'nli-bert-large': 0.6270759340027968,\n",
       "  'all-mpnet-base-v2': 0.5302782058346396,\n",
       "  'all-MiniLM-L6-v2': 0.4501439915460845},\n",
       " {'nli-bert-large': 0.6239840738835434,\n",
       "  'all-mpnet-base-v2': 0.44662807002601523,\n",
       "  'all-MiniLM-L6-v2': 0.4349242237781485})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_similarity_bert(vocab, c_pred, c_gt, device=args.device, base=['nli-bert-large', 'all-mpnet-base-v2', 'all-MiniLM-L6-v2']), \\\n",
    "measure_similarity_bert(vocab, res['c_pred'], res['c_gt'], device=args.device, base=['nli-bert-large', 'all-mpnet-base-v2', 'all-MiniLM-L6-v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de159ea-ac38-497f-9f74-ee05ad960add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['chihuahua', 'chihuahua', 'chihuahua', ..., 'german shepherd',\n",
       "        'german shepherd', 'german shepherd'], dtype='<U21'),\n",
       " array(['chihuahua', 'chihuahua', 'chihuahua', ..., 'african hunting dog',\n",
       "        'african hunting dog', 'african hunting dog'], dtype='<U30'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_pred, c_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d336cb-a944-4618-8b79-906c4d9094c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Metric\n",
    "from torchmetrics.aggregation import BaseAggregator\n",
    "\n",
    "class SentenceScore(Metric):\n",
    "    \"\"\"Metric to evaluate the similarity between two sentences.\n",
    "\n",
    "    It takes as input a list of predicted sentences and a list of target sentences. The metric\n",
    "    computes the cosine similarity between the embeddings of the predicted sentences and the\n",
    "    target sentences.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to use. Defaults to\n",
    "            \"sentence-transformers/all-MiniLM-L6-v2\".\n",
    "        cache_dir (str): Path to BERT cache directory. Defaults to \".cache\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args,\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        cache_dir: str = \".cache\",\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_name = model_name\n",
    "        self.cache_dir = cache_dir\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir=self.cache_dir)\n",
    "        self._model = AutoModel.from_pretrained(self.model_name, cache_dir=self.cache_dir)\n",
    "\n",
    "        self.add_state(\"similarity\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def encode(self, sentence: str, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"Encode the input sentence with the BERT model.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): Input sentence.\n",
    "        \"\"\"\n",
    "        tokens = self._tokenizer.encode_plus(\n",
    "            sentence, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        embeddings = self._model(**tokens).last_hidden_state\n",
    "\n",
    "        # mask out padding tokens\n",
    "        mask = tokens[\"attention_mask\"].unsqueeze(-1).expand(embeddings.size()).float()\n",
    "        masked_embeddings = embeddings * mask\n",
    "\n",
    "        # sum over all tokens\n",
    "        summed = torch.sum(masked_embeddings, dim=1)\n",
    "        summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "\n",
    "        # normalise and remove batch dimension\n",
    "        embeddings = summed / summed_mask\n",
    "        embeddings = embeddings.squeeze(0)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def update(self, values, targets) -> None:\n",
    "        \"\"\"Update state with data.\n",
    "\n",
    "        Args:\n",
    "            values (list[str]): Predicted sentences.\n",
    "            targets (list[str]): Target sentences.\n",
    "        \"\"\"\n",
    "        values_z = []\n",
    "        targets_z = []\n",
    "        for value, target in zip(values, targets):\n",
    "            values_z.append(self.encode(value))\n",
    "            targets_z.append(self.encode(target))\n",
    "\n",
    "        values_z = torch.stack(values_z)\n",
    "        targets_z = torch.stack(targets_z)\n",
    "\n",
    "        # compute cosine similarity\n",
    "        similarity = torch.nn.functional.cosine_similarity(values_z, targets_z)\n",
    "\n",
    "        self.similarity += torch.sum(similarity)\n",
    "        self.total += len(values)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute the metric.\"\"\"\n",
    "        return self.similarity.float() / self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5902209-8099-4a7b-9f26-c34d98c1ef27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 162/132767 [00:25<5:48:09,  6.35it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "sscore = SentenceScore()\n",
    "sscore = sscore.to(args.device)\n",
    "with tqdm(total=len(c_pred)) as pbar:\n",
    "    for a, b in zip(c_pred, c_gt):\n",
    "        sscore.update(a, b)\n",
    "        pbar.update(1)\n",
    "sscore.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
