{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a280f04-9046-417e-89db-b8751f05ba97",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===load nonjit===\n",
      "missing keys:\n",
      "['visual.mask_token_embedding', 'visual.ln_patch.weight', 'visual.ln_patch.bias', 'visual.decoder.0.weight', 'visual.decoder.0.bias', 'visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']\n",
      "get_classifier in21k\n",
      "get_vocab in21k\n",
      "==============================\n",
      "make_entity13\n",
      "recover from epoch=29\n",
      "eval trainval\n",
      "TrainVal:  [  0/654]  eta: 1:20:17  acc1: 0.0000 (0.0000)  acc_top3: 0.0000 (0.0000)  acc_top5: 0.0137 (0.0137)  acc_top10: 0.1250 (0.1250)  ema_acc1: 0.0000 (0.0000)  acc_top3_ema: 0.0000 (0.0000)  acc_top5_ema: 0.0137 (0.0137)  acc_top10_ema: 0.1289 (0.1289)  time: 7.3664  data: 5.0494  max mem: 6184\n",
      "TrainVal:  [ 10/654]  eta: 0:23:34  acc1: 23.0824 (23.0824)  acc_top3: 0.3299 (0.3299)  acc_top5: 0.4197 (0.4197)  acc_top10: 0.5657 (0.5657)  ema_acc1: 23.0824 (23.0824)  acc_top3_ema: 0.3299 (0.3299)  acc_top5_ema: 0.4203 (0.4203)  acc_top10_ema: 0.5721 (0.5721)  time: 2.1961  data: 0.4598  max mem: 6224\n",
      "TrainVal:  [ 20/654]  eta: 0:20:40  acc1: 25.1758 (23.9769)  acc_top3: 0.3475 (0.3309)  acc_top5: 0.4382 (0.4180)  acc_top10: 0.5513 (0.5310)  ema_acc1: 25.2344 (24.0327)  acc_top3_ema: 0.3487 (0.3321)  acc_top5_ema: 0.4395 (0.4192)  acc_top10_ema: 0.5562 (0.5358)  time: 1.6867  data: 0.0009  max mem: 6224\n",
      "TrainVal:  [ 30/654]  eta: 0:19:30  acc1: 36.1816 (31.5335)  acc_top3: 0.4171 (0.3862)  acc_top5: 0.4640 (0.4483)  acc_top10: 0.5103 (0.5299)  ema_acc1: 36.1035 (31.4831)  acc_top3_ema: 0.4196 (0.3878)  acc_top5_ema: 0.4687 (0.4515)  acc_top10_ema: 0.5183 (0.5374)  time: 1.7000  data: 0.0012  max mem: 6224\n",
      "TrainVal:  [ 40/654]  eta: 0:18:47  acc1: 38.2715 (30.9499)  acc_top3: 0.4261 (0.3773)  acc_top5: 0.4453 (0.4313)  acc_top10: 0.4711 (0.5018)  ema_acc1: 38.6621 (31.1690)  acc_top3_ema: 0.4367 (0.3831)  acc_top5_ema: 0.4593 (0.4387)  acc_top10_ema: 0.4890 (0.5130)  time: 1.7104  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [ 50/654]  eta: 0:18:15  acc1: 14.7656 (24.9579)  acc_top3: 0.1977 (0.3122)  acc_top5: 0.2359 (0.3650)  acc_top10: 0.2925 (0.4368)  ema_acc1: 15.3320 (25.1494)  acc_top3_ema: 0.2134 (0.3194)  acc_top5_ema: 0.2562 (0.3749)  acc_top10_ema: 0.3171 (0.4510)  time: 1.7186  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [ 60/654]  eta: 0:17:49  acc1: 3.1348 (21.8302)  acc_top3: 0.1004 (0.2865)  acc_top5: 0.1646 (0.3439)  acc_top10: 0.2896 (0.4322)  ema_acc1: 3.1836 (21.9935)  acc_top3_ema: 0.1102 (0.2936)  acc_top5_ema: 0.1771 (0.3529)  acc_top10_ema: 0.3057 (0.4450)  time: 1.7240  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [ 70/654]  eta: 0:17:24  acc1: 27.3047 (25.6189)  acc_top3: 0.4382 (0.3477)  acc_top5: 0.4881 (0.3997)  acc_top10: 0.6151 (0.4870)  ema_acc1: 27.3145 (25.7592)  acc_top3_ema: 0.4418 (0.3539)  acc_top5_ema: 0.4911 (0.4076)  acc_top10_ema: 0.6202 (0.4987)  time: 1.7258  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [ 80/654]  eta: 0:17:02  acc1: 44.0820 (27.3245)  acc_top3: 0.5879 (0.3609)  acc_top5: 0.6738 (0.4253)  acc_top10: 0.8114 (0.5258)  ema_acc1: 44.0918 (27.4498)  acc_top3_ema: 0.5886 (0.3665)  acc_top5_ema: 0.6747 (0.4324)  acc_top10_ema: 0.8150 (0.5364)  time: 1.7267  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [ 90/654]  eta: 0:16:41  acc1: 55.8691 (32.2673)  acc_top3: 0.7224 (0.4301)  acc_top5: 0.8013 (0.4879)  acc_top10: 0.8987 (0.5775)  ema_acc1: 55.8496 (32.3725)  acc_top3_ema: 0.7226 (0.4349)  acc_top5_ema: 0.8013 (0.4941)  acc_top10_ema: 0.9001 (0.5869)  time: 1.7270  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [100/654]  eta: 0:16:21  acc1: 48.7207 (31.5613)  acc_top3: 0.7548 (0.4389)  acc_top5: 0.8143 (0.5024)  acc_top10: 0.8825 (0.5965)  ema_acc1: 48.6719 (31.6522)  acc_top3_ema: 0.7571 (0.4438)  acc_top5_ema: 0.8200 (0.5091)  acc_top10_ema: 0.8873 (0.6059)  time: 1.7276  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [110/654]  eta: 0:16:01  acc1: 37.8320 (33.2700)  acc_top3: 0.6183 (0.4640)  acc_top5: 0.7030 (0.5267)  acc_top10: 0.7942 (0.6166)  ema_acc1: 37.8027 (33.3509)  acc_top3_ema: 0.6241 (0.4690)  acc_top5_ema: 0.7132 (0.5336)  acc_top10_ema: 0.8073 (0.6266)  time: 1.7300  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [120/654]  eta: 0:15:42  acc1: 62.3145 (36.6445)  acc_top3: 0.8267 (0.5030)  acc_top5: 0.8749 (0.5639)  acc_top10: 0.9051 (0.6475)  ema_acc1: 62.2754 (36.7139)  acc_top3_ema: 0.8304 (0.5077)  acc_top5_ema: 0.8794 (0.5703)  acc_top10_ema: 0.9131 (0.6566)  time: 1.7329  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [130/654]  eta: 0:15:23  acc1: 85.0488 (41.1752)  acc_top3: 0.9644 (0.5404)  acc_top5: 0.9861 (0.5968)  acc_top10: 0.9937 (0.6742)  ema_acc1: 84.9902 (41.2348)  acc_top3_ema: 0.9649 (0.5447)  acc_top5_ema: 0.9866 (0.6028)  acc_top10_ema: 0.9934 (0.6826)  time: 1.7350  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [140/654]  eta: 0:15:04  acc1: 96.5625 (45.1435)  acc_top3: 0.9912 (0.5723)  acc_top5: 0.9946 (0.6250)  acc_top10: 0.9967 (0.6970)  ema_acc1: 96.5723 (45.2045)  acc_top3_ema: 0.9916 (0.5764)  acc_top5_ema: 0.9947 (0.6305)  acc_top10_ema: 0.9965 (0.7048)  time: 1.7342  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [150/654]  eta: 0:14:46  acc1: 75.5176 (45.7238)  acc_top3: 0.9726 (0.5976)  acc_top5: 0.9917 (0.6491)  acc_top10: 0.9961 (0.7168)  ema_acc1: 75.5566 (45.7807)  acc_top3_ema: 0.9740 (0.6016)  acc_top5_ema: 0.9916 (0.6543)  acc_top10_ema: 0.9960 (0.7241)  time: 1.7347  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [160/654]  eta: 0:14:28  acc1: 39.5215 (44.4451)  acc_top3: 0.6426 (0.5810)  acc_top5: 0.7024 (0.6346)  acc_top10: 0.7719 (0.7063)  ema_acc1: 39.5117 (44.4973)  acc_top3_ema: 0.6451 (0.5849)  acc_top5_ema: 0.7056 (0.6399)  acc_top10_ema: 0.7782 (0.7140)  time: 1.7364  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [170/654]  eta: 0:14:09  acc1: 12.5684 (41.8460)  acc_top3: 0.2334 (0.5550)  acc_top5: 0.2887 (0.6070)  acc_top10: 0.3730 (0.6766)  ema_acc1: 12.5586 (41.8951)  acc_top3_ema: 0.2365 (0.5589)  acc_top5_ema: 0.2936 (0.6121)  acc_top10_ema: 0.3834 (0.6843)  time: 1.7341  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [180/654]  eta: 0:13:51  acc1: 11.1719 (40.7685)  acc_top3: 0.3481 (0.5553)  acc_top5: 0.4081 (0.6096)  acc_top10: 0.4658 (0.6797)  ema_acc1: 11.1816 (40.8160)  acc_top3_ema: 0.3494 (0.5589)  acc_top5_ema: 0.4090 (0.6143)  acc_top10_ema: 0.4678 (0.6868)  time: 1.7333  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [190/654]  eta: 0:13:33  acc1: 39.6875 (41.6200)  acc_top3: 0.6911 (0.5693)  acc_top5: 0.7697 (0.6240)  acc_top10: 0.8322 (0.6929)  ema_acc1: 39.8047 (41.6762)  acc_top3_ema: 0.6905 (0.5727)  acc_top5_ema: 0.7685 (0.6285)  acc_top10_ema: 0.8310 (0.6996)  time: 1.7343  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [200/654]  eta: 0:13:15  acc1: 68.5156 (43.5294)  acc_top3: 0.8564 (0.5852)  acc_top5: 0.9024 (0.6387)  acc_top10: 0.9386 (0.7055)  ema_acc1: 68.5840 (43.5790)  acc_top3_ema: 0.8574 (0.5886)  acc_top5_ema: 0.9025 (0.6430)  acc_top10_ema: 0.9396 (0.7119)  time: 1.7326  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [210/654]  eta: 0:12:57  acc1: 85.6738 (45.7957)  acc_top3: 0.9248 (0.6030)  acc_top5: 0.9427 (0.6542)  acc_top10: 0.9587 (0.7181)  ema_acc1: 85.6543 (45.8447)  acc_top3_ema: 0.9266 (0.6062)  acc_top5_ema: 0.9444 (0.6584)  acc_top10_ema: 0.9603 (0.7243)  time: 1.7306  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [220/654]  eta: 0:12:39  acc1: 76.0742 (46.4747)  acc_top3: 0.8429 (0.6086)  acc_top5: 0.8481 (0.6577)  acc_top10: 0.8539 (0.7189)  ema_acc1: 75.9961 (46.5127)  acc_top3_ema: 0.8438 (0.6117)  acc_top5_ema: 0.8493 (0.6617)  acc_top10_ema: 0.8558 (0.7249)  time: 1.7304  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [230/654]  eta: 0:12:21  acc1: 62.4316 (47.2360)  acc_top3: 0.7803 (0.6183)  acc_top5: 0.8137 (0.6680)  acc_top10: 0.8442 (0.7290)  ema_acc1: 62.2754 (47.2673)  acc_top3_ema: 0.7790 (0.6212)  acc_top5_ema: 0.8140 (0.6719)  acc_top10_ema: 0.8451 (0.7348)  time: 1.7322  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [240/654]  eta: 0:12:04  acc1: 55.0195 (47.1838)  acc_top3: 0.7127 (0.6172)  acc_top5: 0.8104 (0.6704)  acc_top10: 0.8988 (0.7338)  ema_acc1: 54.8730 (47.2065)  acc_top3_ema: 0.7192 (0.6206)  acc_top5_ema: 0.8198 (0.6748)  acc_top10_ema: 0.9074 (0.7401)  time: 1.7315  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [250/654]  eta: 0:11:46  acc1: 35.9082 (46.3334)  acc_top3: 0.4559 (0.6054)  acc_top5: 0.5384 (0.6577)  acc_top10: 0.6273 (0.7209)  ema_acc1: 35.8398 (46.3568)  acc_top3_ema: 0.4669 (0.6089)  acc_top5_ema: 0.5528 (0.6624)  acc_top10_ema: 0.6396 (0.7272)  time: 1.7295  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [260/654]  eta: 0:11:28  acc1: 34.5996 (46.2195)  acc_top3: 0.3919 (0.5999)  acc_top5: 0.4122 (0.6506)  acc_top10: 0.4461 (0.7118)  ema_acc1: 34.6191 (46.2419)  acc_top3_ema: 0.3954 (0.6033)  acc_top5_ema: 0.4180 (0.6551)  acc_top10_ema: 0.4498 (0.7178)  time: 1.7296  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [270/654]  eta: 0:11:10  acc1: 48.1543 (46.4678)  acc_top3: 0.5329 (0.6000)  acc_top5: 0.5489 (0.6497)  acc_top10: 0.5676 (0.7096)  ema_acc1: 48.2129 (46.4937)  acc_top3_ema: 0.5380 (0.6036)  acc_top5_ema: 0.5526 (0.6543)  acc_top10_ema: 0.5731 (0.7158)  time: 1.7304  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [280/654]  eta: 0:10:53  acc1: 46.6992 (46.2536)  acc_top3: 0.5518 (0.5965)  acc_top5: 0.5917 (0.6464)  acc_top10: 0.6414 (0.7068)  ema_acc1: 46.8555 (46.2856)  acc_top3_ema: 0.5590 (0.6002)  acc_top5_ema: 0.6006 (0.6513)  acc_top10_ema: 0.6545 (0.7133)  time: 1.7285  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [290/654]  eta: 0:10:35  acc1: 31.6797 (45.4514)  acc_top3: 0.4440 (0.5893)  acc_top5: 0.5246 (0.6411)  acc_top10: 0.5886 (0.7013)  ema_acc1: 31.9336 (45.4930)  acc_top3_ema: 0.4516 (0.5932)  acc_top5_ema: 0.5349 (0.6461)  acc_top10_ema: 0.5991 (0.7078)  time: 1.7278  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [300/654]  eta: 0:10:17  acc1: 46.6016 (46.2767)  acc_top3: 0.5642 (0.5944)  acc_top5: 0.6242 (0.6449)  acc_top10: 0.6650 (0.7040)  ema_acc1: 47.2461 (46.3494)  acc_top3_ema: 0.5748 (0.5985)  acc_top5_ema: 0.6348 (0.6502)  acc_top10_ema: 0.6748 (0.7108)  time: 1.7286  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [310/654]  eta: 0:10:00  acc1: 45.2637 (45.4394)  acc_top3: 0.6187 (0.5912)  acc_top5: 0.7037 (0.6451)  acc_top10: 0.7827 (0.7065)  ema_acc1: 45.8691 (45.5172)  acc_top3_ema: 0.6277 (0.5954)  acc_top5_ema: 0.7148 (0.6505)  acc_top10_ema: 0.7942 (0.7134)  time: 1.7296  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [320/654]  eta: 0:09:42  acc1: 28.7402 (45.1841)  acc_top3: 0.4676 (0.5865)  acc_top5: 0.5732 (0.6405)  acc_top10: 0.7772 (0.7086)  ema_acc1: 29.1602 (45.2784)  acc_top3_ema: 0.4741 (0.5908)  acc_top5_ema: 0.5842 (0.6460)  acc_top10_ema: 0.7890 (0.7156)  time: 1.7296  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [330/654]  eta: 0:09:24  acc1: 53.7305 (45.9403)  acc_top3: 0.5800 (0.5905)  acc_top5: 0.6113 (0.6431)  acc_top10: 0.7522 (0.7093)  ema_acc1: 54.0332 (46.0318)  acc_top3_ema: 0.5834 (0.5947)  acc_top5_ema: 0.6167 (0.6485)  acc_top10_ema: 0.7594 (0.7161)  time: 1.7296  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [340/654]  eta: 0:09:07  acc1: 49.0137 (45.4087)  acc_top3: 0.5337 (0.5834)  acc_top5: 0.5523 (0.6353)  acc_top10: 0.5769 (0.7009)  ema_acc1: 49.3750 (45.5187)  acc_top3_ema: 0.5434 (0.5880)  acc_top5_ema: 0.5615 (0.6411)  acc_top10_ema: 0.5859 (0.7080)  time: 1.7298  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [350/654]  eta: 0:08:49  acc1: 26.4844 (44.8317)  acc_top3: 0.5274 (0.5869)  acc_top5: 0.6001 (0.6406)  acc_top10: 0.6687 (0.7070)  ema_acc1: 26.8945 (44.9414)  acc_top3_ema: 0.5408 (0.5916)  acc_top5_ema: 0.6135 (0.6465)  acc_top10_ema: 0.6792 (0.7140)  time: 1.7298  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [360/654]  eta: 0:08:32  acc1: 25.9766 (44.3322)  acc_top3: 0.5172 (0.5797)  acc_top5: 0.5836 (0.6324)  acc_top10: 0.6432 (0.6977)  ema_acc1: 26.2012 (44.4485)  acc_top3_ema: 0.5242 (0.5844)  acc_top5_ema: 0.5901 (0.6383)  acc_top10_ema: 0.6490 (0.7048)  time: 1.7292  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [370/654]  eta: 0:08:14  acc1: 13.4766 (43.1414)  acc_top3: 0.1707 (0.5645)  acc_top5: 0.1843 (0.6160)  acc_top10: 0.2045 (0.6799)  ema_acc1: 13.7012 (43.2572)  acc_top3_ema: 0.1745 (0.5691)  acc_top5_ema: 0.1879 (0.6217)  acc_top10_ema: 0.2107 (0.6869)  time: 1.7294  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [380/654]  eta: 0:07:57  acc1: 30.8789 (43.6259)  acc_top3: 0.3371 (0.5670)  acc_top5: 0.3489 (0.6175)  acc_top10: 0.3678 (0.6803)  ema_acc1: 31.1621 (43.7510)  acc_top3_ema: 0.3411 (0.5717)  acc_top5_ema: 0.3517 (0.6232)  acc_top10_ema: 0.3729 (0.6873)  time: 1.7311  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [390/654]  eta: 0:07:39  acc1: 42.7441 (43.1211)  acc_top3: 0.4869 (0.5605)  acc_top5: 0.5101 (0.6106)  acc_top10: 0.5467 (0.6731)  ema_acc1: 43.0273 (43.2455)  acc_top3_ema: 0.4957 (0.5654)  acc_top5_ema: 0.5189 (0.6165)  acc_top10_ema: 0.5565 (0.6802)  time: 1.7315  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [400/654]  eta: 0:07:22  acc1: 41.9336 (43.5415)  acc_top3: 0.4882 (0.5630)  acc_top5: 0.5137 (0.6124)  acc_top10: 0.5504 (0.6739)  ema_acc1: 42.2656 (43.6769)  acc_top3_ema: 0.4992 (0.5681)  acc_top5_ema: 0.5266 (0.6184)  acc_top10_ema: 0.5630 (0.6811)  time: 1.7328  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [410/654]  eta: 0:07:04  acc1: 51.7383 (43.5404)  acc_top3: 0.5664 (0.5608)  acc_top5: 0.5846 (0.6093)  acc_top10: 0.6086 (0.6699)  ema_acc1: 52.1094 (43.6768)  acc_top3_ema: 0.5736 (0.5658)  acc_top5_ema: 0.5917 (0.6153)  acc_top10_ema: 0.6173 (0.6772)  time: 1.7316  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [420/654]  eta: 0:06:47  acc1: 30.4883 (42.9214)  acc_top3: 0.4087 (0.5557)  acc_top5: 0.4310 (0.6037)  acc_top10: 0.4613 (0.6638)  ema_acc1: 30.4590 (43.0490)  acc_top3_ema: 0.4125 (0.5607)  acc_top5_ema: 0.4360 (0.6097)  acc_top10_ema: 0.4684 (0.6710)  time: 1.7311  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [430/654]  eta: 0:06:29  acc1: 21.2695 (42.5070)  acc_top3: 0.4106 (0.5538)  acc_top5: 0.4768 (0.6032)  acc_top10: 0.5509 (0.6644)  ema_acc1: 21.1523 (42.6316)  acc_top3_ema: 0.4185 (0.5589)  acc_top5_ema: 0.4872 (0.6093)  acc_top10_ema: 0.5579 (0.6716)  time: 1.7316  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [440/654]  eta: 0:06:12  acc1: 29.8340 (42.3279)  acc_top3: 0.5163 (0.5539)  acc_top5: 0.6193 (0.6045)  acc_top10: 0.7180 (0.6662)  ema_acc1: 30.5469 (42.4820)  acc_top3_ema: 0.5303 (0.5593)  acc_top5_ema: 0.6359 (0.6109)  acc_top10_ema: 0.7257 (0.6735)  time: 1.7321  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [450/654]  eta: 0:05:55  acc1: 49.2285 (42.8051)  acc_top3: 0.6154 (0.5566)  acc_top5: 0.6700 (0.6061)  acc_top10: 0.7194 (0.6668)  ema_acc1: 50.0098 (42.9588)  acc_top3_ema: 0.6250 (0.5619)  acc_top5_ema: 0.6810 (0.6125)  acc_top10_ema: 0.7268 (0.6741)  time: 1.7314  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [460/654]  eta: 0:05:37  acc1: 54.4238 (42.8527)  acc_top3: 0.5755 (0.5548)  acc_top5: 0.5824 (0.6035)  acc_top10: 0.5976 (0.6632)  ema_acc1: 54.5898 (43.0073)  acc_top3_ema: 0.5789 (0.5601)  acc_top5_ema: 0.5864 (0.6099)  acc_top10_ema: 0.6045 (0.6705)  time: 1.7301  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [470/654]  eta: 0:05:20  acc1: 22.6074 (41.9474)  acc_top3: 0.2552 (0.5438)  acc_top5: 0.2693 (0.5918)  acc_top10: 0.2957 (0.6511)  ema_acc1: 22.7734 (42.1017)  acc_top3_ema: 0.2599 (0.5490)  acc_top5_ema: 0.2771 (0.5983)  acc_top10_ema: 0.3076 (0.6585)  time: 1.7311  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [480/654]  eta: 0:05:02  acc1: 20.8789 (41.9390)  acc_top3: 0.2940 (0.5440)  acc_top5: 0.3253 (0.5919)  acc_top10: 0.3619 (0.6507)  ema_acc1: 21.5625 (42.1156)  acc_top3_ema: 0.3064 (0.5496)  acc_top5_ema: 0.3409 (0.5987)  acc_top10_ema: 0.3809 (0.6585)  time: 1.7299  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [490/654]  eta: 0:04:45  acc1: 33.3496 (41.5972)  acc_top3: 0.4226 (0.5388)  acc_top5: 0.4532 (0.5862)  acc_top10: 0.4840 (0.6443)  ema_acc1: 34.0430 (41.7734)  acc_top3_ema: 0.4358 (0.5444)  acc_top5_ema: 0.4682 (0.5930)  acc_top10_ema: 0.5015 (0.6521)  time: 1.7293  data: 0.0014  max mem: 6224\n",
      "TrainVal:  [500/654]  eta: 0:04:27  acc1: 22.9590 (41.1813)  acc_top3: 0.2521 (0.5323)  acc_top5: 0.2640 (0.5788)  acc_top10: 0.2824 (0.6360)  ema_acc1: 23.1152 (41.3571)  acc_top3_ema: 0.2556 (0.5379)  acc_top5_ema: 0.2687 (0.5855)  acc_top10_ema: 0.2913 (0.6438)  time: 1.7297  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [510/654]  eta: 0:04:10  acc1: 30.5566 (41.1651)  acc_top3: 0.4310 (0.5346)  acc_top5: 0.4568 (0.5811)  acc_top10: 0.4824 (0.6379)  ema_acc1: 30.7227 (41.3409)  acc_top3_ema: 0.4346 (0.5401)  acc_top5_ema: 0.4608 (0.5878)  acc_top10_ema: 0.4895 (0.6458)  time: 1.7313  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [520/654]  eta: 0:03:53  acc1: 32.0996 (40.8327)  acc_top3: 0.5795 (0.5342)  acc_top5: 0.6357 (0.5810)  acc_top10: 0.7034 (0.6386)  ema_acc1: 32.2852 (41.0089)  acc_top3_ema: 0.5836 (0.5396)  acc_top5_ema: 0.6422 (0.5877)  acc_top10_ema: 0.7132 (0.6465)  time: 1.7315  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [530/654]  eta: 0:03:35  acc1: 24.2969 (40.5297)  acc_top3: 0.4301 (0.5307)  acc_top5: 0.5173 (0.5787)  acc_top10: 0.5903 (0.6361)  ema_acc1: 24.3945 (40.7026)  acc_top3_ema: 0.4331 (0.5361)  acc_top5_ema: 0.5206 (0.5853)  acc_top10_ema: 0.5961 (0.6439)  time: 1.7302  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [540/654]  eta: 0:03:18  acc1: 31.9141 (40.5030)  acc_top3: 0.4054 (0.5294)  acc_top5: 0.4813 (0.5773)  acc_top10: 0.5371 (0.6348)  ema_acc1: 32.1387 (40.6810)  acc_top3_ema: 0.4104 (0.5348)  acc_top5_ema: 0.4841 (0.5839)  acc_top10_ema: 0.5391 (0.6425)  time: 1.7292  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [550/654]  eta: 0:03:00  acc1: 30.0000 (40.1475)  acc_top3: 0.3585 (0.5244)  acc_top5: 0.3896 (0.5719)  acc_top10: 0.4427 (0.6291)  ema_acc1: 30.4492 (40.3304)  acc_top3_ema: 0.3651 (0.5299)  acc_top5_ema: 0.3985 (0.5785)  acc_top10_ema: 0.4529 (0.6370)  time: 1.7301  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [560/654]  eta: 0:02:43  acc1: 31.9824 (40.1992)  acc_top3: 0.3714 (0.5238)  acc_top5: 0.3941 (0.5708)  acc_top10: 0.4372 (0.6278)  ema_acc1: 32.5684 (40.3917)  acc_top3_ema: 0.3810 (0.5293)  acc_top5_ema: 0.4086 (0.5776)  acc_top10_ema: 0.4538 (0.6358)  time: 1.7316  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [570/654]  eta: 0:02:26  acc1: 49.0625 (40.4598)  acc_top3: 0.6392 (0.5284)  acc_top5: 0.6622 (0.5750)  acc_top10: 0.6931 (0.6314)  ema_acc1: 49.5117 (40.6520)  acc_top3_ema: 0.6475 (0.5340)  acc_top5_ema: 0.6723 (0.5818)  acc_top10_ema: 0.7041 (0.6393)  time: 1.7303  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [580/654]  eta: 0:02:08  acc1: 40.1855 (40.1987)  acc_top3: 0.6140 (0.5269)  acc_top5: 0.6321 (0.5729)  acc_top10: 0.6527 (0.6287)  ema_acc1: 40.3320 (40.3897)  acc_top3_ema: 0.6201 (0.5325)  acc_top5_ema: 0.6374 (0.5797)  acc_top10_ema: 0.6595 (0.6366)  time: 1.7311  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [590/654]  eta: 0:01:51  acc1: 25.8789 (39.9664)  acc_top3: 0.3609 (0.5228)  acc_top5: 0.3757 (0.5683)  acc_top10: 0.4010 (0.6236)  ema_acc1: 25.9863 (40.1557)  acc_top3_ema: 0.3666 (0.5284)  acc_top5_ema: 0.3802 (0.5750)  acc_top10_ema: 0.4088 (0.6315)  time: 1.7304  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [600/654]  eta: 0:01:33  acc1: 16.7090 (39.4171)  acc_top3: 0.2736 (0.5184)  acc_top5: 0.3033 (0.5639)  acc_top10: 0.3442 (0.6192)  ema_acc1: 16.9922 (39.6111)  acc_top3_ema: 0.2836 (0.5242)  acc_top5_ema: 0.3146 (0.5708)  acc_top10_ema: 0.3583 (0.6273)  time: 1.7301  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [610/654]  eta: 0:01:16  acc1: 25.7422 (39.5008)  acc_top3: 0.4033 (0.5189)  acc_top5: 0.4605 (0.5648)  acc_top10: 0.5373 (0.6207)  ema_acc1: 26.1230 (39.6964)  acc_top3_ema: 0.4151 (0.5246)  acc_top5_ema: 0.4776 (0.5718)  acc_top10_ema: 0.5549 (0.6290)  time: 1.7302  data: 0.0014  max mem: 6224\n",
      "TrainVal:  [620/654]  eta: 0:00:59  acc1: 32.9297 (39.2081)  acc_top3: 0.4252 (0.5154)  acc_top5: 0.4877 (0.5615)  acc_top10: 0.5732 (0.6177)  ema_acc1: 33.1152 (39.4019)  acc_top3_ema: 0.4287 (0.5211)  acc_top5_ema: 0.4957 (0.5684)  acc_top10_ema: 0.5819 (0.6259)  time: 1.7316  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [630/654]  eta: 0:00:41  acc1: 32.9980 (39.2946)  acc_top3: 0.3935 (0.5149)  acc_top5: 0.4371 (0.5607)  acc_top10: 0.5006 (0.6169)  ema_acc1: 33.1738 (39.4896)  acc_top3_ema: 0.3963 (0.5206)  acc_top5_ema: 0.4427 (0.5677)  acc_top10_ema: 0.5081 (0.6252)  time: 1.7327  data: 0.0016  max mem: 6224\n",
      "TrainVal:  [640/654]  eta: 0:00:24  acc1: 30.2148 (38.9275)  acc_top3: 0.3465 (0.5102)  acc_top5: 0.3766 (0.5557)  acc_top10: 0.4276 (0.6118)  ema_acc1: 30.4883 (39.1237)  acc_top3_ema: 0.3527 (0.5159)  acc_top5_ema: 0.3856 (0.5627)  acc_top10_ema: 0.4421 (0.6201)  time: 1.7303  data: 0.0015  max mem: 6224\n",
      "TrainVal:  [650/654]  eta: 0:00:06  acc1: 17.3047 (38.6191)  acc_top3: 0.2307 (0.5062)  acc_top5: 0.2582 (0.5514)  acc_top10: 0.3060 (0.6074)  ema_acc1: 17.5684 (38.8162)  acc_top3_ema: 0.2397 (0.5119)  acc_top5_ema: 0.2704 (0.5585)  acc_top10_ema: 0.3249 (0.6159)  time: 1.7277  data: 0.0014  max mem: 6224\n",
      "TrainVal:  [653/654]  eta: 0:00:01  acc1: 17.2461 (38.4569)  acc_top3: 0.2265 (0.5040)  acc_top5: 0.2509 (0.5491)  acc_top10: 0.2910 (0.6048)  ema_acc1: 17.4805 (38.6531)  acc_top3_ema: 0.2343 (0.5098)  acc_top5_ema: 0.2616 (0.5562)  acc_top10_ema: 0.3069 (0.6134)  time: 1.7061  data: 0.0014  max mem: 6224\n",
      "TrainVal: Total time: 0:18:55 (1.7369 s / it)\n",
      "* Acc@1 38.457\n",
      "* Acc@3 0.504\n",
      "* Acc@5 0.549\n",
      "* Acc@10 0.605\n",
      "* Missing Label 57\n",
      "evaluate:: cluster evaluate\n",
      "* acc_clu 0.5825\n",
      "* acc_clu_ema 0.5845\n",
      "evaluate:: tree evaluate\n",
      "* tree_sim_path 0.5007 0.5034\n",
      "* tree_sim_lch 2.4276 2.4392\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "from ema import ModelEma\n",
    "import model as clip\n",
    "from model import clip_classifier_oszsl\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "from data.vocab import get_vocab\n",
    "from data.build_dataset import build_transform\n",
    "from engine_self_training_must import evaluate\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:0'\n",
    "    arch = 'ViT-L/14'\n",
    "    dataset = 'make_nonliving26'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word_L.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "    clip_model = 'ViT-L/14'\n",
    "    mask = True\n",
    "    vocab_name = 'in21k-L'\n",
    "    resume_ckpt = ''\n",
    "    model_ema_decay = 0.9998\n",
    "    devices = None\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "### model\n",
    "model = clip_classifier_oszsl(args)\n",
    "args.nb_classes = model.num_classes\n",
    "model_ema = ModelEma(\n",
    "    model,\n",
    "    decay=args.model_ema_decay,\n",
    "    resume='',\n",
    "    device=args.device,\n",
    "    )\n",
    "### vocab\n",
    "vocab = get_vocab(args.vocab_name)\n",
    "args.num_voc = len(vocab)\n",
    "\n",
    "args.image_mean=(0.48145466, 0.4578275, 0.40821073)\n",
    "args.image_std=(0.26862954, 0.26130258, 0.27577711)\n",
    "args.input_size=224\n",
    "\n",
    "for dataset_name in [\n",
    "    # 'sdogs', 'imagenet', 'sdogs', 'make_living17', 'make_nonliving26', \n",
    "    'make_entity13', \n",
    "    # 'make_entity30', \n",
    "    # 'imagenet1k'\n",
    "]:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    # load checkpoint\n",
    "    args.resume_ckpt = f'/home/sheng/MUST-output/{args.dataset}/must-L-1/checkpoint-current.pth'\n",
    "    ckpt = torch.load(f'{args.resume_ckpt}', map_location='cpu')\n",
    "    epoch_resume = ckpt['epoch']\n",
    "    assert epoch_resume in [28, 29], epoch_resume\n",
    "    model_resume = ckpt['model']\n",
    "    model_ema_resume = ckpt['model_ema']\n",
    "    model.load_state_dict(model_resume)\n",
    "    model.train()\n",
    "    model_ema.ema.load_state_dict(model_ema_resume)\n",
    "    model_ema.ema.eval()\n",
    "    print(f'recover from epoch={epoch_resume}')\n",
    "    model = model.to(args.device)\n",
    "    model_ema.ema = model_ema.ema.to(args.device)\n",
    "    \n",
    "    \n",
    "    transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "    dataset_trainval = get_datasets_oszsl(args, vocab=vocab, is_train=True, transform=transform_val)\n",
    "    dataset_val = get_datasets_oszsl(args, vocab=vocab, is_train=False, transform=transform_val)\n",
    "    data_loader_trainval = torch.utils.data.DataLoader(\n",
    "        dataset_trainval,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    print('eval trainval')\n",
    "    trainval_stats = evaluate(data_loader_trainval, model, args.device, model_ema=model_ema, args=args, \n",
    "                          other_params={\n",
    "                              'vocab': vocab,\n",
    "                              'type': 'trainval',\n",
    "                          }, \n",
    "                          log_writer=None,\n",
    "                          )\n",
    "    pprint(trainval_stats, compact=True)\n",
    "    print('eval test')\n",
    "    test_stats = evaluate(data_loader_val, model, args.device, model_ema=model_ema, args=args, \n",
    "                          other_params={\n",
    "                              'vocab': vocab,\n",
    "                              'type': 'test',\n",
    "                          }, \n",
    "                          log_writer=None,\n",
    "                          )\n",
    "    pprint(test_stats, compact=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e97f0f-e714-4244-a205-b4d05b569b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b82320bf-2a8f-44db-ba34-2a4835632cf1",
   "metadata": {},
   "source": [
    "==============================\n",
    "sdogs\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 0:01:16 (1.6194 s / it)\n",
    "* Acc@1 57.200\n",
    "* Acc@3 0.727\n",
    "* Acc@5 0.769\n",
    "* Acc@10 0.811\n",
    "* Missing Label 14\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.6061\n",
    "* acc_clu_ema 0.6164\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.6701 0.6854\n",
    "* tree_sim_lch 2.9496 2.9982\n",
    "* tree_sim_wup 0.7942 0.8028\n",
    "{'acc1': 57.20000004069011,\n",
    " 'acc_clu': 0.6060833333333333,\n",
    " 'acc_clu_ema': 0.6164166666666666,\n",
    " 'acc_top10': 0.8114166668256124,\n",
    " 'acc_top10_ema': 0.8790833331743876,\n",
    " 'acc_top3': 0.7265833328564961,\n",
    " 'acc_top3_ema': 0.7712500001589457,\n",
    " 'acc_top5': 0.7692500001589457,\n",
    " 'acc_top5_ema': 0.8288333336512248,\n",
    " 'ema_acc1': 58.733333455403645,\n",
    " 'n_missing_labels': 14,\n",
    " 'tree_sim_lch': 2.9496046441939208,\n",
    " 'tree_sim_lch_ema': 2.998228821186231,\n",
    " 'tree_sim_path': 0.6701144459465407,\n",
    " 'tree_sim_path_ema': 0.6854007181759695,\n",
    " 'tree_sim_wup': 0.7942226920699609,\n",
    " 'tree_sim_wup_ema': 0.8027541868922187}\n",
    "TrainVal: Total time: 0:01:12 (2.1386 s / it)\n",
    "* Acc@1 57.890\n",
    "* Acc@3 0.741\n",
    "* Acc@5 0.786\n",
    "* Acc@10 0.827\n",
    "* Missing Label 18\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.6364\n",
    "* acc_clu_ema 0.6388\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.6766 0.6863\n",
    "* tree_sim_lch 2.9690 3.0004\n",
    "* tree_sim_wup 0.7920 0.7985\n",
    "{'acc1': 57.89044291178386,\n",
    " 'acc_clu': 0.6363636363636364,\n",
    " 'acc_clu_ema': 0.6388111888111888,\n",
    " 'acc_top10': 0.826923076534049,\n",
    " 'acc_top10_ema': 0.8899766904212933,\n",
    " 'acc_top3': 0.7411421915034314,\n",
    " 'acc_top3_ema': 0.7818181814013662,\n",
    " 'acc_top5': 0.7863636362802732,\n",
    " 'acc_top5_ema': 0.8427738928572559,\n",
    " 'ema_acc1': 58.881118886454125,\n",
    " 'n_missing_labels': 18,\n",
    " 'tree_sim_lch': 2.9689878737037354,\n",
    " 'tree_sim_lch_ema': 3.000434720490624,\n",
    " 'tree_sim_path': 0.6766170087349189,\n",
    " 'tree_sim_path_ema': 0.6863059465563227,\n",
    " 'tree_sim_wup': 0.7919527247644691,\n",
    " 'tree_sim_wup_ema': 0.7984846726836351}\n",
    "==============================\n",
    "imagenet\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 0:12:27 (1.5034 s / it)\n",
    "* Acc@1 33.369\n",
    "* Acc@3 0.478\n",
    "* Acc@5 0.518\n",
    "* Acc@10 0.570\n",
    "* Missing Label 25\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.5141\n",
    "* acc_clu_ema 0.5168\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4519 0.4549\n",
    "* tree_sim_lch 2.2979 2.3111\n",
    "* tree_sim_wup 0.7302 0.7343\n",
    "{'acc1': 33.3689965776018,\n",
    " 'acc_clu': 0.5140620697793337,\n",
    " 'acc_clu_ema': 0.5168076151516343,\n",
    " 'acc_top10': 0.5704204853774008,\n",
    " 'acc_top10_ema': 0.5772961491464288,\n",
    " 'acc_top3': 0.47824410963821845,\n",
    " 'acc_top3_ema': 0.482830507834618,\n",
    " 'acc_top5': 0.5177201746581358,\n",
    " 'acc_top5_ema': 0.5232899343395845,\n",
    " 'ema_acc1': 33.57353577601264,\n",
    " 'n_missing_labels': 25,\n",
    " 'tree_sim_lch': 2.297879023614412,\n",
    " 'tree_sim_lch_ema': 2.3110766154938744,\n",
    " 'tree_sim_path': 0.4519097445440365,\n",
    " 'tree_sim_path_ema': 0.4549276884060611,\n",
    " 'tree_sim_wup': 0.7301741242309724,\n",
    " 'tree_sim_wup_ema': 0.734327941491266}\n",
    "TrainVal: Total time: 0:00:41 (2.0971 s / it)\n",
    "* Acc@1 33.720\n",
    "* Acc@3 0.477\n",
    "* Acc@5 0.514\n",
    "* Acc@10 0.568\n",
    "* Missing Label 47\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.5256\n",
    "* acc_clu_ema 0.5304\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4546 0.4571\n",
    "* tree_sim_lch 2.3005 2.3107\n",
    "* tree_sim_wup 0.7280 0.7309\n",
    "{'acc1': 33.72000002441406,\n",
    " 'acc_clu': 0.5256,\n",
    " 'acc_clu_ema': 0.5304,\n",
    " 'acc_top10': 0.5677999999046326,\n",
    " 'acc_top10_ema': 0.5735999999046326,\n",
    " 'acc_top3': 0.4772000002384186,\n",
    " 'acc_top3_ema': 0.4816000002384186,\n",
    " 'acc_top5': 0.5140000002384186,\n",
    " 'acc_top5_ema': 0.5203999996185302,\n",
    " 'ema_acc1': 33.89999999694824,\n",
    " 'n_missing_labels': 47,\n",
    " 'tree_sim_lch': 2.3005113979188976,\n",
    " 'tree_sim_lch_ema': 2.310654005897294,\n",
    " 'tree_sim_path': 0.45456661877974114,\n",
    " 'tree_sim_path_ema': 0.4570769690967787,\n",
    " 'tree_sim_wup': 0.7280461978984395,\n",
    " 'tree_sim_wup_ema': 0.7308775870548898}\n",
    "==============================\n",
    "make_living17\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 0:11:11 (1.9418 s / it)\n",
    "* Acc@1 31.707\n",
    "* Acc@3 0.435\n",
    "* Acc@5 0.461\n",
    "* Acc@10 0.504\n",
    "* Missing Label 23\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.4935\n",
    "* acc_clu_ema 0.4966\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4491 0.4544\n",
    "* tree_sim_lch 2.3340 2.3538\n",
    "* tree_sim_wup 0.7799 0.7833\n",
    "{'acc1': 31.706710103615222,\n",
    " 'acc_clu': 0.4934505226007873,\n",
    " 'acc_clu_ema': 0.4965838649834849,\n",
    " 'acc_top10': 0.5035292520673451,\n",
    " 'acc_top10_ema': 0.5174200262539441,\n",
    " 'acc_top3': 0.4351047463917346,\n",
    " 'acc_top3_ema': 0.44416542238211193,\n",
    " 'acc_top5': 0.4613365911091945,\n",
    " 'acc_top5_ema': 0.4716981132122668,\n",
    " 'ema_acc1': 32.100357449889145,\n",
    " 'n_missing_labels': 23,\n",
    " 'tree_sim_lch': 2.3339517336600326,\n",
    " 'tree_sim_lch_ema': 2.353770666018221,\n",
    " 'tree_sim_path': 0.44905873315128114,\n",
    " 'tree_sim_path_ema': 0.45443736618073877,\n",
    " 'tree_sim_wup': 0.7798900427961709,\n",
    " 'tree_sim_wup_ema': 0.783281234369695}\n",
    "TrainVal: Total time: 0:00:36 (2.6270 s / it)\n",
    "* Acc@1 30.952\n",
    "* Acc@3 0.432\n",
    "* Acc@5 0.453\n",
    "* Acc@10 0.495\n",
    "* Missing Label 40\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.4844\n",
    "* acc_clu_ema 0.4906\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4439 0.4494\n",
    "* tree_sim_lch 2.3221 2.3423\n",
    "* tree_sim_wup 0.7779 0.7809\n",
    "{'acc1': 30.952380952380953,\n",
    " 'acc_clu': 0.4844209288653733,\n",
    " 'acc_clu_ema': 0.49059376837154617,\n",
    " 'acc_top10': 0.49500293946490437,\n",
    " 'acc_top10_ema': 0.5064667842620825,\n",
    " 'acc_top3': 0.4315108759553204,\n",
    " 'acc_top3_ema': 0.4400352733686067,\n",
    " 'acc_top5': 0.4532627865172778,\n",
    " 'acc_top5_ema': 0.4650205761886288,\n",
    " 'ema_acc1': 31.363903586125808,\n",
    " 'n_missing_labels': 40,\n",
    " 'tree_sim_lch': 2.322068607644506,\n",
    " 'tree_sim_lch_ema': 2.342324910605489,\n",
    " 'tree_sim_path': 0.4439089382966366,\n",
    " 'tree_sim_path_ema': 0.4493510518889596,\n",
    " 'tree_sim_wup': 0.7778919899439827,\n",
    " 'tree_sim_wup_ema': 0.7808689809796433}\n",
    "==============================\n",
    "make_nonliving26\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 0:13:56 (1.6124 s / it)\n",
    "* Acc@1 35.301\n",
    "* Acc@3 0.443\n",
    "* Acc@5 0.478\n",
    "* Acc@10 0.521\n",
    "* Missing Label 20\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.4868\n",
    "* acc_clu_ema 0.4881\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4377 0.4388\n",
    "* tree_sim_lch 2.1755 2.1779\n",
    "* tree_sim_wup 0.6968 0.6971\n",
    "{'acc1': 35.30094074581786,\n",
    " 'acc_clu': 0.48678511979633493,\n",
    " 'acc_clu_ema': 0.48806555845955696,\n",
    " 'acc_top10': 0.520754404339136,\n",
    " 'acc_top10_ema': 0.524731296174915,\n",
    " 'acc_top3': 0.44307696942349123,\n",
    " 'acc_top3_ema': 0.4458713385061699,\n",
    " 'acc_top5': 0.47761115337765964,\n",
    " 'acc_top5_ema': 0.4811813176442352,\n",
    " 'ema_acc1': 35.43425700663568,\n",
    " 'n_missing_labels': 20,\n",
    " 'tree_sim_lch': 2.1755468933875353,\n",
    " 'tree_sim_lch_ema': 2.1778670186550873,\n",
    " 'tree_sim_path': 0.4376594705218574,\n",
    " 'tree_sim_path_ema': 0.4387557277247362,\n",
    " 'tree_sim_wup': 0.6968038881988378,\n",
    " 'tree_sim_wup_ema': 0.6971117611768303}\n",
    "TrainVal: Total time: 0:00:49 (2.3630 s / it)\n",
    "* Acc@1 33.544\n",
    "* Acc@3 0.427\n",
    "* Acc@5 0.456\n",
    "* Acc@10 0.500\n",
    "* Missing Label 47\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.4838\n",
    "* acc_clu_ema 0.4869\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4192 0.4221\n",
    "* tree_sim_lch 2.1207 2.1283\n",
    "* tree_sim_wup 0.6835 0.6849\n",
    "{'acc1': 33.54442525426981,\n",
    " 'acc_clu': 0.4837843024371522,\n",
    " 'acc_clu_ema': 0.48685473037804644,\n",
    " 'acc_top10': 0.49990404930413973,\n",
    " 'acc_top10_ema': 0.5035501824953899,\n",
    " 'acc_top3': 0.42698138555777415,\n",
    " 'acc_top3_ema': 0.42870850130884186,\n",
    " 'acc_top5': 0.4563423527926178,\n",
    " 'acc_top5_ema': 0.45902897727521497,\n",
    " 'ema_acc1': 33.85146804835924,\n",
    " 'n_missing_labels': 47,\n",
    " 'tree_sim_lch': 2.120662968789657,\n",
    " 'tree_sim_lch_ema': 2.128312738428518,\n",
    " 'tree_sim_path': 0.41922452218124306,\n",
    " 'tree_sim_path_ema': 0.42211339431810085,\n",
    " 'tree_sim_wup': 0.6834556287924133,\n",
    " 'tree_sim_wup_ema': 0.6849117868154109}\n",
    "==============================\n",
    "make_entity13\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 0:41:43 (1.9138 s / it)\n",
    "* Acc@1 38.457\n",
    "* Acc@3 0.504\n",
    "* Acc@5 0.549\n",
    "* Acc@10 0.605\n",
    "* Missing Label 57\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.5825\n",
    "* acc_clu_ema 0.5845\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.5007 0.5034\n",
    "* tree_sim_lch 2.4276 2.4392\n",
    "* tree_sim_wup 0.7665 0.7700\n",
    "{'acc1': 38.45685024408607,\n",
    " 'acc_clu': 0.582529173811985,\n",
    " 'acc_clu_ema': 0.584486044969198,\n",
    " 'acc_top10': 0.604840492593938,\n",
    " 'acc_top10_ema': 0.613370060767744,\n",
    " 'acc_top3': 0.5040362334860987,\n",
    " 'acc_top3_ema': 0.5098052689129356,\n",
    " 'acc_top5': 0.5491040218932287,\n",
    " 'acc_top5_ema': 0.5562055222607023,\n",
    " 'ema_acc1': 38.65313487771796,\n",
    " 'n_missing_labels': 57,\n",
    " 'tree_sim_lch': 2.427588911627093,\n",
    " 'tree_sim_lch_ema': 2.4392347120538225,\n",
    " 'tree_sim_path': 0.5006530640360263,\n",
    " 'tree_sim_path_ema': 0.5033740412434873,\n",
    " 'tree_sim_wup': 0.7664681575873467,\n",
    " 'tree_sim_wup_ema': 0.770007731806267}\n",
    "TrainVal: Total time: 0:01:43 (2.0242 s / it)\n",
    "* Acc@1 37.574\n",
    "* Acc@3 0.491\n",
    "* Acc@5 0.539\n",
    "* Acc@10 0.591\n",
    "* Missing Label 115\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.5784\n",
    "* acc_clu_ema 0.5807\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4914 0.4929\n",
    "* tree_sim_lch 2.3984 2.4064\n",
    "* tree_sim_wup 0.7588 0.7617\n",
    "{'acc1': 37.573987236817615,\n",
    " 'acc_clu': 0.5783688215850565,\n",
    " 'acc_clu_ema': 0.5806749173649012,\n",
    " 'acc_top10': 0.5913598279833326,\n",
    " 'acc_top10_ema': 0.6013529096959931,\n",
    " 'acc_top3': 0.4911984011424349,\n",
    " 'acc_top3_ema': 0.4955031132648117,\n",
    " 'acc_top5': 0.5385502344679618,\n",
    " 'acc_top5_ema': 0.5465446998380902,\n",
    " 'ema_acc1': 37.63548312428014,\n",
    " 'n_missing_labels': 115,\n",
    " 'tree_sim_lch': 2.398391779181742,\n",
    " 'tree_sim_lch_ema': 2.406440016223863,\n",
    " 'tree_sim_path': 0.49144927244049313,\n",
    " 'tree_sim_path_ema': 0.49293618054466165,\n",
    " 'tree_sim_wup': 0.7587764926727909,\n",
    " 'tree_sim_wup_ema': 0.7617471249513307}\n",
    "==============================\n",
    "make_entity30\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 0:36:09 (1.8031 s / it)\n",
    "* Acc@1 33.413\n",
    "* Acc@3 0.443\n",
    "* Acc@5 0.482\n",
    "* Acc@10 0.532\n",
    "* Missing Label 37\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.4708\n",
    "* acc_clu_ema 0.4716\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4449 0.4459\n",
    "* tree_sim_lch 2.2505 2.2555\n",
    "* tree_sim_wup 0.7295 0.7312\n",
    "{'acc1': 33.41270485812205,\n",
    " 'acc_clu': 0.4708366495037926,\n",
    " 'acc_clu_ema': 0.47160979095944255,\n",
    " 'acc_top10': 0.5317361573569047,\n",
    " 'acc_top10_ema': 0.5363295271816484,\n",
    " 'acc_top3': 0.442600743904893,\n",
    " 'acc_top3_ema': 0.445189793233267,\n",
    " 'acc_top5': 0.48165413289574854,\n",
    " 'acc_top5_ema': 0.485246966719063,\n",
    " 'ema_acc1': 33.46013286338461,\n",
    " 'n_missing_labels': 37,\n",
    " 'tree_sim_lch': 2.2505443684707216,\n",
    " 'tree_sim_lch_ema': 2.25549689409272,\n",
    " 'tree_sim_path': 0.44490559531107976,\n",
    " 'tree_sim_path_ema': 0.44591223651507306,\n",
    " 'tree_sim_wup': 0.7294826579966954,\n",
    " 'tree_sim_wup_ema': 0.7312272407712389}\n",
    "TrainVal: Total time: 0:01:40 (2.1442 s / it)\n",
    "* Acc@1 32.917\n",
    "* Acc@3 0.434\n",
    "* Acc@5 0.472\n",
    "* Acc@10 0.524\n",
    "* Missing Label 99\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.4735\n",
    "* acc_clu_ema 0.4756\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4391 0.4405\n",
    "* tree_sim_lch 2.2332 2.2392\n",
    "* tree_sim_wup 0.7253 0.7273\n",
    "{'acc1': 32.91677080729818,\n",
    " 'acc_clu': 0.4735482795967675,\n",
    " 'acc_clu_ema': 0.4756310922269433,\n",
    " 'acc_top10': 0.5237024076972605,\n",
    " 'acc_top10_ema': 0.5282845955506857,\n",
    " 'acc_top3': 0.43355827708032235,\n",
    " 'acc_top3_ema': 0.43772390237248615,\n",
    " 'acc_top5': 0.47171540449354105,\n",
    " 'acc_top5_ema': 0.4758810297118384,\n",
    " 'ema_acc1': 33.02507706406732,\n",
    " 'n_missing_labels': 99,\n",
    " 'tree_sim_lch': 2.2331614285564885,\n",
    " 'tree_sim_lch_ema': 2.239240187051791,\n",
    " 'tree_sim_path': 0.4391300836995432,\n",
    " 'tree_sim_path_ema': 0.44053934584024196,\n",
    " 'tree_sim_wup': 0.7252684623099671,\n",
    " 'tree_sim_wup_ema': 0.7272518179262681}\n",
    "==============================\n",
    "imagenet1k\n",
    "recover from epoch=29\n",
    "TrainVal: Total time: 1:16:04 (0.9119 s / it)\n",
    "* Acc@1 28.968\n",
    "* Acc@3 0.402\n",
    "* Acc@5 0.456\n",
    "* Acc@10 0.523\n",
    "* Missing Label 61\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.3659\n",
    "* acc_clu_ema 0.3700\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.4043 0.4091\n",
    "* tree_sim_lch 2.1771 2.1971\n",
    "* tree_sim_wup 0.7125 0.7184\n",
    "{'acc1': 28.967925032057853,\n",
    " 'acc_clu': 0.3659187658934768,\n",
    " 'acc_clu_ema': 0.37000560418237477,\n",
    " 'acc_top10': 0.5227390870675261,\n",
    " 'acc_top10_ema': 0.5359838462203735,\n",
    " 'acc_top3': 0.4016161587773241,\n",
    " 'acc_top3_ema': 0.4094768441106043,\n",
    " 'acc_top5': 0.4555911475777416,\n",
    " 'acc_top5_ema': 0.4659261028448206,\n",
    " 'ema_acc1': 29.29356080998749,\n",
    " 'n_missing_labels': 61,\n",
    " 'tree_sim_lch': 2.1771150799123404,\n",
    " 'tree_sim_lch_ema': 2.1970924436204826,\n",
    " 'tree_sim_path': 0.40433240532199033,\n",
    " 'tree_sim_path_ema': 0.4090965613531554,\n",
    " 'tree_sim_wup': 0.7125054214545673,\n",
    " 'tree_sim_wup_ema': 0.7184456773561153}\n",
    "TrainVal: Total time: 0:04:32 (1.3911 s / it)\n",
    "* Acc@1 28.399\n",
    "* Acc@3 0.392\n",
    "* Acc@5 0.444\n",
    "* Acc@10 0.509\n",
    "* Missing Label 351\n",
    "evaluate:: cluster evaluate\n",
    "* acc_clu 0.3696\n",
    "* acc_clu_ema 0.3734\n",
    "evaluate:: tree evaluate\n",
    "* tree_sim_path 0.3984 0.4032\n",
    "* tree_sim_lch 2.1575 2.1779\n",
    "* tree_sim_wup 0.7068 0.7129\n",
    "{'acc1': 28.398960623816446,\n",
    " 'acc_clu': 0.3696182290625625,\n",
    " 'acc_clu_ema': 0.3734159504297421,\n",
    " 'acc_top10': 0.5090345792399391,\n",
    " 'acc_top10_ema': 0.5223465920376249,\n",
    " 'acc_top3': 0.3923645812601846,\n",
    " 'acc_top3_ema': 0.3999000599729569,\n",
    " 'acc_top5': 0.4440335798556629,\n",
    " 'acc_top5_ema': 0.45326803918006825,\n",
    " 'ema_acc1': 28.720767539666934,\n",
    " 'n_missing_labels': 351,\n",
    " 'tree_sim_lch': 2.157518887293309,\n",
    " 'tree_sim_lch_ema': 2.1779184997993988,\n",
    " 'tree_sim_path': 0.39840100749574936,\n",
    " 'tree_sim_path_ema': 0.4032001848159285,\n",
    " 'tree_sim_wup': 0.7067951841750111,\n",
    " 'tree_sim_wup_ema': 0.7129235648119043}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93f8e11-c9d8-40e1-add0-e4645e944ce8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing keys:\n",
      "['visual.mask_token_embedding', 'visual.ln_patch.weight', 'visual.ln_patch.bias', 'visual.decoder.0.weight', 'visual.decoder.0.bias', 'visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']\n",
      "get_classifier in21k\n",
      "get_vocab in21k\n",
      "==============================\n",
      "cifar100\n",
      "29\n",
      "recover from epoch=29\n",
      "eval trainval\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TrainVal:  [  0/196]  eta: 0:07:08    time: 2.1846  data: 1.3762  max mem: 0\n",
      "TrainVal:  [ 10/196]  eta: 0:02:56    time: 0.9472  data: 0.1262  max mem: 0\n",
      "TrainVal:  [ 20/196]  eta: 0:02:36    time: 0.8240  data: 0.0010  max mem: 0\n",
      "TrainVal:  [ 30/196]  eta: 0:02:24    time: 0.8263  data: 0.0011  max mem: 0\n",
      "TrainVal:  [ 40/196]  eta: 0:02:14    time: 0.8297  data: 0.0013  max mem: 0\n",
      "TrainVal:  [ 50/196]  eta: 0:02:04    time: 0.8328  data: 0.0012  max mem: 0\n",
      "TrainVal:  [ 60/196]  eta: 0:01:55    time: 0.8354  data: 0.0012  max mem: 0\n",
      "TrainVal:  [ 70/196]  eta: 0:01:47    time: 0.8379  data: 0.0012  max mem: 0\n",
      "TrainVal:  [ 80/196]  eta: 0:01:38    time: 0.8399  data: 0.0012  max mem: 0\n",
      "TrainVal:  [ 90/196]  eta: 0:01:29    time: 0.8413  data: 0.0012  max mem: 0\n",
      "TrainVal:  [100/196]  eta: 0:01:21    time: 0.8425  data: 0.0012  max mem: 0\n",
      "TrainVal:  [110/196]  eta: 0:01:12    time: 0.8433  data: 0.0013  max mem: 0\n",
      "TrainVal:  [120/196]  eta: 0:01:04    time: 0.8441  data: 0.0013  max mem: 0\n",
      "TrainVal:  [130/196]  eta: 0:00:55    time: 0.8448  data: 0.0012  max mem: 0\n",
      "TrainVal:  [140/196]  eta: 0:00:47    time: 0.8452  data: 0.0012  max mem: 0\n",
      "TrainVal:  [150/196]  eta: 0:00:38    time: 0.8453  data: 0.0013  max mem: 0\n",
      "TrainVal:  [160/196]  eta: 0:00:30    time: 0.8453  data: 0.0012  max mem: 0\n",
      "TrainVal:  [170/196]  eta: 0:00:22    time: 0.8455  data: 0.0012  max mem: 0\n",
      "TrainVal:  [180/196]  eta: 0:00:13    time: 0.8455  data: 0.0012  max mem: 0\n",
      "TrainVal:  [190/196]  eta: 0:00:05    time: 0.8452  data: 0.0012  max mem: 0\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TrainVal:  [195/196]  eta: 0:00:00    time: 0.8170  data: 0.0012  max mem: 0\n",
      "TrainVal: Total time: 0:02:45 (0.8447 s / it)\n",
      "evaluate:: cluster evaluate\n",
      "* acc_clu 0.4071\n",
      "* acc_clu_ema 0.3938\n",
      "{'acc_clu': 0.4071,\n",
      " 'acc_clu_ema': 0.39376,\n",
      " 'sim_bert': 0.4205591743094684,\n",
      " 'sim_bert_ema': 0.42925190301529365}\n",
      "eval test\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TrainVal:  [ 0/40]  eta: 0:01:31    time: 2.2920  data: 1.4690  max mem: 0\n",
      "TrainVal:  [10/40]  eta: 0:00:29    time: 0.9740  data: 0.1348  max mem: 0\n",
      "TrainVal:  [20/40]  eta: 0:00:18    time: 0.8433  data: 0.0013  max mem: 0\n",
      "TrainVal:  [30/40]  eta: 0:00:08    time: 0.8436  data: 0.0012  max mem: 0\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TrainVal:  [39/40]  eta: 0:00:00    time: 0.8060  data: 0.0013  max mem: 0\n",
      "TrainVal: Total time: 0:00:34 (0.8669 s / it)\n",
      "evaluate:: cluster evaluate\n",
      "* acc_clu 0.4085\n",
      "* acc_clu_ema 0.3965\n",
      "{'acc_clu': 0.4085,\n",
      " 'acc_clu_ema': 0.3965,\n",
      " 'sim_bert': 0.4214163320553489,\n",
      " 'sim_bert_ema': 0.429563484009821}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "from ema import ModelEma\n",
    "import model as clip\n",
    "from model import clip_classifier_oszsl\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "from data.vocab import get_vocab\n",
    "from data.build_dataset import build_transform\n",
    "from engine_self_training_must import evaluate, evaluate_label\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:1'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'make_nonliving26'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 256\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "    clip_model = 'ViT-B/16'\n",
    "    mask = True\n",
    "    vocab_name = 'in21k'\n",
    "    resume_ckpt = ''\n",
    "    model_ema_decay = 0.9998\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "### model\n",
    "model = clip_classifier_oszsl(args)\n",
    "args.nb_classes = model.num_classes\n",
    "model_ema = ModelEma(\n",
    "    model,\n",
    "    decay=args.model_ema_decay,\n",
    "    resume='',\n",
    "    device=args.device,\n",
    "    )\n",
    "### vocab\n",
    "vocab = get_vocab(args.vocab_name)\n",
    "args.num_voc = len(vocab)\n",
    "\n",
    "args.image_mean=(0.48145466, 0.4578275, 0.40821073)\n",
    "args.image_std=(0.26862954, 0.26130258, 0.27577711)\n",
    "args.input_size=224\n",
    "\n",
    "for dataset_name in [\n",
    "    'cifar100']:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    # load checkpoint\n",
    "    args.resume_ckpt = f'/home/sheng/MUST-output/{args.dataset}/sssa/checkpoint-current.pth'\n",
    "    ckpt = torch.load(f'{args.resume_ckpt}', map_location='cpu')\n",
    "    epoch_resume = ckpt['epoch']\n",
    "    # assert epoch_resume in [28, 29], epoch_resume\n",
    "    print(epoch_resume)\n",
    "    model_resume = ckpt['model']\n",
    "    model_ema_resume = ckpt['model_ema']\n",
    "    model.load_state_dict(model_resume)\n",
    "    model.train()\n",
    "    model_ema.ema.load_state_dict(model_ema_resume)\n",
    "    model_ema.ema.eval()\n",
    "    print(f'recover from epoch={epoch_resume}')\n",
    "    model = model.to(args.device)\n",
    "    model_ema.ema = model_ema.ema.to(args.device)\n",
    "    \n",
    "    \n",
    "    transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "    dataset_trainval = get_datasets_oszsl(args, vocab=vocab, is_train=True, transform=transform_val)\n",
    "    dataset_val = get_datasets_oszsl(args, vocab=vocab, is_train=False, transform=transform_val)\n",
    "    data_loader_trainval = torch.utils.data.DataLoader(\n",
    "        dataset_trainval,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=8,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    print('eval trainval')\n",
    "    trainval_stats = evaluate_label(data_loader_trainval, model, args.device, model_ema=model_ema, args=args, \n",
    "                          other_params={\n",
    "                              'vocab': vocab,\n",
    "                              'type': 'trainval',\n",
    "                          }, \n",
    "                          log_writer=None,\n",
    "                          )\n",
    "    pprint(trainval_stats, compact=True)\n",
    "    print('eval test')\n",
    "    test_stats = evaluate_label(data_loader_val, model, args.device, model_ema=model_ema, args=args, \n",
    "                          other_params={\n",
    "                              'vocab': vocab,\n",
    "                              'type': 'test',\n",
    "                          }, \n",
    "                          log_writer=None,\n",
    "                          )\n",
    "    pprint(test_stats, compact=True)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
