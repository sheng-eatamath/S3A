{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9697f3d5-2ba7-469c-a775-6eeb71e15a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b1eee8-530e-4b7b-92b7-94b1a53b7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cuda:1'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'sdogs'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/checkpoint-current.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa092744-2c02-47a5-be8b-516524d7a1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n00004475': 'organism',\n",
       " 'n00005787': 'benthos',\n",
       " 'n00006024': 'heterotroph',\n",
       " 'n00006484': 'cell',\n",
       " 'n00007846': 'person',\n",
       " 'n00015388': 'animal',\n",
       " 'n00017222': 'plant',\n",
       " 'n00021265': 'food',\n",
       " 'n00021939': 'artifact',\n",
       " 'n00120010': 'hop',\n",
       " 'n00141669': 'check-in',\n",
       " 'n00288000': 'dressage',\n",
       " 'n00288190': 'curvet',\n",
       " 'n00288384': 'piaffe',\n",
       " 'n00324978': 'funambulism',\n",
       " 'n00326094': 'rock_climbing',\n",
       " 'n00433458': 'contact_sport',\n",
       " 'n00433661': 'outdoor_sport',\n",
       " 'n00433802': 'gymnastics',\n",
       " 'n00434075': 'acrobatics',\n",
       " 'n00439826': 'track_and_field',\n",
       " 'n00440039': 'track',\n",
       " 'n00440218': 'jumping',\n",
       " 'n00440382': 'broad_jump',\n",
       " 'n00440509': 'high_jump',\n",
       " 'n00440643': 'fosbury_flop',\n",
       " 'n00440747': 'skiing',\n",
       " 'n00440941': 'cross-country_skiing',\n",
       " 'n00441073': 'ski_jumping',\n",
       " 'n00441824': 'water_sport',\n",
       " 'n00442115': 'swimming',\n",
       " 'n00442437': 'bathe',\n",
       " 'n00442847': 'dip',\n",
       " 'n00442981': 'dive',\n",
       " 'n00443231': 'floating',\n",
       " 'n00443375': \"dead-man's_float\",\n",
       " 'n00443517': 'belly_flop',\n",
       " 'n00443692': 'cliff_diving',\n",
       " 'n00443803': 'flip',\n",
       " 'n00443917': 'gainer',\n",
       " 'n00444142': 'half_gainer',\n",
       " 'n00444340': 'jackknife',\n",
       " 'n00444490': 'swan_dive',\n",
       " 'n00444651': 'skin_diving',\n",
       " 'n00444846': 'scuba_diving',\n",
       " 'n00444937': 'snorkeling',\n",
       " 'n00445055': 'surfing',\n",
       " 'n00445226': 'water-skiing',\n",
       " 'n00445351': 'rowing',\n",
       " 'n00445685': 'sculling',\n",
       " 'n00445802': 'boxing',\n",
       " 'n00446311': 'professional_boxing',\n",
       " 'n00446411': 'in-fighting',\n",
       " 'n00446493': 'fight',\n",
       " 'n00446632': 'rope-a-dope',\n",
       " 'n00446804': 'spar',\n",
       " 'n00446980': 'archery',\n",
       " 'n00447073': 'sledding',\n",
       " 'n00447221': 'tobogganing',\n",
       " 'n00447361': 'luging',\n",
       " 'n00447463': 'bobsledding',\n",
       " 'n00447540': 'wrestling',\n",
       " 'n00447957': 'greco-roman_wrestling',\n",
       " 'n00448126': 'professional_wrestling',\n",
       " 'n00448232': 'sumo',\n",
       " 'n00448466': 'skating',\n",
       " 'n00448640': 'ice_skating',\n",
       " 'n00448748': 'figure_skating',\n",
       " 'n00448872': 'rollerblading',\n",
       " 'n00448958': 'roller_skating',\n",
       " 'n00449054': 'skateboarding',\n",
       " 'n00449168': 'speed_skating',\n",
       " 'n00449295': 'racing',\n",
       " 'n00449517': 'auto_racing',\n",
       " 'n00449695': 'boat_racing',\n",
       " 'n00449796': 'hydroplane_racing',\n",
       " 'n00449892': 'camel_racing',\n",
       " 'n00449977': 'greyhound_racing',\n",
       " 'n00450070': 'horse_racing',\n",
       " 'n00450335': 'riding',\n",
       " 'n00450700': 'equestrian_sport',\n",
       " 'n00450866': 'pony-trekking',\n",
       " 'n00450998': 'showjumping',\n",
       " 'n00451186': 'cross-country_riding',\n",
       " 'n00451370': 'cycling',\n",
       " 'n00451563': 'bicycling',\n",
       " 'n00451635': 'motorcycling',\n",
       " 'n00451768': 'dune_cycling',\n",
       " 'n00451866': 'blood_sport',\n",
       " 'n00452034': 'bullfighting',\n",
       " 'n00452152': 'cockfighting',\n",
       " 'n00452293': 'hunt',\n",
       " 'n00452734': 'battue',\n",
       " 'n00452864': 'beagling',\n",
       " 'n00453126': 'coursing',\n",
       " 'n00453313': 'deer_hunting',\n",
       " 'n00453396': 'ducking',\n",
       " 'n00453478': 'fox_hunting',\n",
       " 'n00453631': 'pigsticking',\n",
       " 'n00453935': 'fishing',\n",
       " 'n00454237': 'angling',\n",
       " 'n00454395': 'fly-fishing',\n",
       " 'n00454493': 'troll',\n",
       " 'n00454624': 'casting',\n",
       " 'n00454855': 'bait_casting',\n",
       " 'n00454983': 'fly_casting',\n",
       " 'n00455076': 'overcast',\n",
       " 'n00455173': 'surf_casting',\n",
       " 'n00456465': 'day_game',\n",
       " 'n00463246': 'athletic_game',\n",
       " 'n00463543': 'ice_hockey',\n",
       " 'n00464277': 'tetherball',\n",
       " 'n00464478': 'water_polo',\n",
       " 'n00464651': 'outdoor_game',\n",
       " 'n00464894': 'golf',\n",
       " 'n00466273': 'professional_golf',\n",
       " 'n00466377': 'round_of_golf',\n",
       " 'n00466524': 'medal_play',\n",
       " 'n00466630': 'match_play',\n",
       " 'n00466712': 'miniature_golf',\n",
       " 'n00466880': 'croquet',\n",
       " 'n00467320': 'quoits',\n",
       " 'n00467536': 'shuffleboard',\n",
       " 'n00467719': 'field_game',\n",
       " 'n00467995': 'field_hockey',\n",
       " 'n00468299': 'shinny',\n",
       " 'n00468480': 'football',\n",
       " 'n00469651': 'american_football',\n",
       " 'n00470554': 'professional_football',\n",
       " 'n00470682': 'touch_football',\n",
       " 'n00470830': 'hurling',\n",
       " 'n00470966': 'rugby',\n",
       " 'n00471437': 'ball_game',\n",
       " 'n00471613': 'baseball',\n",
       " 'n00474568': 'ball',\n",
       " 'n00474657': 'professional_baseball',\n",
       " 'n00474769': 'hardball',\n",
       " 'n00474881': 'perfect_game',\n",
       " 'n00475014': 'no-hit_game',\n",
       " 'n00475142': 'one-hitter',\n",
       " 'n00475273': 'two-hitter',\n",
       " 'n00475403': 'three-hitter',\n",
       " 'n00475535': 'four-hitter',\n",
       " 'n00475661': 'five-hitter',\n",
       " 'n00475787': 'softball',\n",
       " 'n00476140': 'rounders',\n",
       " 'n00476235': 'stickball',\n",
       " 'n00476389': 'cricket',\n",
       " 'n00477392': 'lacrosse',\n",
       " 'n00477639': 'polo',\n",
       " 'n00477827': 'pushball',\n",
       " 'n00478262': 'soccer',\n",
       " 'n00479076': 'court_game',\n",
       " 'n00479440': 'handball',\n",
       " 'n00479616': 'racquetball',\n",
       " 'n00479734': 'fives',\n",
       " 'n00479887': 'squash',\n",
       " 'n00480211': 'volleyball',\n",
       " 'n00480366': 'jai_alai',\n",
       " 'n00480508': 'badminton',\n",
       " 'n00480885': 'battledore',\n",
       " 'n00480993': 'basketball',\n",
       " 'n00481803': 'professional_basketball',\n",
       " 'n00481938': 'deck_tennis',\n",
       " 'n00482122': 'netball',\n",
       " 'n00482298': 'tennis',\n",
       " 'n00483205': 'professional_tennis',\n",
       " 'n00483313': 'singles',\n",
       " 'n00483409': 'singles',\n",
       " 'n00483508': 'doubles',\n",
       " 'n00483605': 'doubles',\n",
       " 'n00483705': 'royal_tennis',\n",
       " 'n00483848': 'pallone',\n",
       " 'n00523513': 'sport',\n",
       " 'n00812526': 'clasp',\n",
       " 'n00825773': 'judo',\n",
       " 'n00887544': 'team_sport',\n",
       " 'n01035504': 'last_supper',\n",
       " 'n01035667': 'seder',\n",
       " 'n01055165': 'camping',\n",
       " 'n01314388': 'pest',\n",
       " 'n01314663': 'critter',\n",
       " 'n01314781': 'creepy-crawly',\n",
       " 'n01314910': 'darter',\n",
       " 'n01315213': 'peeper',\n",
       " 'n01315330': 'homeotherm',\n",
       " 'n01315581': 'poikilotherm',\n",
       " 'n01315805': 'range_animal',\n",
       " 'n01316422': 'scavenger',\n",
       " 'n01316579': 'bottom-feeder',\n",
       " 'n01316734': 'bottom-feeder',\n",
       " 'n01316949': 'work_animal',\n",
       " 'n01317089': 'beast_of_burden',\n",
       " 'n01317294': 'draft_animal',\n",
       " 'n01317391': 'pack_animal',\n",
       " 'n01317541': 'domestic_animal',\n",
       " 'n01317813': 'feeder',\n",
       " 'n01317916': 'feeder',\n",
       " 'n01318053': 'stocker',\n",
       " 'n01318279': 'hatchling',\n",
       " 'n01318381': 'head',\n",
       " 'n01318478': 'migrator',\n",
       " 'n01318660': 'molter',\n",
       " 'n01318894': 'pet',\n",
       " 'n01319001': 'stayer',\n",
       " 'n01319187': 'stunt',\n",
       " 'n01319467': 'marine_animal',\n",
       " 'n01319685': 'by-catch',\n",
       " 'n01320872': 'female',\n",
       " 'n01321123': 'hen',\n",
       " 'n01321230': 'male',\n",
       " 'n01321456': 'adult',\n",
       " 'n01321579': 'young',\n",
       " 'n01321770': 'orphan',\n",
       " 'n01321854': 'young_mammal',\n",
       " 'n01322221': 'baby',\n",
       " 'n01322343': 'pup',\n",
       " 'n01322508': 'wolf_pup',\n",
       " 'n01322604': 'puppy',\n",
       " 'n01322685': 'cub',\n",
       " 'n01322898': 'lion_cub',\n",
       " 'n01322983': 'bear_cub',\n",
       " 'n01323068': 'tiger_cub',\n",
       " 'n01323155': 'kit',\n",
       " 'n01323261': 'suckling',\n",
       " 'n01323355': 'sire',\n",
       " 'n01323493': 'dam',\n",
       " 'n01323599': 'thoroughbred',\n",
       " 'n01323781': 'giant',\n",
       " 'n01324305': 'mutant',\n",
       " 'n01324431': 'carnivore',\n",
       " 'n01324610': 'herbivore',\n",
       " 'n01324799': 'insectivore',\n",
       " 'n01324916': 'acrodont',\n",
       " 'n01325060': 'pleurodont',\n",
       " 'n01326291': 'microorganism',\n",
       " 'n01327909': 'monohybrid',\n",
       " 'n01329186': 'arbovirus',\n",
       " 'n01330126': 'adenovirus',\n",
       " 'n01330497': 'arenavirus',\n",
       " 'n01332181': 'marburg_virus',\n",
       " 'n01333082': 'arenaviridae',\n",
       " 'n01333483': 'vesiculovirus',\n",
       " 'n01333610': 'reoviridae',\n",
       " 'n01334217': 'variola_major',\n",
       " 'n01334690': 'viroid',\n",
       " 'n01335218': 'coliphage',\n",
       " 'n01337191': 'paramyxovirus',\n",
       " 'n01337734': 'poliovirus',\n",
       " 'n01338685': 'herpes',\n",
       " 'n01339083': 'herpes_simplex_1',\n",
       " 'n01339336': 'herpes_zoster',\n",
       " 'n01339471': 'herpes_varicella_zoster',\n",
       " 'n01339801': 'cytomegalovirus',\n",
       " 'n01340014': 'varicella_zoster_virus',\n",
       " 'n01340522': 'polyoma',\n",
       " 'n01340785': 'lyssavirus',\n",
       " 'n01340935': 'reovirus',\n",
       " 'n01341090': 'rotavirus',\n",
       " 'n01342269': 'moneran',\n",
       " 'n01347583': 'archaebacteria',\n",
       " 'n01349735': 'bacteroid',\n",
       " 'n01350226': 'bacillus_anthracis',\n",
       " 'n01350701': 'yersinia_pestis',\n",
       " 'n01351170': 'brucella',\n",
       " 'n01351315': 'spirillum',\n",
       " 'n01357328': 'botulinus',\n",
       " 'n01357507': 'clostridium_perfringens',\n",
       " 'n01358572': 'cyanobacteria',\n",
       " 'n01359762': 'trichodesmium',\n",
       " 'n01362336': 'nitric_bacteria',\n",
       " 'n01363719': 'spirillum',\n",
       " 'n01365474': 'francisella',\n",
       " 'n01365885': 'gonococcus',\n",
       " 'n01366700': 'corynebacterium_diphtheriae',\n",
       " 'n01367772': 'enteric_bacteria',\n",
       " 'n01368672': 'klebsiella',\n",
       " 'n01369358': 'salmonella_typhimurium',\n",
       " 'n01369484': 'typhoid_bacillus',\n",
       " 'n01374703': 'nitrate_bacterium',\n",
       " 'n01374846': 'nitrite_bacterium',\n",
       " 'n01375204': 'actinomycete',\n",
       " 'n01376237': 'streptomyces',\n",
       " 'n01376437': 'streptomyces_erythreus',\n",
       " 'n01376543': 'streptomyces_griseus',\n",
       " 'n01377278': 'tubercle_bacillus',\n",
       " 'n01377510': 'pus-forming_bacteria',\n",
       " 'n01377694': 'streptobacillus',\n",
       " 'n01378545': 'myxobacteria',\n",
       " 'n01379389': 'staphylococcus',\n",
       " 'n01380610': 'diplococcus',\n",
       " 'n01380754': 'pneumococcus',\n",
       " 'n01381044': 'streptococcus',\n",
       " 'n01382033': 'spirochete',\n",
       " 'n01384084': 'planktonic_algae',\n",
       " 'n01384164': 'zooplankton',\n",
       " 'n01384687': 'parasite',\n",
       " 'n01385017': 'endoparasite',\n",
       " 'n01385330': 'ectoparasite',\n",
       " 'n01386007': 'pathogen',\n",
       " 'n01386182': 'commensal',\n",
       " 'n01386354': 'myrmecophile',\n",
       " 'n01387065': 'protoctist',\n",
       " 'n01389507': 'protozoan',\n",
       " 'n01390123': 'sarcodinian',\n",
       " 'n01390763': 'heliozoan',\n",
       " 'n01392275': 'endameba',\n",
       " 'n01392380': 'ameba',\n",
       " 'n01393486': 'globigerina',\n",
       " 'n01394040': 'testacean',\n",
       " 'n01394492': 'arcella',\n",
       " 'n01394771': 'difflugia',\n",
       " 'n01395254': 'ciliate',\n",
       " 'n01396048': 'paramecium',\n",
       " 'n01396617': 'stentor',\n",
       " 'n01397114': 'alga',\n",
       " 'n01397690': 'arame',\n",
       " 'n01397871': 'seagrass',\n",
       " 'n01400247': 'golden_algae',\n",
       " 'n01400391': 'yellow-green_algae',\n",
       " 'n01402600': 'brown_algae',\n",
       " 'n01403457': 'kelp',\n",
       " 'n01404365': 'fucoid',\n",
       " 'n01404495': 'fucoid',\n",
       " 'n01405007': 'fucus',\n",
       " 'n01405616': 'bladderwrack',\n",
       " 'n01407798': 'green_algae',\n",
       " 'n01410457': 'pond_scum',\n",
       " 'n01411450': 'chlorella',\n",
       " 'n01412694': 'stonewort',\n",
       " 'n01413457': 'desmid',\n",
       " 'n01414216': 'sea_moss',\n",
       " 'n01415626': 'eukaryote',\n",
       " 'n01415920': 'prokaryote',\n",
       " 'n01416213': 'zooid',\n",
       " 'n01418498': 'leishmania',\n",
       " 'n01418620': 'zoomastigote',\n",
       " 'n01419332': 'polymastigote',\n",
       " 'n01419573': 'costia',\n",
       " 'n01419888': 'giardia',\n",
       " 'n01421333': 'cryptomonad',\n",
       " 'n01421807': 'sporozoan',\n",
       " 'n01422185': 'sporozoite',\n",
       " 'n01422335': 'trophozoite',\n",
       " 'n01422450': 'merozoite',\n",
       " 'n01423302': 'coccidium',\n",
       " 'n01423617': 'gregarine',\n",
       " 'n01424420': 'plasmodium',\n",
       " 'n01425223': 'leucocytozoan',\n",
       " 'n01427399': 'microsporidian',\n",
       " 'n01429172': 'ostariophysi',\n",
       " 'n01438208': 'cypriniform_fish',\n",
       " 'n01438581': 'loach',\n",
       " 'n01439121': 'cyprinid',\n",
       " 'n01439514': 'carp',\n",
       " 'n01439808': 'domestic_carp',\n",
       " 'n01440160': 'leather_carp',\n",
       " 'n01440242': 'mirror_carp',\n",
       " 'n01440467': 'european_bream',\n",
       " 'n01440764': 'tench',\n",
       " 'n01441117': 'dace',\n",
       " 'n01441272': 'chub',\n",
       " 'n01441425': 'shiner',\n",
       " 'n01441910': 'common_shiner',\n",
       " 'n01442450': 'roach',\n",
       " 'n01442710': 'rudd',\n",
       " 'n01442972': 'minnow',\n",
       " 'n01443243': 'gudgeon',\n",
       " 'n01443537': 'goldfish',\n",
       " 'n01443831': 'crucian_carp',\n",
       " 'n01444339': 'electric_eel',\n",
       " 'n01444783': 'catostomid',\n",
       " 'n01445429': 'buffalo_fish',\n",
       " 'n01445593': 'black_buffalo',\n",
       " 'n01445857': 'hog_sucker',\n",
       " 'n01446152': 'redhorse',\n",
       " 'n01446589': 'cyprinodont',\n",
       " 'n01446760': 'killifish',\n",
       " 'n01447139': 'mummichog',\n",
       " 'n01447331': 'striped_killifish',\n",
       " 'n01447658': 'rivulus',\n",
       " 'n01447946': 'flagfish',\n",
       " 'n01448291': 'swordtail',\n",
       " 'n01448594': 'guppy',\n",
       " 'n01448951': 'topminnow',\n",
       " 'n01449374': 'mosquitofish',\n",
       " 'n01449712': 'platy',\n",
       " 'n01449980': 'mollie',\n",
       " 'n01450661': 'squirrelfish',\n",
       " 'n01450950': 'reef_squirrelfish',\n",
       " 'n01451115': 'deepwater_squirrelfish',\n",
       " 'n01451295': 'holocentrus_ascensionis',\n",
       " 'n01451426': 'soldierfish',\n",
       " 'n01451863': 'anomalops',\n",
       " 'n01452345': 'flashlight_fish',\n",
       " 'n01453087': 'john_dory',\n",
       " 'n01453475': 'boarfish',\n",
       " 'n01453742': 'boarfish',\n",
       " 'n01454545': 'cornetfish',\n",
       " 'n01454856': 'stickleback',\n",
       " 'n01455317': 'three-spined_stickleback',\n",
       " 'n01455461': 'ten-spined_stickleback',\n",
       " 'n01455778': 'pipefish',\n",
       " 'n01456137': 'dwarf_pipefish',\n",
       " 'n01456454': 'deepwater_pipefish',\n",
       " 'n01456756': 'seahorse',\n",
       " 'n01457082': 'snipefish',\n",
       " 'n01457407': 'shrimpfish',\n",
       " 'n01457852': 'trumpetfish',\n",
       " 'n01458746': 'pellicle',\n",
       " 'n01458842': 'embryo',\n",
       " 'n01459791': 'fetus',\n",
       " 'n01460303': 'abortus',\n",
       " 'n01461315': 'spawn',\n",
       " 'n01461646': 'blastula',\n",
       " 'n01462042': 'blastocyst',\n",
       " 'n01462544': 'gastrula',\n",
       " 'n01462803': 'morula',\n",
       " 'n01464844': 'yolk',\n",
       " 'n01466257': 'chordate',\n",
       " 'n01467336': 'cephalochordate',\n",
       " 'n01467804': 'lancelet',\n",
       " 'n01468238': 'tunicate',\n",
       " 'n01468712': 'ascidian',\n",
       " 'n01469103': 'sea_squirt',\n",
       " 'n01469723': 'salp',\n",
       " 'n01470145': 'doliolum',\n",
       " 'n01470479': 'larvacean',\n",
       " 'n01470733': 'appendicularia',\n",
       " 'n01470895': 'ascidian_tadpole',\n",
       " 'n01471682': 'vertebrate',\n",
       " 'n01472303': 'amniota',\n",
       " 'n01472502': 'amniote',\n",
       " 'n01473806': 'aquatic_vertebrate',\n",
       " 'n01474283': 'jawless_vertebrate',\n",
       " 'n01474864': 'ostracoderm',\n",
       " 'n01475232': 'heterostracan',\n",
       " 'n01475940': 'anaspid',\n",
       " 'n01476418': 'conodont',\n",
       " 'n01477080': 'cyclostome',\n",
       " 'n01477525': 'lamprey',\n",
       " 'n01477875': 'sea_lamprey',\n",
       " 'n01478511': 'hagfish',\n",
       " 'n01478969': 'myxine_glutinosa',\n",
       " 'n01479213': 'eptatretus',\n",
       " 'n01479820': 'gnathostome',\n",
       " 'n01480106': 'placoderm',\n",
       " 'n01480516': 'cartilaginous_fish',\n",
       " 'n01480880': 'holocephalan',\n",
       " 'n01481331': 'chimaera',\n",
       " 'n01481498': 'rabbitfish',\n",
       " 'n01482071': 'elasmobranch',\n",
       " 'n01482330': 'shark',\n",
       " 'n01483021': 'cow_shark',\n",
       " 'n01483522': 'mackerel_shark',\n",
       " 'n01483830': 'porbeagle',\n",
       " 'n01484097': 'mako',\n",
       " 'n01484285': 'shortfin_mako',\n",
       " 'n01484447': 'longfin_mako',\n",
       " 'n01484562': 'bonito_shark',\n",
       " 'n01484850': 'great_white_shark',\n",
       " 'n01485479': 'basking_shark',\n",
       " 'n01486010': 'thresher',\n",
       " 'n01486540': 'carpet_shark',\n",
       " 'n01486838': 'nurse_shark',\n",
       " 'n01487506': 'sand_tiger',\n",
       " 'n01488038': 'whale_shark',\n",
       " 'n01488918': 'requiem_shark',\n",
       " 'n01489501': 'bull_shark',\n",
       " 'n01489709': 'sandbar_shark',\n",
       " 'n01489920': 'blacktip_shark',\n",
       " 'n01490112': 'whitetip_shark',\n",
       " 'n01490360': 'dusky_shark',\n",
       " 'n01490670': 'lemon_shark',\n",
       " 'n01491006': 'blue_shark',\n",
       " 'n01491361': 'tiger_shark',\n",
       " 'n01491661': 'soupfin_shark',\n",
       " 'n01491874': 'dogfish',\n",
       " 'n01492357': 'smooth_dogfish',\n",
       " 'n01492569': 'smoothhound',\n",
       " 'n01492708': 'american_smooth_dogfish',\n",
       " 'n01492860': 'florida_smoothhound',\n",
       " 'n01493146': 'whitetip_shark',\n",
       " 'n01493541': 'spiny_dogfish',\n",
       " 'n01493829': 'atlantic_spiny_dogfish',\n",
       " 'n01494041': 'pacific_spiny_dogfish',\n",
       " 'n01494475': 'hammerhead',\n",
       " 'n01494757': 'smooth_hammerhead',\n",
       " 'n01494882': 'smalleye_hammerhead',\n",
       " 'n01495006': 'shovelhead',\n",
       " 'n01495493': 'angel_shark',\n",
       " 'n01495701': 'ray',\n",
       " 'n01496331': 'electric_ray',\n",
       " 'n01497118': 'sawfish',\n",
       " 'n01497413': 'smalltooth_sawfish',\n",
       " 'n01497738': 'guitarfish',\n",
       " 'n01498041': 'stingray',\n",
       " 'n01498406': 'roughtail_stingray',\n",
       " 'n01498699': 'butterfly_ray',\n",
       " 'n01498989': 'eagle_ray',\n",
       " 'n01499396': 'spotted_eagle_ray',\n",
       " 'n01499732': 'cownose_ray',\n",
       " 'n01500091': 'manta',\n",
       " 'n01500476': 'atlantic_manta',\n",
       " 'n01500854': 'devil_ray',\n",
       " 'n01501160': 'skate',\n",
       " 'n01501641': 'grey_skate',\n",
       " 'n01501777': 'little_skate',\n",
       " 'n01501948': 'thorny_skate',\n",
       " 'n01502101': 'barndoor_skate',\n",
       " 'n01503061': 'bird',\n",
       " 'n01503976': 'dickeybird',\n",
       " 'n01504179': 'fledgling',\n",
       " 'n01504344': 'nestling',\n",
       " 'n01514668': 'cock',\n",
       " 'n01514752': 'gamecock',\n",
       " 'n01514859': 'hen',\n",
       " 'n01514926': 'nester',\n",
       " 'n01515078': 'night_bird',\n",
       " 'n01515217': 'night_raven',\n",
       " 'n01515303': 'bird_of_passage',\n",
       " 'n01516212': 'archaeopteryx',\n",
       " 'n01517389': 'archaeornis',\n",
       " 'n01517565': 'ratite',\n",
       " 'n01517966': 'carinate',\n",
       " 'n01518878': 'ostrich',\n",
       " 'n01519563': 'cassowary',\n",
       " 'n01519873': 'emu',\n",
       " 'n01520576': 'kiwi',\n",
       " 'n01521399': 'rhea',\n",
       " 'n01521756': 'rhea',\n",
       " 'n01522450': 'elephant_bird',\n",
       " 'n01523105': 'moa',\n",
       " 'n01524359': 'passerine',\n",
       " 'n01524761': 'nonpasserine_bird',\n",
       " 'n01525720': 'oscine',\n",
       " 'n01526521': 'songbird',\n",
       " 'n01526766': 'honey_eater',\n",
       " 'n01527194': 'accentor',\n",
       " 'n01527347': 'hedge_sparrow',\n",
       " 'n01527617': 'lark',\n",
       " 'n01527917': 'skylark',\n",
       " 'n01528396': 'wagtail',\n",
       " 'n01528654': 'pipit',\n",
       " 'n01528845': 'meadow_pipit',\n",
       " 'n01529672': 'finch',\n",
       " 'n01530439': 'chaffinch',\n",
       " 'n01530575': 'brambling',\n",
       " 'n01531178': 'goldfinch',\n",
       " 'n01531344': 'linnet',\n",
       " 'n01531512': 'siskin',\n",
       " 'n01531639': 'red_siskin',\n",
       " 'n01531811': 'redpoll',\n",
       " 'n01531971': 'redpoll',\n",
       " 'n01532325': 'new_world_goldfinch',\n",
       " 'n01532511': 'pine_siskin',\n",
       " 'n01532829': 'house_finch',\n",
       " 'n01533000': 'purple_finch',\n",
       " 'n01533339': 'canary',\n",
       " 'n01533481': 'common_canary',\n",
       " 'n01533651': 'serin',\n",
       " 'n01533893': 'crossbill',\n",
       " 'n01534155': 'bullfinch',\n",
       " 'n01534433': 'junco',\n",
       " 'n01534582': 'dark-eyed_junco',\n",
       " 'n01534762': 'new_world_sparrow',\n",
       " 'n01535140': 'vesper_sparrow',\n",
       " 'n01535469': 'white-throated_sparrow',\n",
       " 'n01535690': 'white-crowned_sparrow',\n",
       " 'n01536035': 'chipping_sparrow',\n",
       " 'n01536186': 'field_sparrow',\n",
       " 'n01536334': 'tree_sparrow',\n",
       " 'n01536644': 'song_sparrow',\n",
       " 'n01536780': 'swamp_sparrow',\n",
       " 'n01537134': 'bunting',\n",
       " 'n01537544': 'indigo_bunting',\n",
       " 'n01537895': 'ortolan',\n",
       " 'n01538059': 'reed_bunting',\n",
       " 'n01538200': 'yellowhammer',\n",
       " 'n01538362': 'yellow-breasted_bunting',\n",
       " 'n01538630': 'snow_bunting',\n",
       " 'n01538955': 'honeycreeper',\n",
       " 'n01539272': 'banana_quit',\n",
       " 'n01539573': 'sparrow',\n",
       " 'n01539925': 'english_sparrow',\n",
       " 'n01540090': 'tree_sparrow',\n",
       " 'n01540233': 'grosbeak',\n",
       " 'n01540566': 'evening_grosbeak',\n",
       " 'n01540832': 'hawfinch',\n",
       " 'n01541102': 'pine_grosbeak',\n",
       " 'n01541386': 'cardinal',\n",
       " 'n01541760': 'pyrrhuloxia',\n",
       " 'n01541922': 'towhee',\n",
       " 'n01542168': 'chewink',\n",
       " 'n01542433': 'green-tailed_towhee',\n",
       " 'n01542786': 'weaver',\n",
       " 'n01543175': 'baya',\n",
       " 'n01543383': 'whydah',\n",
       " 'n01543632': 'java_sparrow',\n",
       " 'n01543936': 'avadavat',\n",
       " 'n01544208': 'grassfinch',\n",
       " 'n01544389': 'zebra_finch',\n",
       " 'n01544704': 'honeycreeper',\n",
       " 'n01545574': 'lyrebird',\n",
       " 'n01546039': 'scrubbird',\n",
       " 'n01546506': 'broadbill',\n",
       " 'n01546921': 'tyrannid',\n",
       " 'n01547832': 'new_world_flycatcher',\n",
       " 'n01548301': 'kingbird',\n",
       " 'n01548492': 'arkansas_kingbird',\n",
       " 'n01548694': \"cassin's_kingbird\",\n",
       " 'n01548865': 'eastern_kingbird',\n",
       " 'n01549053': 'grey_kingbird',\n",
       " 'n01549430': 'pewee',\n",
       " 'n01549641': 'western_wood_pewee',\n",
       " 'n01549886': 'phoebe',\n",
       " 'n01550172': 'vermillion_flycatcher',\n",
       " 'n01550761': 'cotinga',\n",
       " 'n01551080': 'cock_of_the_rock',\n",
       " 'n01551300': 'cock_of_the_rock',\n",
       " 'n01551711': 'manakin',\n",
       " 'n01552034': 'bellbird',\n",
       " 'n01552333': 'umbrella_bird',\n",
       " 'n01552813': 'ovenbird',\n",
       " 'n01553142': 'antbird',\n",
       " 'n01553527': 'ant_thrush',\n",
       " 'n01553762': 'ant_shrike',\n",
       " 'n01554017': 'spotted_antbird',\n",
       " 'n01554448': 'woodhewer',\n",
       " 'n01555004': 'pitta',\n",
       " 'n01555305': 'scissortail',\n",
       " 'n01555809': 'old_world_flycatcher',\n",
       " 'n01556182': 'spotted_flycatcher',\n",
       " 'n01556514': 'thickhead',\n",
       " 'n01557185': 'thrush',\n",
       " 'n01557962': 'missel_thrush',\n",
       " 'n01558149': 'song_thrush',\n",
       " 'n01558307': 'fieldfare',\n",
       " 'n01558461': 'redwing',\n",
       " 'n01558594': 'blackbird',\n",
       " 'n01558765': 'ring_ouzel',\n",
       " 'n01558993': 'robin',\n",
       " 'n01559160': 'clay-colored_robin',\n",
       " 'n01559477': 'hermit_thrush',\n",
       " 'n01559639': 'veery',\n",
       " 'n01559804': 'wood_thrush',\n",
       " 'n01560105': 'nightingale',\n",
       " 'n01560280': 'thrush_nightingale',\n",
       " 'n01560419': 'bulbul',\n",
       " 'n01560636': 'old_world_chat',\n",
       " 'n01560793': 'stonechat',\n",
       " 'n01560935': 'whinchat',\n",
       " 'n01561181': 'solitaire',\n",
       " 'n01561452': 'redstart',\n",
       " 'n01561732': 'wheatear',\n",
       " 'n01562014': 'bluebird',\n",
       " 'n01562265': 'robin',\n",
       " 'n01562451': 'bluethroat',\n",
       " 'n01563128': 'warbler',\n",
       " 'n01563449': 'gnatcatcher',\n",
       " 'n01563746': 'kinglet',\n",
       " 'n01563945': 'goldcrest',\n",
       " 'n01564101': 'gold-crowned_kinglet',\n",
       " 'n01564217': 'ruby-crowned_kinglet',\n",
       " 'n01564394': 'old_world_warbler',\n",
       " 'n01564773': 'blackcap',\n",
       " 'n01564914': 'greater_whitethroat',\n",
       " 'n01565078': 'lesser_whitethroat',\n",
       " 'n01565345': 'wood_warbler',\n",
       " 'n01565599': 'sedge_warbler',\n",
       " 'n01565930': 'wren_warbler',\n",
       " 'n01566207': 'tailorbird',\n",
       " 'n01566645': 'babbler',\n",
       " 'n01567133': 'new_world_warbler',\n",
       " 'n01567678': 'parula_warbler',\n",
       " 'n01567879': \"wilson's_warbler\",\n",
       " 'n01568132': 'flycatching_warbler',\n",
       " 'n01568294': 'american_redstart',\n",
       " 'n01568720': 'cape_may_warbler',\n",
       " 'n01568892': 'yellow_warbler',\n",
       " 'n01569060': 'blackburn',\n",
       " 'n01569262': \"audubon's_warbler\",\n",
       " 'n01569423': 'myrtle_warbler',\n",
       " 'n01569566': 'blackpoll',\n",
       " 'n01569836': 'new_world_chat',\n",
       " 'n01569971': 'yellow-breasted_chat',\n",
       " 'n01570267': 'ovenbird',\n",
       " 'n01570421': 'water_thrush',\n",
       " 'n01570676': 'yellowthroat',\n",
       " 'n01570839': 'common_yellowthroat',\n",
       " 'n01571410': 'riflebird',\n",
       " 'n01571904': 'new_world_oriole',\n",
       " 'n01572328': 'northern_oriole',\n",
       " 'n01572489': 'baltimore_oriole',\n",
       " 'n01572654': \"bullock's_oriole\",\n",
       " 'n01572782': 'orchard_oriole',\n",
       " 'n01573074': 'meadowlark',\n",
       " 'n01573240': 'eastern_meadowlark',\n",
       " 'n01573360': 'western_meadowlark',\n",
       " 'n01573627': 'cacique',\n",
       " 'n01573898': 'bobolink',\n",
       " 'n01574045': 'new_world_blackbird',\n",
       " 'n01574390': 'grackle',\n",
       " 'n01574560': 'purple_grackle',\n",
       " 'n01574801': 'rusty_blackbird',\n",
       " 'n01575117': 'cowbird',\n",
       " 'n01575401': 'red-winged_blackbird',\n",
       " 'n01575745': 'old_world_oriole',\n",
       " 'n01576076': 'golden_oriole',\n",
       " 'n01576358': 'fig-bird',\n",
       " 'n01576695': 'starling',\n",
       " 'n01577035': 'common_starling',\n",
       " 'n01577458': 'rose-colored_starling',\n",
       " 'n01577659': 'myna',\n",
       " 'n01577941': 'crested_myna',\n",
       " 'n01578180': 'hill_myna',\n",
       " 'n01578575': 'corvine_bird',\n",
       " 'n01579028': 'crow',\n",
       " 'n01579149': 'american_crow',\n",
       " 'n01579260': 'raven',\n",
       " 'n01579410': 'rook',\n",
       " 'n01579578': 'jackdaw',\n",
       " 'n01579729': 'chough',\n",
       " 'n01580077': 'jay',\n",
       " 'n01580379': 'old_world_jay',\n",
       " 'n01580490': 'common_european_jay',\n",
       " 'n01580772': 'new_world_jay',\n",
       " 'n01580870': 'blue_jay',\n",
       " 'n01581166': 'canada_jay',\n",
       " 'n01581434': 'rocky_mountain_jay',\n",
       " 'n01581730': 'nutcracker',\n",
       " 'n01581874': 'common_nutcracker',\n",
       " 'n01581984': \"clark's_nutcracker\",\n",
       " 'n01582220': 'magpie',\n",
       " 'n01582398': 'european_magpie',\n",
       " 'n01582498': 'american_magpie',\n",
       " 'n01582856': 'australian_magpie',\n",
       " 'n01583209': 'butcherbird',\n",
       " 'n01583495': 'currawong',\n",
       " 'n01583828': 'piping_crow',\n",
       " 'n01584225': 'wren',\n",
       " 'n01584695': 'winter_wren',\n",
       " 'n01584853': 'house_wren',\n",
       " 'n01585121': 'marsh_wren',\n",
       " 'n01585287': 'long-billed_marsh_wren',\n",
       " 'n01585422': 'sedge_wren',\n",
       " 'n01585715': 'rock_wren',\n",
       " 'n01586020': 'carolina_wren',\n",
       " 'n01586374': 'cactus_wren',\n",
       " 'n01586941': 'mockingbird',\n",
       " 'n01587278': 'blue_mockingbird',\n",
       " 'n01587526': 'catbird',\n",
       " 'n01587834': 'thrasher',\n",
       " 'n01588002': 'brown_thrasher',\n",
       " 'n01588431': 'new_zealand_wren',\n",
       " 'n01588725': 'rock_wren',\n",
       " 'n01588996': 'rifleman_bird',\n",
       " 'n01589286': 'creeper',\n",
       " 'n01589718': 'brown_creeper',\n",
       " 'n01589893': 'european_creeper',\n",
       " 'n01590220': 'wall_creeper',\n",
       " 'n01591005': 'european_nuthatch',\n",
       " 'n01591123': 'red-breasted_nuthatch',\n",
       " 'n01591301': 'white-breasted_nuthatch',\n",
       " 'n01591697': 'titmouse',\n",
       " 'n01592084': 'chickadee',\n",
       " 'n01592257': 'black-capped_chickadee',\n",
       " 'n01592387': 'tufted_titmouse',\n",
       " 'n01592540': 'carolina_chickadee',\n",
       " 'n01592694': 'blue_tit',\n",
       " 'n01593028': 'bushtit',\n",
       " 'n01593282': 'wren-tit',\n",
       " 'n01593553': 'verdin',\n",
       " 'n01594004': 'fairy_bluebird',\n",
       " 'n01594372': 'swallow',\n",
       " 'n01594787': 'barn_swallow',\n",
       " 'n01594968': 'cliff_swallow',\n",
       " 'n01595168': 'tree_swallow',\n",
       " 'n01595450': 'white-bellied_swallow',\n",
       " 'n01595624': 'martin',\n",
       " 'n01595974': 'house_martin',\n",
       " 'n01596273': 'bank_martin',\n",
       " 'n01596608': 'purple_martin',\n",
       " 'n01597022': 'wood_swallow',\n",
       " 'n01597336': 'tanager',\n",
       " 'n01597737': 'scarlet_tanager',\n",
       " 'n01597906': 'western_tanager',\n",
       " 'n01598074': 'summer_tanager',\n",
       " 'n01598271': 'hepatic_tanager',\n",
       " 'n01598588': 'shrike',\n",
       " 'n01598988': 'butcherbird',\n",
       " 'n01599159': 'european_shrike',\n",
       " 'n01599269': 'northern_shrike',\n",
       " 'n01599388': 'white-rumped_shrike',\n",
       " 'n01599556': 'loggerhead_shrike',\n",
       " 'n01599741': 'migrant_shrike',\n",
       " 'n01600085': 'bush_shrike',\n",
       " 'n01600341': 'black-fronted_bush_shrike',\n",
       " 'n01600657': 'bowerbird',\n",
       " 'n01601068': 'satin_bowerbird',\n",
       " 'n01601410': 'great_bowerbird',\n",
       " 'n01601694': 'water_ouzel',\n",
       " 'n01602080': 'european_water_ouzel',\n",
       " 'n01602209': 'american_water_ouzel',\n",
       " 'n01602630': 'vireo',\n",
       " 'n01602832': 'red-eyed_vireo',\n",
       " 'n01603000': 'solitary_vireo',\n",
       " 'n01603152': 'blue-headed_vireo',\n",
       " 'n01603600': 'waxwing',\n",
       " 'n01603812': 'cedar_waxwing',\n",
       " 'n01603953': 'bohemian_waxwing',\n",
       " 'n01604330': 'bird_of_prey',\n",
       " 'n01604968': 'accipitriformes',\n",
       " 'n01605630': 'hawk',\n",
       " 'n01606097': 'eyas',\n",
       " 'n01606177': 'tiercel',\n",
       " 'n01606522': 'goshawk',\n",
       " 'n01606672': 'sparrow_hawk',\n",
       " 'n01606809': \"cooper's_hawk\",\n",
       " 'n01606978': 'chicken_hawk',\n",
       " 'n01607309': 'buteonine',\n",
       " 'n01607429': 'redtail',\n",
       " 'n01607600': 'rough-legged_hawk',\n",
       " 'n01607812': 'red-shouldered_hawk',\n",
       " 'n01607962': 'buzzard',\n",
       " 'n01608265': 'honey_buzzard',\n",
       " 'n01608432': 'kite',\n",
       " 'n01608814': 'black_kite',\n",
       " 'n01609062': 'swallow-tailed_kite',\n",
       " 'n01609391': 'white-tailed_kite',\n",
       " 'n01609751': 'harrier',\n",
       " 'n01609956': 'marsh_harrier',\n",
       " 'n01610100': \"montagu's_harrier\",\n",
       " 'n01610226': 'marsh_hawk',\n",
       " 'n01610552': 'harrier_eagle',\n",
       " 'n01610955': 'falcon',\n",
       " 'n01611472': 'peregrine',\n",
       " 'n01611674': 'falcon-gentle',\n",
       " 'n01611800': 'gyrfalcon',\n",
       " 'n01611969': 'kestrel',\n",
       " 'n01612122': 'sparrow_hawk',\n",
       " 'n01612275': 'pigeon_hawk',\n",
       " 'n01612476': 'hobby',\n",
       " 'n01612628': 'caracara',\n",
       " 'n01612955': \"audubon's_caracara\",\n",
       " 'n01613177': 'carancha',\n",
       " 'n01613294': 'eagle',\n",
       " 'n01613615': 'young_bird',\n",
       " 'n01613807': 'eaglet',\n",
       " 'n01614038': 'harpy',\n",
       " 'n01614343': 'golden_eagle',\n",
       " 'n01614556': 'tawny_eagle',\n",
       " 'n01614925': 'bald_eagle',\n",
       " 'n01615121': 'sea_eagle',\n",
       " 'n01615303': 'kamchatkan_sea_eagle',\n",
       " 'n01615458': 'ern',\n",
       " 'n01615703': 'fishing_eagle',\n",
       " 'n01616086': 'osprey',\n",
       " 'n01616318': 'vulture',\n",
       " 'n01616551': 'aegypiidae',\n",
       " 'n01616764': 'old_world_vulture',\n",
       " 'n01617095': 'griffon_vulture',\n",
       " 'n01617443': 'bearded_vulture',\n",
       " 'n01617766': 'egyptian_vulture',\n",
       " 'n01618082': 'black_vulture',\n",
       " 'n01618503': 'secretary_bird',\n",
       " 'n01618922': 'new_world_vulture',\n",
       " 'n01619310': 'buzzard',\n",
       " 'n01619536': 'condor',\n",
       " 'n01619835': 'andean_condor',\n",
       " 'n01620135': 'california_condor',\n",
       " 'n01620414': 'black_vulture',\n",
       " 'n01620735': 'king_vulture',\n",
       " 'n01621127': 'owl',\n",
       " 'n01621635': 'owlet',\n",
       " 'n01622120': 'little_owl',\n",
       " 'n01622352': 'horned_owl',\n",
       " 'n01622483': 'great_horned_owl',\n",
       " 'n01622779': 'great_grey_owl',\n",
       " 'n01622959': 'tawny_owl',\n",
       " 'n01623110': 'barred_owl',\n",
       " 'n01623425': 'screech_owl',\n",
       " 'n01623615': 'screech_owl',\n",
       " 'n01623706': 'scops_owl',\n",
       " 'n01623880': 'spotted_owl',\n",
       " 'n01624115': 'old_world_scops_owl',\n",
       " 'n01624212': 'oriental_scops_owl',\n",
       " 'n01624305': 'hoot_owl',\n",
       " 'n01624537': 'hawk_owl',\n",
       " 'n01624833': 'long-eared_owl',\n",
       " 'n01625121': 'laughing_owl',\n",
       " 'n01625562': 'barn_owl',\n",
       " 'n01627424': 'amphibian',\n",
       " 'n01628331': 'ichyostega',\n",
       " 'n01628770': 'urodele',\n",
       " 'n01629276': 'salamander',\n",
       " 'n01629819': 'european_fire_salamander',\n",
       " 'n01629962': 'spotted_salamander',\n",
       " 'n01630148': 'alpine_salamander',\n",
       " 'n01630284': 'newt',\n",
       " 'n01630670': 'common_newt',\n",
       " 'n01630901': 'red_eft',\n",
       " 'n01631175': 'pacific_newt',\n",
       " 'n01631354': 'rough-skinned_newt',\n",
       " 'n01631512': 'california_newt',\n",
       " 'n01631663': 'eft',\n",
       " 'n01632047': 'ambystomid',\n",
       " 'n01632308': 'mole_salamander',\n",
       " 'n01632458': 'spotted_salamander',\n",
       " 'n01632601': 'tiger_salamander',\n",
       " 'n01632777': 'axolotl',\n",
       " 'n01632952': 'waterdog',\n",
       " 'n01633406': 'hellbender',\n",
       " 'n01633781': 'giant_salamander',\n",
       " 'n01634227': 'olm',\n",
       " 'n01634522': 'mud_puppy',\n",
       " 'n01635027': 'dicamptodon',\n",
       " 'n01635176': 'pacific_giant_salamander',\n",
       " 'n01635480': 'olympic_salamander',\n",
       " 'n01636127': 'lungless_salamander',\n",
       " 'n01636352': 'eastern_red-backed_salamander',\n",
       " 'n01636510': 'western_red-backed_salamander',\n",
       " 'n01636829': 'dusky_salamander',\n",
       " 'n01637112': 'climbing_salamander',\n",
       " 'n01637338': 'arboreal_salamander',\n",
       " 'n01637615': 'slender_salamander',\n",
       " 'n01637932': 'web-toed_salamander',\n",
       " 'n01638194': 'shasta_salamander',\n",
       " 'n01638329': 'limestone_salamander',\n",
       " 'n01638722': 'amphiuma',\n",
       " 'n01639187': 'siren',\n",
       " 'n01639765': 'frog',\n",
       " 'n01640846': 'true_frog',\n",
       " 'n01641206': 'wood-frog',\n",
       " 'n01641391': 'leopard_frog',\n",
       " 'n01641577': 'bullfrog',\n",
       " 'n01641739': 'green_frog',\n",
       " 'n01641930': 'cascades_frog',\n",
       " 'n01642097': 'goliath_frog',\n",
       " 'n01642257': 'pickerel_frog',\n",
       " 'n01642391': 'tarahumara_frog',\n",
       " 'n01642539': 'grass_frog',\n",
       " 'n01642943': 'leptodactylid_frog',\n",
       " 'n01643255': 'robber_frog',\n",
       " 'n01643507': 'barking_frog',\n",
       " 'n01643896': 'crapaud',\n",
       " 'n01644373': 'tree_frog',\n",
       " 'n01644900': 'tailed_frog',\n",
       " 'n01645466': 'liopelma_hamiltoni',\n",
       " 'n01645776': 'true_toad',\n",
       " 'n01646292': 'bufo',\n",
       " 'n01646388': 'agua',\n",
       " 'n01646555': 'european_toad',\n",
       " 'n01646648': 'natterjack',\n",
       " 'n01646802': 'american_toad',\n",
       " 'n01646902': 'eurasian_green_toad',\n",
       " 'n01647033': 'american_green_toad',\n",
       " 'n01647180': 'yosemite_toad',\n",
       " 'n01647303': 'texas_toad',\n",
       " 'n01647466': 'southwestern_toad',\n",
       " 'n01647640': 'western_toad',\n",
       " 'n01648139': 'obstetrical_toad',\n",
       " 'n01648356': 'midwife_toad',\n",
       " 'n01648620': 'fire-bellied_toad',\n",
       " 'n01649170': 'spadefoot',\n",
       " 'n01649412': 'western_spadefoot',\n",
       " 'n01649556': 'southern_spadefoot',\n",
       " 'n01649726': 'plains_spadefoot',\n",
       " 'n01650167': 'tree_toad',\n",
       " 'n01650690': 'spring_peeper',\n",
       " 'n01650901': 'pacific_tree_toad',\n",
       " 'n01651059': 'canyon_treefrog',\n",
       " 'n01651285': 'chameleon_tree_frog',\n",
       " 'n01651487': 'cricket_frog',\n",
       " 'n01651641': 'northern_cricket_frog',\n",
       " 'n01651778': 'eastern_cricket_frog',\n",
       " 'n01652026': 'chorus_frog',\n",
       " 'n01652297': 'lowland_burrowing_treefrog',\n",
       " 'n01653026': 'western_narrow-mouthed_toad',\n",
       " 'n01653223': 'eastern_narrow-mouthed_toad',\n",
       " 'n01653509': 'sheep_frog',\n",
       " 'n01653773': 'tongueless_frog',\n",
       " 'n01654083': 'surinam_toad',\n",
       " 'n01654637': 'african_clawed_frog',\n",
       " 'n01654863': 'south_american_poison_toad',\n",
       " 'n01655344': 'caecilian',\n",
       " 'n01661091': 'reptile',\n",
       " 'n01661592': 'anapsid',\n",
       " 'n01661818': 'diapsid',\n",
       " 'n01662060': 'diapsida',\n",
       " 'n01662622': 'chelonian',\n",
       " 'n01662784': 'turtle',\n",
       " 'n01663401': 'sea_turtle',\n",
       " 'n01663782': 'green_turtle',\n",
       " 'n01664065': 'loggerhead',\n",
       " 'n01664369': 'ridley',\n",
       " 'n01664492': 'atlantic_ridley',\n",
       " 'n01664674': 'pacific_ridley',\n",
       " 'n01664990': 'hawksbill_turtle',\n",
       " 'n01665541': 'leatherback_turtle',\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.mapping_ids_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99258ee0-367e-4f99-8eff-f0a2c118c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3955f8-fecd-44cb-86ce-fba688a93077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" build entire wn-graph \"\"\"\n",
    "from nxgraph_model import *\n",
    "\n",
    "with open('/home/sheng/dataset/wordnet_nouns_with_synset.pkl', 'rb') as f:\n",
    "    entire_vocab = pickle.load(f)\n",
    "    \n",
    "G = create_graph([wn.synset(x) for x in entire_vocab['synsets']], entire_vocab['ids'], entire_vocab['names'], entire_vocab['def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e31e8a-5026-4a3f-bdaa-7c2b62707b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 12000\n",
      "missing keys:\n",
      "['visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']\n",
      "Model parameters: 150,408,193\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_val, seed=0)\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "print('dataset size', len(dataset))\n",
    "\n",
    "# model, preprocess = load_clip(args)\n",
    "model = load_clip2(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cafe5f-0418-40a3-8460-5b0cacb29677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### subsample in21k graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606f9c1-993c-41d1-b11e-a2182ebccc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute parent subgraph with hierarchical closure \"\"\"\n",
    "### compute hierarchical closure of @classes\n",
    "classes_closure = classes\n",
    "for c in classes:\n",
    "    classes_closure = classes_closure | predecessor_set(G, source=c)\n",
    "\n",
    "subgraph = G.subgraph(list(classes_closure))\n",
    "\n",
    "parent_classes = set()\n",
    "for c in classes:\n",
    "    parent_classes = parent_classes | predecessor_set(G, source=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e80709bf-c668-4c33-9418-3dec71d39722",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "1. compute parent classes at K level\n",
    "2. build classifier per name\n",
    "\"\"\"\n",
    "k_hier = 6\n",
    "parent_classes_at_k = list(filter(lambda x: G.nodes[x]['depth']==k_hier, parent_classes))\n",
    "# print(len(parent_classes_at_k))\n",
    "# pprint(parent_classes_at_k, compact=True)\n",
    "parent_classes_at_k = sorted(list(set(map(lambda x: x.split('.')[0], parent_classes_at_k))))\n",
    "classifier = build_classifier(args, model, templates, parent_classes_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e734847b-cc92-4033-bb52-e5121d214a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_synset_to_parents = {k: predecessor_set(G, source=k) for k in classes}\n",
    "vfeatures = np.load(f'./cache/vfeatures-{args.dataset}.npy')\n",
    "\n",
    "sim = torch.from_numpy(vfeatures)@classifier.cpu().t()\n",
    "pred_topk = sim.topk(k=5).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "962e45c9-5881-4af0-99a4-0560a08643f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topk = np.array(parent_classes_at_k)[pred_topk.flatten()].reshape(-1, 5)\n",
    "to_name = lambda r: list(map(lambda x: x.split('.')[0], r))\n",
    "instance_to_parents = [\n",
    "    to_name(reduce(lambda x,y: x|y, list(map(lambda x: mapping_synset_to_parents[x.name()], mapping_vocidx_to_synsets(x.item(), vocab)))))\n",
    "    for x in all_gt_voc\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c2d0c9d2-0151-406d-85a2-10b741316a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" visualization: adding `type` causes confusion of classifier \"\"\"\n",
    "# parent_classes_at_k = list(filter(lambda x: G.nodes[x]['depth']==k_hier, parent_classes))\n",
    "# parent_classes_at_k = sorted(list(parent_classes_at_k))\n",
    "# name_parent_classes_at_k = list(map(lambda x: x.split('.')[0], parent_classes_at_k))\n",
    "# parent_parent_classes_at_k = [ ', '.join([pp.name().split('.')[0] for pp in wn.synset(p).hypernyms()]) for p in parent_classes_at_k]\n",
    "# classifier_p = build_classifier(args, model, templates, name_parent_classes_at_k, parent_parent_classes_at_k)\n",
    "\n",
    "# sim = classifier@classifier.t()\n",
    "# sns.distplot(sim.topk(k=10).values[:, 1].cpu().numpy(), bins=100)\n",
    "\n",
    "# sim = classifier_p@classifier_p.t()\n",
    "# sns.distplot(sim.topk(k=10).values[:, 1].cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8da2e-1cc6-41f6-8ca8-d2d320517c6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f663af-6f48-4314-b08b-77e9b888b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_classifier(args, model, templates, vocab_classnames, parent_classnames=None):\n",
    "    batch_size = 64\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        assert parent_classnames is None\n",
    "        with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "            for classname_set in np.array_split(vocab_classnames, len(vocab_classnames)//batch_size):\n",
    "                texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "                texts = tokenize(texts).to(args.device) #tokenize\n",
    "                class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "                class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "                class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "                class_embedding = class_embeddings.mean(dim=1)\n",
    "                class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "                zeroshot_weights.append(class_embedding.cpu())\n",
    "                pbar.update(1)\n",
    "        # else:\n",
    "        #     with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "        #         for classname_set, parentname_set in zip(\n",
    "        #             np.array_split(vocab_classnames, len(vocab_classnames)//batch_size),\n",
    "        #             np.array_split(parent_classnames, len(parent_classnames)//batch_size),\n",
    "        #         ):\n",
    "        #             texts = [template.format(classname)+f' A type of {pname}.' for classname, pname in zip(classname_set, parentname_set) for template in templates] #format with class\n",
    "        #             texts = tokenize(texts).to(args.device) #tokenize\n",
    "        #             class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "        #             class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "        #             class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "        #             class_embedding = class_embeddings.mean(dim=1)\n",
    "        #             class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "        #             zeroshot_weights.append(class_embedding.cpu())\n",
    "        #             pbar.update(1)\n",
    "    classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1ff2189-0380-48f4-bb3f-8d819c274688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:41<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = build_classifier(args, model, templates, vocab.classnames)\n",
    "torch.save(classifier, './cache/wordnet_classifier_in21k_word.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd907a29-a14e-41fc-81b8-7756f2eabc34",
   "metadata": {},
   "source": [
    "### performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3751c-e1d4-4c63-84d1-8827f2dff0ce",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1adcb05-b0e4-4806-93c4-142f00f333c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip(args):\n",
    "    model, preprocess = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model, preprocess\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "def load_mixture_clip(args, decay=1.0):\n",
    "    model1 = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model1.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model1.to(args.device).eval()\n",
    "    model2 = clip.load(args.arch)\n",
    "    model2.to(args.device).eval()\n",
    "    with torch.no_grad():\n",
    "        msd = model1.state_dict()\n",
    "        for k, ema_v in model2.state_dict().items():\n",
    "            # if needs_module:\n",
    "            #     k = 'module.' + k\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * decay + (1. - decay) * model_v)\n",
    "    return model2\n",
    "\n",
    "def topk_acc(all_pred_voc_topk, all_gt_voc):\n",
    "    acc = []\n",
    "    ### topK accuracy\n",
    "    for i in range(all_pred_voc_topk.size(1)):\n",
    "        vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "        for j in range(i+1):\n",
    "            vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "        print(f'k={i} acc={vec.float().mean()}')\n",
    "        acc.append(vec.float().mean().item())\n",
    "    return acc\n",
    "\n",
    "def semantic_acc(y_pred, y_true, metrics={}):\n",
    "    \"\"\" compute soft semantic acc for @y_pred and @y_true \"\"\"\n",
    "    assert len(metrics)>0\n",
    "    assert y_pred.size(0)==y_true.size(0)\n",
    "    scores = {m:[] for m in metrics.keys()}\n",
    "    with tqdm(total=y_pred.size(0)) as pbar:\n",
    "        for i in range(y_pred.size(0)):\n",
    "            syn_pred = mapping_vocidx_to_synsets(y_pred[i].item(), vocab)\n",
    "            syn_true = mapping_vocidx_to_synsets(y_true[i].item(), vocab)\n",
    "            pairs = list(itertools.product(range(len(syn_pred)), range(len(syn_true))))\n",
    "            for m_name, m in metrics.items():\n",
    "                scores[m_name].append( max([ m(syn_pred[p[0]], syn_true[p[1]]) for p in pairs ]) )\n",
    "            pbar.update(1)\n",
    "    for m_name in metrics.keys():\n",
    "        scores[m_name] = np.array(scores[m_name]).mean()\n",
    "    return scores\n",
    "    \n",
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48060709-150f-490c-8967-e15adfbdf404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### naive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df58b595-ee52-4ec2-979e-270d85d6c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2503/2503 [24:41<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "all_pred_voc_topk = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_pred_voc_topk.append(prob.topk(k=5, dim=-1).indices.cpu())\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_pred_voc_topk = torch.cat(all_pred_voc_topk, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f947de4f-bd2c-4d9b-b55c-40692f45cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.33346137404441833\n",
      "n_missing=2\n"
     ]
    }
   ],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')\n",
    "n_missing = len(set(all_gt_voc.unique().numpy()) - set(all_pred_voc.unique().numpy()))\n",
    "print(f'n_missing={n_missing}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384c09-940e-4fc4-b6cb-28cb36b3fbea",
   "metadata": {},
   "source": [
    "#### SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b10c10-78de-45c9-b436-1bd3da399979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(args.input_size, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=torch.tensor(mean),\n",
    "        std=torch.tensor(std))\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=0)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d0db35-1f20-463c-954d-c375b31039c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    n_count = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count.append( selected.sum().item() )\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected, :]).ravel(), return_counts=True)\n",
    "        # counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    n_count = torch.tensor(n_count).cpu()\n",
    "    \n",
    "    # all_clu_pred = setdiff_assignment(all_clu_pred)\n",
    "    \n",
    "    all_clu_pred = all_clu_pred/(n_count.view(-1, 1) + 1e-20)\n",
    "    \n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc, return_results=False):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    inst_acc = (label_voc_kmeans==all_gt_voc).float().mean().item()\n",
    "    print('instance label acc::', inst_acc)\n",
    "    if return_results:\n",
    "        return label_voc_kmeans, res_ass, inst_acc\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, model, classifier, device, \n",
    "                             all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                            prob = classifier_selected[similarity.softmax(-1)]\n",
    "                            cluster_ind.append(prob.cpu().argmax(dim=-1))\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    if classifier_selected is not None:\n",
    "        cluster_ind_voc = classifier_selected[cluster_ind]\n",
    "    else:\n",
    "        cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, \n",
    "                             preextracted_vfeatures=None):\n",
    "    \"\"\" given vocab label set @label_voc_kmeans, \n",
    "    Args:\n",
    "        label_voc_kmeans: cluster-assigned label on vocab\n",
    "        ...\n",
    "        preextracted_vfeatures: np.array([N x D])\n",
    "    Returns:\n",
    "        cluster_ind: tensor([N]): re-ordered cluster assignment\n",
    "        cluster_ind_voc: tensor([N]): cluster assignment indiced by vocab\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device).unique()\n",
    "    cluster_ind = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        if preextracted_vfeatures is not None:\n",
    "            N = len(loader_f.dataset)\n",
    "            batch_size = 10000\n",
    "            indices = np.array_split(np.arange(N), N//batch_size)\n",
    "            with torch.no_grad():\n",
    "                for group in indices:\n",
    "                    logits = torch.from_numpy(preextracted_vfeatures[group]).float()\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits@classifier.t().cpu()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    cluster_ind.append(prob[:, label_voc_kmeans.cpu()].argmax(dim=-1))\n",
    "        else:\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if preextracted_vfeatures is not None:\n",
    "                            logits = torch.from_numpy(preextracted_vfeatures[idx_img.cpu().numpy()]).float().to(device)\n",
    "                        else:\n",
    "                            logits = model.ema.extract_vfeatures(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        similarity = 100 * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "    cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def computation_reassign_by_pred_cluster(row, idx, args, model, classifier, candidate_classifier_ind):\n",
    "    \"\"\"\n",
    "    candidate_classifier_ind = label_voc_kmeans.unique().to(args.device)\n",
    "    \"\"\"\n",
    "    images, label_voc, label_clu, idx_img = row[:4]\n",
    "    images = images.to(args.device)\n",
    "    with amp_autocast():\n",
    "        vfeatures = model.visual(images).float()\n",
    "        # vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "    vfeatures = F.normalize(vfeatures, dim=-1)\n",
    "    batch_sim = 100*vfeatures@classifier[candidate_classifier_ind].t()\n",
    "    cluster_ind = batch_sim.argmax(dim=-1)\n",
    "    cluster_ind_voc = candidate_classifier_ind[cluster_ind].cpu()\n",
    "    return cluster_ind_voc\n",
    "\n",
    "def aggregation_reassign_by_pred_cluster(r, candidate_classifier_ind):\n",
    "    cluster_ind_voc = torch.cat(r, dim=0)\n",
    "    mapping_ind = dict(zip(cluster_ind_voc.unique().numpy(), torch.arange(cluster_ind_voc.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind_voc])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vfeatures(model, data_loader, device):\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(data_loader)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(data_loader):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(device)\n",
    "            with amp_autocast():\n",
    "                vfeatures = model.visual(images).float()\n",
    "            vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "            all_vfeatures.append(vfeatures.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    return all_vfeatures\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def loop_row_collect_results_nograd(obj_iter, computations={}, aggregations={}):\n",
    "    \"\"\" compute and aggregate results, looping over @obj_iter \n",
    "    func_computation(@row, @index_row)\n",
    "    aggregations(list(@results_computation))\n",
    "    \"\"\"\n",
    "    assert set(list(computations.keys())) == set(list(aggregations.keys()))\n",
    "    collector = { k:[] for k in computations }\n",
    "    with tqdm(total=len(obj_iter)) as pbar:\n",
    "        for i, row in enumerate(obj_iter):\n",
    "            ### apply computations\n",
    "            for k, func in computations.items():\n",
    "                collector[k].append(func(row, i))\n",
    "            pbar.update(1)\n",
    "    ### aggregate results\n",
    "    results = {}\n",
    "    for k, func_agg in aggregations.items():\n",
    "        results[k] = func_agg(collector[k])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da71399a-710f-449b-8849-f74ab4eda18d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:16<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 1\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                # logits = model.extract_vfeatures(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd664e9d-4102-447a-aaa0-4675aa1465d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_by_pred_cluster\n",
      "is mutex assignment:: False\n",
      "assignment collision num:: 30\n",
      "linear_assign\n",
      "assignment shape=(120, 20071)\n",
      "instance label acc:: 0.19366666674613953\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 25\n",
      "cluster acc 0.55925\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(120, 20071)\n",
      "instance label acc:: 0.5263333320617676\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 25\n",
      "cluster acc 0.55925\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(120, 20071)\n",
      "instance label acc:: 0.5263333320617676\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 25\n",
      "cluster acc 0.55925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset}-train-clip.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/pred_kmeans_t.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/pred_kmeans_t.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a2a1b8-bf4a-4562-a114-066223571a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-scd.pth', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599b73d-bc0d-4fd9-bb1e-430123b880a7",
   "metadata": {},
   "source": [
    "### Multi Agent Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe60588-c815-4ea5-a6b7-2ee8667dcae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}, verbose=False):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(completion)\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    return result\n",
    "\n",
    "def openai_chatgpt_post_multirounds(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=content,\n",
    "    **parameters,\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "def save_results(res, fpath='test.pkl'):\n",
    "    with open(f'./cache/openai/MAG/{fpath}', 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "    return \n",
    "\n",
    "def load_results(fpath='test.pkl'):\n",
    "    with open(f'./cache/openai/MAG/{fpath}', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ed84f6-45a1-4d29-8038-9dbf8f1a232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@3 = 0.8333333134651184\n"
     ]
    }
   ],
   "source": [
    "k_1 = 3\n",
    "\n",
    "def generate_concepts(record_pred_kmeans_t, all_gt_voc, k_1=3):\n",
    "    all_clu_gt_voc = []\n",
    "    for c in record_pred_kmeans_t.unique():\n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "    all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "    topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "    cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "    for i in range(k_1):\n",
    "        cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "    print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')\n",
    "\n",
    "    \"\"\" gather concepts \"\"\"\n",
    "    to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "    cluster_row_synsets = []\n",
    "    for row in topk_all_clu_pred:\n",
    "        row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "        cluster_row_synsets.append(row_synsets)\n",
    "    return cluster_row_synsets, topk_all_clu_pred\n",
    "\n",
    "cluster_row_synsets, topk_all_clu_pred = generate_concepts(record_pred_kmeans_t, all_gt_voc, k_1=k_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8316f37-a85c-4266-92c0-285cad37cb2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "def format_concept_request_with_def(cluster_row_synsets):\n",
    "    concept_request = []\n",
    "    for row in cluster_row_synsets:\n",
    "        ccpts = reduce(lambda x, y: x+y, row)\n",
    "        names = list(map(lambda x: \"'\"+x.split(':')[0]+\"'\", ccpts))\n",
    "        ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "        ccpts = ', '.join(ccpts)\n",
    "        concept_request.append((', '.join(names), ccpts))\n",
    "    return concept_request\n",
    "\n",
    "def format_concept_request(cluster_row_synsets):\n",
    "    concept_request = []\n",
    "    for row in cluster_row_synsets:\n",
    "        row_names = []\n",
    "        row_names = list(map(lambda x: x[0].split('.')[0], row))\n",
    "        concept_request.append((', '.join(row_names), None))\n",
    "    return concept_request\n",
    "\n",
    "\n",
    "def clean_round_1(all_chatgpt_res):\n",
    "    invalid_inds = []\n",
    "    clean_all_chatgpt_res = [[] for _ in range(len(all_chatgpt_res))]\n",
    "    for i in range(len(all_chatgpt_res)):\n",
    "        for j, row in enumerate(all_chatgpt_res[i]):\n",
    "            lines = row.split('\\n')\n",
    "            if len(lines)<10:\n",
    "                invalid_inds.append((i,j))\n",
    "            for l in lines[:10]:\n",
    "                re_match_res = re.match('[0-9]{1,2}\\..*', l)\n",
    "                if re_match_res is None:\n",
    "                    invalid_inds.append((i,j))\n",
    "            clean_all_chatgpt_res[i].append(lines[:10])\n",
    "    invalid_inds = list(set(invalid_inds))\n",
    "    return clean_all_chatgpt_res, invalid_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308ab485-053f-4d13-a6f3-754025a22db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept_request = format_concept_request_with_def(cluster_row_synsets)\n",
    "concept_request = format_concept_request(cluster_row_synsets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df8f85-31bb-4018-b509-aebb7cf80c41",
   "metadata": {},
   "source": [
    "#### round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e03b02-c725-45fb-94b7-f72f780ce32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_round_1_with_def = lambda concepts, concepts_with_def: \"Let's play a game. You are given three category names (\" + concepts_with_def + \"). GOAL: to visually discriminate \" + concepts + \". Please ask ten questions to distinguish which category is presented in an imaginary image. Rule: you can only ask about their visual appearance, visual features, or visual characteristics. Please ask all questions at once and list each in a row sequentially.\"\n",
    "template_round_1 = lambda concepts, concepts_with_def: \"Let's play a game. You are given three category names (\" + concepts + \"). GOAL: to visually discriminate \" + concepts + \". Please ask ten questions to distinguish which category is presented in an imaginary image. Rule: you can only ask about their visual appearance, visual features, or visual characteristics. Please ask all questions at once and list each in a row sequentially.\"\n",
    "\n",
    "n_repeat = 3\n",
    "template_in_use = template_round_1\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(*row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0835d683-0e52-406a-8378-4d02c4855cd7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 139/360 [15:34<23:26,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID acf169393c7da84442ef90cb58a891f4 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 159/360 [18:14<22:39,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a88b29dd5fafb6b3bf7fdd22bea2158d in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 175/360 [20:34<21:06,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0148729a897ff588fec3d8a752c181ce in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 257/360 [30:22<10:47,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6948dc7a221fc3d3626fcaa3cf03bbb2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 296/360 [35:22<07:47,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d5d51af2c98e2ba49a5c2d3e42db4496 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [42:53<00:00,  7.15s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "n_repeat = 3\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2404c5c9-7ad9-4568-8a90-c73dcfa27c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res, fpath=f'{args.dataset}-round=1-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4850a5a-8091-4a3f-960d-d2c1ab508a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_chatgpt_res = load_results(fpath=f'{args.dataset}-round=1-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fd69fc8-13c2-4f78-a4dc-3e4d0fae54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### repair_r1\n",
    "while 1:\n",
    "    all_chatgpt_res_clean, invalid_inds = clean_round_1(all_chatgpt_res)\n",
    "    if len(invalid_inds)==0:\n",
    "        break\n",
    "    else:\n",
    "        for item in invalid_inds:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[item[0]][item[1]] = openai_chatgpt_post(item[1])\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \n",
    "all_chatgpt_res = all_chatgpt_res_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c54a95ef-e872-4bee-a6e7-e6d6eaf71f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res, fpath=f'{args.dataset}-round=1-no_def.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bcf55-f5ba-4f23-9538-89907b677abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ec1c9b-777d-4ac6-bd68-4ad72bcd7415",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [1:17:04<00:00, 12.85s/it]\n"
     ]
    }
   ],
   "source": [
    "template_round_2_with_def = lambda concepts, concepts_with_def: \"Let's play a game. GOAL: to visually discriminate \" + concepts + \". You are given three category names with definitions (\" + concepts_with_def + \"). I will give you a number of questions. Please answer these questions concisely and accurately for each category. Imagine you are given an imagenery image. For each category name, please answer all questions at once and list each in a row sequentially. I will give you the questions now.\"\n",
    "template_round_2 = lambda concepts, concepts_with_def: \"Let's play a game. GOAL: to visually discriminate \" + concepts + \". You are given three category names (\" + concepts + \"). I will give you a number of questions. Please answer these questions concisely and accurately for each category. Imagine you are given an imagenery image. For each category name, please answer all questions at once and list each in a row sequentially. I will give you the questions now.\"\n",
    "\n",
    "template_in_use_r2 = template_round_2\n",
    "concept_templates_r2 = [[] for _ in range(n_repeat)]\n",
    "all_chatgpt_res_r2 = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=n_repeat*len(concept_request)) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            ### prepare template\n",
    "            content = \\\n",
    "                [\n",
    "                    {'role': 'user', 'content': template_in_use_r2(*row)},\n",
    "                    {'role': 'system', 'content': \"Sure, I'm ready to play the game. Please go ahead and provide me with the questions\"},\n",
    "                    {'role': 'user', 'content': '\\n'.join(all_chatgpt_res[i][j]) + 'Please mention the category name before your listed answers.'}\n",
    "                ]\n",
    "            concept_templates_r2[i].append(content)\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### collect result\n",
    "                    all_chatgpt_res_r2[i].append(openai_chatgpt_post_multirounds(content)[\"choices\"][0].message.content)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            pbar.update(1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de53bd65-9c63-491a-b5ff-7e3c7deb4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r2, fpath=f'{args.dataset}-round=2-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf4d447-d142-40cb-aa94-f3813b922100",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chatgpt_res = np.array(all_chatgpt_res).reshape(3, -1, 10).tolist()\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    for j, row in enumerate(concept_request):\n",
    "        all_chatgpt_res[i][j] = '\\n'.join(all_chatgpt_res[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c6aff9-b322-4cc8-9dc3-221e9b113fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:59<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" check: missing concepts \"\"\"\n",
    "while 1:\n",
    "    ### check validity\n",
    "    invalid_inds = []\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            concepts = list(map(lambda x: x.strip('\\''), concept_request[j][0].split(', ')))\n",
    "            answers = all_chatgpt_res_r2[i][j].split('\\n\\n')[-len(concepts):]\n",
    "            if len(answers) not in [len(concepts), len(concepts)+1]:\n",
    "                invalid_inds.append((i,j))\n",
    "            # for k in range(len(concepts)):\n",
    "                # concepts[k] == answers[k][:len(concepts[k])]\n",
    "    ### request\n",
    "    if len(invalid_inds)==0:\n",
    "        break\n",
    "    with tqdm(total=len(invalid_inds)) as pbar:\n",
    "        for ind in invalid_inds:\n",
    "            row = concept_request[ind[1]]\n",
    "            ### prepare template\n",
    "            content = \\\n",
    "                [\n",
    "                    {'role': 'user', 'content': template_in_use_r2(*row)},\n",
    "                    {'role': 'system', 'content': \"Sure, I'm ready to play the game. Please go ahead and provide me with the questions\"},\n",
    "                    {'role': 'user', 'content': all_chatgpt_res[ind[0]][ind[1]] + 'Please mention the category name before your listed answers.'}\n",
    "                ]\n",
    "            concept_templates_r2[ind[0]][ind[1]] = content\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### collect result\n",
    "                    all_chatgpt_res_r2[ind[0]][ind[1]] = openai_chatgpt_post_multirounds(content)[\"choices\"][0].message.content\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc7464-2e7e-46a2-807e-450ea3450a16",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [03:21<00:00, 12.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:00<00:00, 12.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:49<00:00, 12.43s/it]\n",
      "100%|██████████| 4/4 [00:51<00:00, 12.93s/it]\n",
      "100%|██████████| 3/3 [00:38<00:00, 12.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:36<00:00, 12.06s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.23s/it]\n",
      "100%|██████████| 2/2 [00:32<00:00, 16.42s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.83s/it]\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.82s/it]\n",
      "100%|██████████| 2/2 [00:21<00:00, 10.54s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.37s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.23s/it]\n",
      "100%|██████████| 2/2 [00:31<00:00, 15.83s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.99s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.70s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.59s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.59s/it]\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.50s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.35s/it]\n",
      "100%|██████████| 2/2 [00:25<00:00, 12.53s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.06s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.05s/it]\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.57s/it]\n",
      " 50%|█████     | 1/2 [00:11<00:11, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a187020c08df74f4f1fb993f62f32c7 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:59<00:00, 29.85s/it]\n",
      "100%|██████████| 2/2 [00:20<00:00, 10.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.26s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.04s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.07s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.18s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.58s/it]\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.40s/it]\n",
      "100%|██████████| 2/2 [00:30<00:00, 15.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:19<00:00,  9.52s/it]\n",
      "100%|██████████| 2/2 [00:31<00:00, 15.79s/it]\n",
      "100%|██████████| 2/2 [00:27<00:00, 13.67s/it]\n",
      "100%|██████████| 2/2 [00:19<00:00,  9.96s/it]\n",
      "100%|██████████| 2/2 [00:30<00:00, 15.26s/it]\n",
      "100%|██████████| 2/2 [00:28<00:00, 14.49s/it]\n",
      "100%|██████████| 2/2 [00:25<00:00, 12.75s/it]\n",
      "100%|██████████| 2/2 [00:26<00:00, 13.09s/it]\n",
      "100%|██████████| 2/2 [00:25<00:00, 12.71s/it]\n",
      "100%|██████████| 2/2 [00:29<00:00, 14.71s/it]\n",
      "100%|██████████| 2/2 [00:23<00:00, 11.73s/it]\n",
      "100%|██████████| 2/2 [00:20<00:00, 10.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n",
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.87s/it]\n",
      "100%|██████████| 2/2 [00:25<00:00, 12.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# template_round_2_with_def = lambda concepts, concepts_with_def: \"Let's play a game. GOAL: to visually discriminate \" + concepts + \". You are given three category names with definitions (\" + concepts_with_def + \"). I will give you a number of questions. Please answer all these questions concisely and accurately for each category based on your knowledge. Imagine you are given an imagenery image. For each category name, please answer all questions at once and list each in a row sequentially. I will give you the questions now.\"\n",
    "\"\"\" check: missing answer \"\"\"\n",
    "while 1:\n",
    "    all_qa_pairs = [[] for _ in range(n_repeat)] ### N x R x C x P\n",
    "    invalid_inds = []\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            concepts = list(map(lambda x: x.strip('\\''), concept_request[j][0].split(', ')))\n",
    "            answers = all_chatgpt_res_r2[i][j].split('\\n\\n')[-len(concepts):]\n",
    "            answers = answers[-len(concepts): ]\n",
    "            names = [item.strip(\"'\") for item in row[0].split(', ')]\n",
    "            names_def = [item.strip(\"'\") for item in row[1].split(', ')] if row[1] is not None else [None]*len(row[0].split(', '))\n",
    "\n",
    "            qa_pairs = []\n",
    "            q = [' '.join(item.split(' ')[1:]) for item in all_chatgpt_res[i][j].split('\\n')]\n",
    "            for k in range(len(concepts)):\n",
    "                extract_lines = lambda x: list(filter(lambda y: len(y), x.split('\\n')))\n",
    "                extract_ans = lambda x: ' '.join(x.split(' ')[1:])\n",
    "                try:\n",
    "                    a = [extract_ans(item) for item in extract_lines(answers[k])[1:]]\n",
    "                    qa_pairs.append([names[k], names_def[k], q, a])\n",
    "                    if len(q)!=len(a):\n",
    "                        invalid_inds.append((i, j))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    invalid_inds.append((i, j))\n",
    "            all_qa_pairs[i].append(qa_pairs)\n",
    "    invalid_inds = list(set(invalid_inds))\n",
    "    if len(invalid_inds) == 0:\n",
    "        break\n",
    "\n",
    "    with tqdm(total=len(invalid_inds)) as pbar:\n",
    "        for ind in invalid_inds:\n",
    "            row = concept_request[ind[1]]\n",
    "            ### prepare template\n",
    "            content = \\\n",
    "                [\n",
    "                    {'role': 'user', 'content': template_in_use_r2(*row)},\n",
    "                    {'role': 'system', 'content': \"Sure, I'm ready to play the game. Please go ahead and provide me with the questions\"},\n",
    "                    {'role': 'user', 'content': all_chatgpt_res[ind[0]][ind[1]] + 'Please mention the category name before your listed answers.'}\n",
    "                ]\n",
    "            ### update template\n",
    "            concept_templates_r2[ind[0]][ind[1]] = content\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### update result\n",
    "                    all_chatgpt_res_r2[ind[0]][ind[1]] = openai_chatgpt_post_multirounds(content)[\"choices\"][0].message.content\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516ab86-6169-4e1a-838e-05c8134e1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r2, fpath=f'{args.dataset}-round=2-no_def.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24c159-c1ff-4539-a689-add676bccb36",
   "metadata": {},
   "source": [
    "#### round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70baf483-6131-4224-abad-0d4fd4eafd5d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_round_3_with_def = lambda concepts, concepts_with_def, qa, query: \"GOAL: to visually discriminate \" + concepts + \". Their definitions are given as (\" + concepts_with_def + \"). Please generate a consise descriptive image caption for \" + query + \" only based on the information of this Q&A \" + qa + \". Please answer in template \\\"caption: {caption}\\\".\"\n",
    "template_round_3 = lambda concepts, concepts_with_def, qa, query: \"GOAL: to visually discriminate \" + concepts + \". Please generate a consise descriptive image caption for \" + query + \" only based on the information of this Q&A \" + qa + \". Please answer in template \\\"caption: {caption}\\\".\"\n",
    "synthesize_qa = lambda q, a: [item_q + ' ' + item_a for item_q, item_a in zip(q, a)]\n",
    "\n",
    "template_round_3 = lambda concepts, concepts_with_def, qa, query: \"GOAL: to visually discriminate \" + concepts + \". Please generate a consise descriptive image caption for \" + query + \" only based on the information of this Q&A \" + qa + \". Please answer in template \\\"caption: {caption}\\\". Please make sure do not include concept name in the caption.\"\n",
    "\n",
    "\n",
    "template_in_use_r3 = template_round_3\n",
    "concept_templates_r3 = [[[] for _ in range(len(concept_request))] for _ in range(n_repeat)]\n",
    "all_chatgpt_res_r3 = [[[] for _ in range(len(concept_request))] for _ in range(n_repeat)]\n",
    "with tqdm(total=n_repeat*len(concept_request)*10*len(concepts)) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for j, row in enumerate(concept_request):\n",
    "            concepts = list(map(lambda x: x.strip(\"'\"), concept_request[j][0].split(', ')))\n",
    "            concept_templates_r3[i][j] = [ [] for _ in range(len(concepts)) ]\n",
    "            all_chatgpt_res_r3[i][j] = [ [] for _ in range(len(concepts)) ]\n",
    "            for k in range(len(concepts)):\n",
    "                qas = synthesize_qa(*all_qa_pairs[i][j][k][-2:])\n",
    "                for n in range(10):\n",
    "                    ### prepare template\n",
    "                    content = template_in_use_r3(row[0], row[1], qas[n], all_qa_pairs[i][j][k][0])\n",
    "                    concept_templates_r3[i][j][k].append(content)\n",
    "                    ### make request\n",
    "                    while 1:\n",
    "                        try:\n",
    "                            ### collect result\n",
    "                            all_chatgpt_res_r3[i][j][k].append(openai_chatgpt_post(content, verbose=False))\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    pbar.update(1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0944f-468c-425f-95e2-29d7bdaadb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r3, fpath=f'{args.dataset}-round=3-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4ea1f-8c6c-4688-8036-639ba8ca56bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### validate round 3\n",
    "all_qa_captions = deepcopy(all_chatgpt_res_r3)\n",
    "# all_qa_captions = np.empty([len(all_chatgpt_res_r3[0]), len(all_chatgpt_res_r3[0][0]), len(all_chatgpt_res_r3), len(all_chatgpt_res_r3[0][0][0][0])]).tolist()\n",
    "while 1:\n",
    "    invalid_inds = []\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(len(concept_request)):\n",
    "            concepts = list(map(lambda x: x.strip(\"'\"), concept_request[j][0].split(', ')))\n",
    "            for k in range(len(concepts)):\n",
    "                for i_c, cap in enumerate(all_chatgpt_res_r3[i][j][k]):\n",
    "                    if (not cap[:len('caption:')].lower() == 'caption:') or ('sorry' in cap):\n",
    "                        invalid_inds.append((i,j,k,i_c))\n",
    "                    else:\n",
    "                        extract_caption = lambda x: x.lower().split('caption: ')[-1].strip('{}\\\"')\n",
    "                        all_qa_captions[i][j][k][i_c] = extract_caption(all_chatgpt_res_r3[i][j][k][i_c])\n",
    "                        # all_qa_captions[j][k][i][i_c] = extract_caption(all_chatgpt_res_r3[i][j][k][i_c])\n",
    "    if len(invalid_inds)==0:\n",
    "        break\n",
    "        \n",
    "    with tqdm(total=len(invalid_inds)) as pbar:\n",
    "        for row in invalid_inds:\n",
    "            i, j, k, i_cap = row\n",
    "            qas = synthesize_qa(*all_qa_pairs[i][j][k][-2:])\n",
    "            content = template_in_use_r3(concept_request[j][0], concept_request[j][1], qas[i_cap], all_qa_pairs[i][j][k][0])\n",
    "            concept_templates_r3[i][j][k][i_cap] = content\n",
    "            ### make request\n",
    "            while 1:\n",
    "                try:\n",
    "                    ### collect result\n",
    "                    all_chatgpt_res_r3[i][j][k][i_cap] = openai_chatgpt_post(content, verbose=False)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390b15c-5163-4f64-9ae9-3bce71810b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(all_chatgpt_res_r3, fpath=f'{args.dataset}-round=3-no_def.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a6d60-3e94-4a0e-8b06-79be9c7b9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qa_captions = np.transpose(np.array(all_qa_captions), (1,2,0,3))\n",
    "all_qa_captions = all_qa_captions.reshape(all_qa_captions.shape[0], all_qa_captions.shape[1], np.prod(all_qa_captions.shape[-2:])).tolist() ### row x [concept x repeat x caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932a8db-5ce0-458f-a49c-4498e0be3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_classifier_qa_captions(all_qa_captions, model, all_row_key_name=None):\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_qa_captions)) as pbar:\n",
    "        for idx, row in enumerate(all_qa_captions):\n",
    "            shape_row = np.array(row).shape ### 3 x 30\n",
    "            row = np.array(row).ravel().tolist()\n",
    "            if all_row_key_name is not None:\n",
    "                pass\n",
    "            row_t = tokenize(row).to(args.device)\n",
    "            features = model.encode_text(row_t)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25f05c-61d1-4130-a3e3-8bc4c9466b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_cap_classifiers = build_classifier_qa_captions(all_qa_captions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbb121-21e1-4b5a-b147-17ad8987a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_inds_qa_cap = [torch.arange(90).int().div(30, rounding_mode='floor') for _ in range(len(qa_cap_classifiers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e4804a-06f4-4c94-808c-e28b67db0d14",
   "metadata": {},
   "source": [
    "#### naive ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6914315-c82a-49db-8f2d-aae1407d7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeatures = all_vfeatures\n",
    "k_2 = 2\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "all_clu_pred_qa_cap = torch.zeros_like(all_clu_pred[:, 0])\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "for c in range(len(qa_cap_classifiers)):\n",
    "    ### selection \n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = qa_cap_classifiers[c]\n",
    "    ### prediction \n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    ### counting\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each ind\n",
    "    count_smask = []\n",
    "    smask = np.array(candidate_inds_qa_cap[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        # if enable_weight:\n",
    "        #     row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "        #     row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "        #     row_weight /= row_weight.sum()\n",
    "        #     count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        count_smask.append(count_names[smask==s].sum().item())\n",
    "    prediction = torch.tensor(count_smask).argmax(dim=-1)\n",
    "    instance_pred_voc[select] = topk_all_clu_pred[c, prediction]\n",
    "    all_clu_pred_qa_cap[c] = topk_all_clu_pred[c, prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0bb4778d-367e-47c6-bb88-9ec61cad8880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4386)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(instance_pred_voc == all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e86a-2f86-4ff9-8aa1-732ec9681f8d",
   "metadata": {},
   "source": [
    "#### logical ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e36446ae-7904-4961-aad5-e9d2a285b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row x [(concept x repeat x caption)] -> row x [caption x (concept x repeat)]\n",
    "dim_concept, dim_repeat, dim_caption, dim_feature = 3, 3, 10, 512\n",
    "qa_cap_classifiers = [x.view(dim_concept, dim_repeat, dim_caption, dim_feature).permute(2,0,1,3).view(dim_caption, -1, dim_feature) for x in qa_cap_classifiers]\n",
    "\n",
    "candidate_inds_qa_cap = [torch.arange(9).int().div(3, rounding_mode='floor') for _ in range(len(qa_cap_classifiers))] ### row x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc8fa68c-193e-4c57-80c6-3d080a6b8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeatures = all_vfeatures\n",
    "k_2 = 2\n",
    "N = record_pred_kmeans_t.shape[0]\n",
    "R = all_clu_pred.shape[0]\n",
    "instance_pred_voc = torch.zeros(dim_caption, N)\n",
    "all_clu_pred_qa_cap = torch.zeros(dim_caption, R)\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "for i_cap in range(dim_caption):\n",
    "    for c in range(len(qa_cap_classifiers)):\n",
    "        ### selection \n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        row_classifier = qa_cap_classifiers[c][i_cap]\n",
    "        ### prediction \n",
    "        sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "        sim_topk = sim.topk(k=k_2)\n",
    "        ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "        ### counting\n",
    "        count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "        count_names[ind] = val ### count of each ind\n",
    "        count_smask = []\n",
    "        smask = np.array(candidate_inds_qa_cap[c]) ### partition mask\n",
    "        for s in np.unique(smask):\n",
    "            count_smask.append(count_names[smask==s].sum().item())\n",
    "        prediction = torch.tensor(count_smask).argmax(dim=-1)\n",
    "        instance_pred_voc[i_cap, select] = topk_all_clu_pred[c, prediction]\n",
    "        all_clu_pred_qa_cap[i_cap, c] = topk_all_clu_pred[c, prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "588ae30a-04cc-42a7-89c0-e15645364ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_pred_voc = instance_pred_voc.mode(dim=0).values.int()\n",
    "\n",
    "all_clu_pred_qa_cap = all_clu_pred_qa_cap.mode(dim=0).values.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9cb06f96-2ea7-46c9-86e6-b8ccbe487e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4291)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(instance_pred_voc == all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4644d5e-8ab4-4cf3-b242-b70f1ce213ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([334718]), torch.Size([259]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_pred_voc.shape, all_clu_pred_qa_cap.int().unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dec99c-d61b-4121-a428-ff51186c7113",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CHATGPT request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97155be-e602-4509-a3fa-e91d4a7a5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    # completion = openai.Completion.create(\n",
    "    #     model=\"text-davinci-003\",\n",
    "    #     prompt=content,  \n",
    "    #     temperature=0.7,\n",
    "    #     max_tokens=256,\n",
    "    #     top_p=1,\n",
    "    #     frequency_penalty=0,\n",
    "    #     presence_penalty=0,\n",
    "    # )\n",
    "    # result = completion['choices'][0]['text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c210a91-0cb2-429f-a936-87914e2ccc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@3 = 0.49000000953674316\n"
     ]
    }
   ],
   "source": [
    "all_clu_gt_voc = []\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "k_1 = 3\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "for i in range(k_1):\n",
    "    cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00ad4dbd-f755-4709-812c-f48f96e95801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46253372-83e5-4b5b-bb2b-d813d1795e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/OSZSL/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_13 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible adjective phrases of visual descriptions for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible visual descriptive phrases for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9_1\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be16e8ec-4a49-4493-81d2-925d7a5347cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [40:49<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20293320-5aca-4921-aec9-58806f30cc45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)\n",
    "    \n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-iter=1.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(data, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'rb') as f:\n",
    "#     all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14a08e2c-498e-449e-b972-3e9e670aba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-uk{args.estimate_k}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb5cfa-2ec9-4824-b743-90b31b59a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-iter=1.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "878b68f2-a270-439c-bc5e-6e23c4f7ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "191fef52-2b54-4946-b3eb-c0a1e1db7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-uk{args.estimate_k}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84ba2f3d-8fb1-4e49-af3a-f7a61f79fced",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34\n",
      "[\"'\", \"'\", \"'\"] [\"florest's_cineraria.n.01\", 'red_campion.n.01', 'blue-eyed_african_daisy.n.01']\n",
      "2 34\n",
      "['\"forest\\'s cineraria'] [\"florest's_cineraria.n.01\", 'red_campion.n.01', 'blue-eyed_african_daisy.n.01']\n",
      "2 72\n",
      "['runner.n.10', '- a silver fish with black stripes swimming in the atlantic; ', '- a school of runner fish swimming in the ocean; ', '- a fisherman holding a runner fish caught off the coast of cape cod; ', '- a plate of grilled runner fish served with vegetables and rice.', 'runner.n.03', '- a man in a suit running with a briefcase in hand; ', '- a messenger delivering a package on foot; ', '- a person running with a stack of papers; ', '- a courier running with a large envelope.', 'runner.n.02', '- a woman jogging in the park; ', '- a man sprinting on a track; ', '- a group of runners participating in a marathon; ', '- a person running on a treadmill at the gym.', 'runner.n.06', '- a runner crossing the finish line with arms raised; ', '- a group of runners lining up at the starting line; ', '- a runner stretching before a race; ', '- a medal being awarded to a winning runner.', 'miler.n.02', '- a runner in the middle of a mile race; ', '- a group of milers rounding a turn on a track; ', '- a runner crossing the finish line of a mile race; ', '- a miler being awarded a medal after a race.', 'running_suit.n.01', '- a woman wearing a pink running suit and matching sneakers; ', '- a man wearing a running suit with reflective stripes; ', '- a pile of running suits in different colors; ', '- a man and woman jogging together in matching running suits.'] ['runner.n.10', 'runner.n.03', 'runner.n.02', 'runner.n.06', 'miler.n.02', 'running_suit.n.01']\n",
      "repair (0, 34)\n",
      "repair (2, 34)\n",
      "repair (2, 72)\n",
      "repair 43 0\n",
      "repair 43 1\n",
      "repair 43 2\n",
      "repair 43 0\n",
      "repair 43 1\n",
      "repair 43 2\n"
     ]
    }
   ],
   "source": [
    "while 1:\n",
    "    \"\"\" integrity check \"\"\"\n",
    "    while 1:\n",
    "        invalid_res = []\n",
    "        for i in range(n_repeat):\n",
    "            for j, row in enumerate(all_chatgpt_res[i]):\n",
    "                extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "                remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "                synsets = extract_synsetid(remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n')))\n",
    "                gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "                try:\n",
    "                    start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "                    synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "                    assert set(synsets)==set(gt_synsets)\n",
    "                except Exception as e:\n",
    "                    print(i, j)\n",
    "                    print(synsets, gt_synsets)\n",
    "                    invalid_res.append((i,j))\n",
    "\n",
    "        if len(invalid_res)==0:\n",
    "            break\n",
    "        else:\n",
    "            for i,j in invalid_res:\n",
    "                print(f'repair {(i,j)}')\n",
    "                content = concept_templates[j]\n",
    "                while 1:\n",
    "                    try:\n",
    "                        res = openai_chatgpt_post(content)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                all_chatgpt_res[i][j] = res\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" extract key-value-list from @chatgpt-res \"\"\"\n",
    "    extracted_chatgpt_res = []\n",
    "    for j, row in enumerate(all_chatgpt_res[0]):\n",
    "        # all_chatgpt_res[0][j] = \n",
    "        chatgpt_row_res = {}\n",
    "        extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "        remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "        extract_synnames = lambda r: list(map(lambda x: x.split(': ')[1].split('; '), r))\n",
    "        for i in range(n_repeat):\n",
    "            row = all_chatgpt_res[i][j]\n",
    "            row_data = remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n'))\n",
    "            synsets = extract_synsetid(row_data)\n",
    "            synnames = extract_synnames(row_data)\n",
    "            gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "            start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "            synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "            for idx_s, s in enumerate(synsets):\n",
    "                chatgpt_row_res.setdefault(s, [])\n",
    "                chatgpt_row_res[s].append( remove_space(synnames[idx_s]) )\n",
    "        extracted_chatgpt_res.append(chatgpt_row_res)\n",
    "\n",
    "    \"\"\" deduplication \"\"\"\n",
    "    use_dedup = True\n",
    "    all_candidates = []\n",
    "    all_candidates_set = []\n",
    "    for i, row in enumerate(extracted_chatgpt_res):\n",
    "        ### flatten multiple results\n",
    "        row_all_synset_names = list(map(lambda x: x.split('.')[0], row.keys()))\n",
    "        row_candidates = {}\n",
    "        row_candidates_set = {}\n",
    "        for k, v in row.items():\n",
    "            candidates = list(reduce(lambda x, y: x+y, v))\n",
    "            candidates = [c for c in candidates if c not in row_all_synset_names] ### remove competing synset names\n",
    "            set_candidates = set(candidates)\n",
    "            k = k.split('.')[0] ### key synset name\n",
    "            row_candidates.setdefault(k, [])\n",
    "            row_candidates_set.setdefault(k, set([]))\n",
    "            row_candidates[k].extend(candidates)\n",
    "            row_candidates_set[k] |= set_candidates\n",
    "        ### collect duplicates\n",
    "        duplicates = set()\n",
    "        for k1, v1 in row.items():\n",
    "            k1 = k1.split('.')[0]\n",
    "            for k2, v2 in row.items():\n",
    "                k2 = k2.split('.')[0]\n",
    "                if k1!=k2:\n",
    "                    duplicates |= row_candidates_set[k1]&row_candidates_set[k2]\n",
    "        ### remove duplication with synset-names (keys)\n",
    "        row_candidates_update = {}\n",
    "        row_candidates_set_update = {}\n",
    "        for k1, v1 in row.items():\n",
    "            k1 = k1.split('.')[0]\n",
    "            for k2, v2 in row.items():\n",
    "                k2 = k2.split('.')[0]\n",
    "            row_candidates_set_update[k1] = row_candidates_set[k1] - duplicates if use_dedup else row_candidates_set[k1]\n",
    "            row_candidates_update[k1] = [item for item in row_candidates[k1] if item not in duplicates ] if row_candidates_set[k1] else row_candidates[k1]\n",
    "\n",
    "        all_candidates.append(row_candidates_update)\n",
    "        all_candidates_set.append(row_candidates_set_update)\n",
    "\n",
    "\n",
    "    ### check non-empty\n",
    "    empty_list = []\n",
    "    for i, line in enumerate(all_candidates_set):\n",
    "        for k, v in line.items():\n",
    "            if len(v)==0:\n",
    "                for j in range(n_repeat):\n",
    "                    empty_list.append(j)\n",
    "                    print(f'repair {i} {j}')\n",
    "                    while 1:\n",
    "                        try:\n",
    "                            res = openai_chatgpt_post(concept_templates[i])\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    all_chatgpt_res[j][i] = res\n",
    "\n",
    "    if len(empty_list)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3695fa28-601f-465e-8862-e405d825c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "{\n",
    "    'all_candidates': all_candidates,\n",
    "    'all_candidates_set': all_candidates_set,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b053c196-dfb3-4491-9ad6-e6d400f4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" counter sorting \"\"\"\n",
    "all_candidates = data['all_candidates']\n",
    "all_counter_candidates = []\n",
    "all_number_candidates = []\n",
    "for row in all_candidates:\n",
    "    row_counter = {}\n",
    "    total_num = 0\n",
    "    for k, v in row.items():\n",
    "        ct = Counter(v)\n",
    "        row_counter[k] = OrderedDict(sorted(ct.items())) ### order key\n",
    "        total_num += sum(ct.values())\n",
    "    all_counter_candidates.append(OrderedDict(sorted(row_counter.items()))) ### order key\n",
    "    all_number_candidates.append(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcc250a1-2d48-41ef-a166-649d1662be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### flatten\n",
    "all_row_mapping_idx_synset_name = []\n",
    "all_row_chatgpt_names = []\n",
    "all_row_i_syn = []\n",
    "all_row_weight = []\n",
    "all_row_key_name = []\n",
    "for i in range(len(all_counter_candidates)):\n",
    "    row_synset_names = all_counter_candidates[i].keys()\n",
    "    row_mapping_idx_synset_name = dict(zip(range(len(row_synset_names)), row_synset_names))\n",
    "    row_i_syn = []\n",
    "    row_chatgpt_names = []\n",
    "    row_weight = []\n",
    "    for i_syn, syn in enumerate(row_synset_names):\n",
    "        row_i_syn.extend([i_syn for _ in range(len(all_counter_candidates[i][syn]))])\n",
    "        row_chatgpt_names.extend(list(all_counter_candidates[i][syn]))\n",
    "        row_weight.extend(list(all_counter_candidates[i][syn].values()))\n",
    "    \n",
    "    all_row_mapping_idx_synset_name.append(row_mapping_idx_synset_name)\n",
    "    all_row_chatgpt_names.append(row_chatgpt_names)\n",
    "    all_row_i_syn.append(row_i_syn)\n",
    "    all_row_weight.append(row_weight)\n",
    "    all_row_key_name.append(list(map(lambda x: row_mapping_idx_synset_name[x], row_i_syn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a821d8-1775-4645-a77a-0269bc30fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=None):\n",
    "    \"\"\" build classifier for chatgpt\n",
    "    Args:\n",
    "        all_row_chatgpt_names: [[names]]\n",
    "    \"\"\"\n",
    "    if all_row_key_name is None: ### single name\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 1\n",
    "            templates = json.load(f)['imagenet']\n",
    "    else:\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 2\n",
    "            templates = json.load(f)[f'{args.dataset}-parent-3']\n",
    "            \n",
    "    len_t = len(templates)\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_row_chatgpt_names)) as pbar:\n",
    "        for idx, row in enumerate(all_row_chatgpt_names):\n",
    "            len_row = len(row)\n",
    "            if all_row_key_name is None:\n",
    "                row_t = [ t.format(name) for name in row for t in templates ]\n",
    "            else:\n",
    "                row_t = [ t.format(pname, name) for pname, name in zip(all_row_key_name[idx], row) for t in templates ]\n",
    "            row_t = tokenize(row_t).to(args.device)\n",
    "            features = model.encode_text(row_t)\n",
    "            features = features.view(len_row, len_t, -1).float()\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            features = features.mean(dim=1)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1145633a-4d2e-4ab6-93dd-1d24d31ee023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "all_row_classifier = build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=all_row_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61b61a-000c-43ba-bfa2-d652de176881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/vfeatures-{args.dataset}.npy')\n",
    "# is_correct = []\n",
    "# k_2 = 1\n",
    "# enable_weight = True\n",
    "# instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "# for c in range(len(all_row_classifier)):\n",
    "#     select = (record_pred_kmeans_t==c)\n",
    "#     row_classifier = all_row_classifier[c]\n",
    "#     sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "#     sim_topk = sim.topk(k=k_2)\n",
    "#     # reliable_samples = sim_topk.values.flatten().topk(k=int(0.9*sim.size(0))).indices\n",
    "#     ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "#     count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "#     count_names[ind] = val ### count of each name\n",
    "#     count_smask = []\n",
    "#     smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "#     for s in np.unique(smask):\n",
    "#         if enable_weight:\n",
    "#             row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "#             # row_weight /= row_weight.sum()\n",
    "#             # row_weight = torch.ones(len(all_row_weight[c])).float()\n",
    "#             # row_weight[smask==s] = row_weight[smask==s] / row_weight[smask==s].sum()\n",
    "#             row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "#             row_weight /= row_weight.sum()\n",
    "#             count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "#         else:\n",
    "#             count_smask.append(count_names[smask==s].sum())\n",
    "#     name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "#     name_gt = all_gt_voc[select].mode().values\n",
    "#     name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "#     is_correct.append(name_pred==name_gt)\n",
    "#     instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73726305-326d-4047-b855-daed75f86105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/features/vfeatures-{args.dataset}.npy')\n",
    "vfeatures = all_vfeatures\n",
    "all_clu_pred_chatgpt = torch.zeros_like(all_clu_pred)\n",
    "is_correct = []\n",
    "k_2 = 3\n",
    "enable_weight = True\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "for c in range(len(all_row_classifier)):\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = all_row_classifier[c]\n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each name\n",
    "    count_smask = []\n",
    "    smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        if enable_weight:\n",
    "            row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "            row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "            row_weight /= row_weight.sum()\n",
    "            count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        else:\n",
    "            count_smask.append(count_names[smask==s].sum())\n",
    "    name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "    name_gt = all_gt_voc[select].mode().values\n",
    "    name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "    is_correct.append(name_pred==name_gt)\n",
    "    instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]\n",
    "    \n",
    "    val_count = torch.tensor(count_smask)\n",
    "    ind_count = [ all_row_mapping_idx_synset_name[c][ii] for ii in range(k_1) ]\n",
    "    ind_count = torch.tensor([vocab.mapping_names_idx[xx] for xx in ind_count])\n",
    "    all_clu_pred_chatgpt[c, ind_count] = val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c058e853-7b70-4c67-a46d-d689cbc32012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_acc=0.39, instance_acc=0.38294899463653564, missing=60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]), torch.Size([100]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_acc = np.array(is_correct).mean().item()\n",
    "instance_acc = (instance_pred_voc==all_gt_voc).float().mean().item()\n",
    "missing = all_gt_voc.unique().size(0) - all_gt_voc[(instance_pred_voc==all_gt_voc)].unique().size(0)\n",
    "\n",
    "print(f'name_acc={name_acc}, instance_acc={instance_acc}, missing={missing}')\n",
    "instance_pred_voc.unique().shape, all_gt_voc.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8db0d60-45de-46db-82b8-9cf10ee6c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_voc_clu = dict(zip(instance_pred_voc.unique().numpy().tolist(), range(len(instance_pred_voc))))\n",
    "r_pred_kmeans_t = np.array([mapping_voc_clu[item.item()] for item in instance_pred_voc])\n",
    "\n",
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt-uk{args.estimate_k}.pth', r_pred_kmeans_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfd56e80-c545-4fd7-aa76-033e3b51d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "\n",
    "# with open(f'./cache/openai/inov-cluster_visual_chatgpt-repeat={n_repeat}-k_1={k_1}-dataset={args.dataset}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "instance_acc = ((instance_pred_voc==all_gt_voc) | (cluster_ind_voc.cpu()==all_gt_voc)).float().mean().item()\n",
    "print(instance_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f541e79a-8854-4c18-8d3d-26ddba25bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_assign\n",
      "assignment shape=(100, 20071)\n",
      "instance label acc:: 0.38294899463653564\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/213 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "a, res_ass = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "r_pred_kmeans_t, r_cluster_ind_voc = reassign_by_pred_cluster(a, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fba0b7de-3708-4a81-ab75-8ef96501a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 62\n",
      "iou voc:: 0.2345679012345679\n",
      "cluster acc 0.7707804626871161\n"
     ]
    }
   ],
   "source": [
    "set_pred = set(res_ass[1].tolist())\n",
    "set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "iou_voc = n_inter/n_union\n",
    "n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "print('missing label::', n_missing_label)\n",
    "print('iou voc::', iou_voc)\n",
    "print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=r_pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e95f8569-6111-46f5-aa44-73f94a022318",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt-uk206.pth', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddece1d5-371e-4ea2-b057-45e6a1c52f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt.pth', r_pred_kmeans_t.cpu().numpy())\n",
    "# np.save(f'/home/sheng/sssa/ipynb/cache/cluster/cache-inov-{args.dataset}-clip-chatgpt-iter=1.pth', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f089427d-e49d-42d3-b96b-8ec64be6b327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([167, 167, 167, ..., 232, 232, 232])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f'/home/sheng/sssa/ipynb/cache/cluster/cache-inov-{args.dataset}-clip-chatgpt.pth.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158387f-c808-4a68-bb83-a887f9cc62b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### vocab ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f40302f9-1061-44b9-8651-50ba1e4128a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 3\n",
    "topk_all_clu_pred = (classifier@classifier.t()).topk(k=k_1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d905939-2990-4c9f-88c8-21c341c325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86ecafa0-5463-40b1-9e6a-d577a29377bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/sssa/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0d2d4-4343-40bc-be8c-f944383e7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60/20071 [06:52<38:59:28,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8c6d3b0b746aa518c8842e4e54019d28 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 126/20071 [15:12<47:16:45,  8.53s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b6404-bfff-49b5-a283-313d7efbd14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
