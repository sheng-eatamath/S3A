{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9697f3d5-2ba7-469c-a775-6eeb71e15a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6b1eee8-530e-4b7b-92b7-94b1a53b7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cuda:0'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'make_entity13'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/checkpoint-current.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99258ee0-367e-4f99-8eff-f0a2c118c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb3955f8-fecd-44cb-86ce-fba688a93077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" build entire wn-graph \"\"\"\n",
    "from nxgraph_model import *\n",
    "\n",
    "with open('/home/sheng/dataset/wordnet_nouns_with_synset.pkl', 'rb') as f:\n",
    "    entire_vocab = pickle.load(f)\n",
    "    \n",
    "G = create_graph([wn.synset(x) for x in entire_vocab['synsets']], entire_vocab['ids'], entire_vocab['names'], entire_vocab['def'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4e31e8a-5026-4a3f-bdaa-7c2b62707b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 334718\n",
      "missing keys:\n",
      "[]\n",
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_val, seed=0)\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "print('dataset size', len(dataset))\n",
    "\n",
    "# model, preprocess = load_clip(args)\n",
    "model = load_clip2(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cafe5f-0418-40a3-8460-5b0cacb29677",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### subsample in21k graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606f9c1-993c-41d1-b11e-a2182ebccc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute parent subgraph with hierarchical closure \"\"\"\n",
    "### compute hierarchical closure of @classes\n",
    "classes_closure = classes\n",
    "for c in classes:\n",
    "    classes_closure = classes_closure | predecessor_set(G, source=c)\n",
    "\n",
    "subgraph = G.subgraph(list(classes_closure))\n",
    "\n",
    "parent_classes = set()\n",
    "for c in classes:\n",
    "    parent_classes = parent_classes | predecessor_set(G, source=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e80709bf-c668-4c33-9418-3dec71d39722",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  8.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "1. compute parent classes at K level\n",
    "2. build classifier per name\n",
    "\"\"\"\n",
    "k_hier = 6\n",
    "parent_classes_at_k = list(filter(lambda x: G.nodes[x]['depth']==k_hier, parent_classes))\n",
    "# print(len(parent_classes_at_k))\n",
    "# pprint(parent_classes_at_k, compact=True)\n",
    "parent_classes_at_k = sorted(list(set(map(lambda x: x.split('.')[0], parent_classes_at_k))))\n",
    "classifier = build_classifier(args, model, templates, parent_classes_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e734847b-cc92-4033-bb52-e5121d214a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_synset_to_parents = {k: predecessor_set(G, source=k) for k in classes}\n",
    "vfeatures = np.load(f'./cache/vfeatures-{args.dataset}.npy')\n",
    "\n",
    "sim = torch.from_numpy(vfeatures)@classifier.cpu().t()\n",
    "pred_topk = sim.topk(k=5).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "962e45c9-5881-4af0-99a4-0560a08643f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_topk = np.array(parent_classes_at_k)[pred_topk.flatten()].reshape(-1, 5)\n",
    "to_name = lambda r: list(map(lambda x: x.split('.')[0], r))\n",
    "instance_to_parents = [\n",
    "    to_name(reduce(lambda x,y: x|y, list(map(lambda x: mapping_synset_to_parents[x.name()], mapping_vocidx_to_synsets(x.item(), vocab)))))\n",
    "    for x in all_gt_voc\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c2d0c9d2-0151-406d-85a2-10b741316a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" visualization: adding `type` causes confusion of classifier \"\"\"\n",
    "# parent_classes_at_k = list(filter(lambda x: G.nodes[x]['depth']==k_hier, parent_classes))\n",
    "# parent_classes_at_k = sorted(list(parent_classes_at_k))\n",
    "# name_parent_classes_at_k = list(map(lambda x: x.split('.')[0], parent_classes_at_k))\n",
    "# parent_parent_classes_at_k = [ ', '.join([pp.name().split('.')[0] for pp in wn.synset(p).hypernyms()]) for p in parent_classes_at_k]\n",
    "# classifier_p = build_classifier(args, model, templates, name_parent_classes_at_k, parent_parent_classes_at_k)\n",
    "\n",
    "# sim = classifier@classifier.t()\n",
    "# sns.distplot(sim.topk(k=10).values[:, 1].cpu().numpy(), bins=100)\n",
    "\n",
    "# sim = classifier_p@classifier_p.t()\n",
    "# sns.distplot(sim.topk(k=10).values[:, 1].cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8da2e-1cc6-41f6-8ca8-d2d320517c6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f663af-6f48-4314-b08b-77e9b888b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_classifier(args, model, templates, vocab_classnames, parent_classnames=None):\n",
    "    batch_size = 64\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        assert parent_classnames is None\n",
    "        with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "            for classname_set in np.array_split(vocab_classnames, len(vocab_classnames)//batch_size):\n",
    "                texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "                texts = tokenize(texts).to(args.device) #tokenize\n",
    "                class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "                class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "                class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "                class_embedding = class_embeddings.mean(dim=1)\n",
    "                class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "                zeroshot_weights.append(class_embedding.cpu())\n",
    "                pbar.update(1)\n",
    "        # else:\n",
    "        #     with tqdm(total=len(vocab_classnames)//batch_size) as pbar:\n",
    "        #         for classname_set, parentname_set in zip(\n",
    "        #             np.array_split(vocab_classnames, len(vocab_classnames)//batch_size),\n",
    "        #             np.array_split(parent_classnames, len(parent_classnames)//batch_size),\n",
    "        #         ):\n",
    "        #             texts = [template.format(classname)+f' A type of {pname}.' for classname, pname in zip(classname_set, parentname_set) for template in templates] #format with class\n",
    "        #             texts = tokenize(texts).to(args.device) #tokenize\n",
    "        #             class_embeddings = model.encode_text(texts).float() #embed with text encoder\n",
    "        #             class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "        #             class_embeddings = F.normalize(class_embeddings, dim=-1)\n",
    "        #             class_embedding = class_embeddings.mean(dim=1)\n",
    "        #             class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "        #             zeroshot_weights.append(class_embedding.cpu())\n",
    "        #             pbar.update(1)\n",
    "    classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1ff2189-0380-48f4-bb3f-8d819c274688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:41<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = build_classifier(args, model, templates, vocab.classnames)\n",
    "torch.save(classifier, './cache/wordnet_classifier_in21k_word.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd907a29-a14e-41fc-81b8-7756f2eabc34",
   "metadata": {},
   "source": [
    "### performance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3751c-e1d4-4c63-84d1-8827f2dff0ce",
   "metadata": {},
   "source": [
    "#### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1adcb05-b0e4-4806-93c4-142f00f333c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip(args):\n",
    "    model, preprocess = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model, preprocess\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "def load_mixture_clip(args, decay=1.0):\n",
    "    model1 = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model1.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model1.to(args.device).eval()\n",
    "    model2 = clip.load(args.arch)\n",
    "    model2.to(args.device).eval()\n",
    "    with torch.no_grad():\n",
    "        msd = model1.state_dict()\n",
    "        for k, ema_v in model2.state_dict().items():\n",
    "            # if needs_module:\n",
    "            #     k = 'module.' + k\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * decay + (1. - decay) * model_v)\n",
    "    return model2\n",
    "\n",
    "def topk_acc(all_pred_voc_topk, all_gt_voc):\n",
    "    acc = []\n",
    "    ### topK accuracy\n",
    "    for i in range(all_pred_voc_topk.size(1)):\n",
    "        vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "        for j in range(i+1):\n",
    "            vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "        print(f'k={i} acc={vec.float().mean()}')\n",
    "        acc.append(vec.float().mean().item())\n",
    "    return acc\n",
    "\n",
    "def semantic_acc(y_pred, y_true, metrics={}):\n",
    "    \"\"\" compute soft semantic acc for @y_pred and @y_true \"\"\"\n",
    "    assert len(metrics)>0\n",
    "    assert y_pred.size(0)==y_true.size(0)\n",
    "    scores = {m:[] for m in metrics.keys()}\n",
    "    with tqdm(total=y_pred.size(0)) as pbar:\n",
    "        for i in range(y_pred.size(0)):\n",
    "            syn_pred = mapping_vocidx_to_synsets(y_pred[i].item(), vocab)\n",
    "            syn_true = mapping_vocidx_to_synsets(y_true[i].item(), vocab)\n",
    "            pairs = list(itertools.product(range(len(syn_pred)), range(len(syn_true))))\n",
    "            for m_name, m in metrics.items():\n",
    "                scores[m_name].append( max([ m(syn_pred[p[0]], syn_true[p[1]]) for p in pairs ]) )\n",
    "            pbar.update(1)\n",
    "    for m_name in metrics.keys():\n",
    "        scores[m_name] = np.array(scores[m_name]).mean()\n",
    "    return scores\n",
    "    \n",
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48060709-150f-490c-8967-e15adfbdf404",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### naive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df58b595-ee52-4ec2-979e-270d85d6c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2503/2503 [24:41<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "all_pred_voc_topk = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_pred_voc_topk.append(prob.topk(k=5, dim=-1).indices.cpu())\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_pred_voc_topk = torch.cat(all_pred_voc_topk, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f947de4f-bd2c-4d9b-b55c-40692f45cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc=0.33346137404441833\n",
      "n_missing=2\n"
     ]
    }
   ],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')\n",
    "n_missing = len(set(all_gt_voc.unique().numpy()) - set(all_pred_voc.unique().numpy()))\n",
    "print(f'n_missing={n_missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc069df8-4c3f-41ab-8b3d-983a241ecf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [01:06<00:00, 2002.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wup_similarity': 0.751388662643735, 'lch_similarity': 2.3188224544809772}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score_baseline = semantic_acc(all_pred_voc, all_gt_voc, \n",
    "                              metrics={'wup_similarity': wn.wup_similarity, 'lch_similarity': wn.lch_similarity})\n",
    "print(score_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d49fec-3e32-45c7-88e0-27b887448e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=0 acc=0.33335593342781067\n",
      "k=1 acc=0.45885586738586426\n",
      "k=2 acc=0.5238654613494873\n",
      "k=3 acc=0.5691183805465698\n",
      "k=4 acc=0.6037208437919617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33335593342781067,\n",
       " 0.45885586738586426,\n",
       " 0.5238654613494873,\n",
       " 0.5691183805465698,\n",
       " 0.6037208437919617]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_acc(all_pred_voc_topk, all_gt_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0f21c-afc2-4bd0-9aca-225dfd61fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vfeatures = np.load(f'./cache/vfeatures-{args.dataset}.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dcb0b6d-188a-4a13-a799-77703b1c5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind, val = all_pred_voc.unique(return_counts=True)\n",
    "sns.distplot(val.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fa4202c-72bf-4282-ab9c-5080029f5f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3488)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(val<2).sum()/ind.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94384c09-940e-4fc4-b6cb-28cb36b3fbea",
   "metadata": {},
   "source": [
    "#### SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19b10c10-78de-45c9-b436-1bd3da399979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(args.input_size, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=torch.tensor(mean),\n",
    "        std=torch.tensor(std))\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=0)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26d0db35-1f20-463c-954d-c375b31039c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    n_count = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count.append( selected.sum().item() )\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected]).ravel(), return_counts=True)\n",
    "        # counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    n_count = torch.tensor(n_count).cpu()\n",
    "    \n",
    "    # all_clu_pred = setdiff_assignment(all_clu_pred)\n",
    "    \n",
    "    all_clu_pred = all_clu_pred/(n_count.view(-1, 1) + 1e-20)\n",
    "    \n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc, return_results=False):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    inst_acc = (label_voc_kmeans==all_gt_voc).float().mean().item()\n",
    "    print('instance label acc::', inst_acc)\n",
    "    if return_results:\n",
    "        return label_voc_kmeans, res_ass, inst_acc\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, model, classifier, device, \n",
    "                             all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                            prob = classifier_selected[similarity.softmax(-1)]\n",
    "                            cluster_ind.append(prob.cpu().argmax(dim=-1))\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    if classifier_selected is not None:\n",
    "        cluster_ind_voc = classifier_selected[cluster_ind]\n",
    "    else:\n",
    "        cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, \n",
    "                             preextracted_vfeatures=None):\n",
    "    \"\"\" given vocab label set @label_voc_kmeans, \n",
    "    Args:\n",
    "        label_voc_kmeans: cluster-assigned label on vocab\n",
    "        ...\n",
    "        preextracted_vfeatures: np.array([N x D])\n",
    "    Returns:\n",
    "        cluster_ind: tensor([N]): re-ordered cluster assignment\n",
    "        cluster_ind_voc: tensor([N]): cluster assignment indiced by vocab\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device).unique()\n",
    "    cluster_ind = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        if preextracted_vfeatures is not None:\n",
    "            N = len(loader_f.dataset)\n",
    "            batch_size = 10000\n",
    "            indices = np.array_split(np.arange(N), N//batch_size)\n",
    "            with torch.no_grad():\n",
    "                for group in indices:\n",
    "                    logits = torch.from_numpy(preextracted_vfeatures[group]).float()\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits@classifier.t().cpu()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    cluster_ind.append(prob[:, label_voc_kmeans.cpu()].argmax(dim=-1))\n",
    "        else:\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if preextracted_vfeatures is not None:\n",
    "                            logits = torch.from_numpy(preextracted_vfeatures[idx_img.cpu().numpy()]).float().to(device)\n",
    "                        else:\n",
    "                            logits = model.ema.extract_vfeatures(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        similarity = 100 * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "    cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def computation_reassign_by_pred_cluster(row, idx, args, model, classifier, candidate_classifier_ind):\n",
    "    \"\"\"\n",
    "    candidate_classifier_ind = label_voc_kmeans.unique().to(args.device)\n",
    "    \"\"\"\n",
    "    images, label_voc, label_clu, idx_img = row[:4]\n",
    "    images = images.to(args.device)\n",
    "    with amp_autocast():\n",
    "        vfeatures = model.visual(images).float()\n",
    "        # vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "    vfeatures = F.normalize(vfeatures, dim=-1)\n",
    "    batch_sim = 100*vfeatures@classifier[candidate_classifier_ind].t()\n",
    "    cluster_ind = batch_sim.argmax(dim=-1)\n",
    "    cluster_ind_voc = candidate_classifier_ind[cluster_ind].cpu()\n",
    "    return cluster_ind_voc\n",
    "\n",
    "def aggregation_reassign_by_pred_cluster(r, candidate_classifier_ind):\n",
    "    cluster_ind_voc = torch.cat(r, dim=0)\n",
    "    mapping_ind = dict(zip(cluster_ind_voc.unique().numpy(), torch.arange(cluster_ind_voc.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind_voc])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vfeatures(model, data_loader, device):\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(data_loader)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(data_loader):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(device)\n",
    "            with amp_autocast():\n",
    "                vfeatures = model.visual(images).float()\n",
    "            vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "            all_vfeatures.append(vfeatures.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    return all_vfeatures\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def loop_row_collect_results_nograd(obj_iter, computations={}, aggregations={}):\n",
    "    \"\"\" compute and aggregate results, looping over @obj_iter \n",
    "    func_computation(@row, @index_row)\n",
    "    aggregations(list(@results_computation))\n",
    "    \"\"\"\n",
    "    assert set(list(computations.keys())) == set(list(aggregations.keys()))\n",
    "    collector = { k:[] for k in computations }\n",
    "    with tqdm(total=len(obj_iter)) as pbar:\n",
    "        for i, row in enumerate(obj_iter):\n",
    "            ### apply computations\n",
    "            for k, func in computations.items():\n",
    "                collector[k].append(func(row, i))\n",
    "            pbar.update(1)\n",
    "    ### aggregate results\n",
    "    results = {}\n",
    "    for k, func_agg in aggregations.items():\n",
    "        results[k] = func_agg(collector[k])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbf7bb34-74cb-4c02-8345-0c14d1de9cb0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 70/260 [01:11<03:14,  1.02s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 1\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                # logits = model.extract_vfeatures(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)\n",
    "\n",
    "# pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset}-train-clip.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/pred_kmeans_t.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/pred_kmeans_t.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "#     # results = \\\n",
    "#     # loop_row_collect_results_nograd(loader_f, \n",
    "#     #                                 computations={\n",
    "#     #                                     'reassign_by_pred_cluster': partial(\n",
    "#     #                                         computation_reassign_by_pred_cluster, \n",
    "#     #                                         args=args, \n",
    "#     #                                         model=model, \n",
    "#     #                                         classifier=classifier, \n",
    "#     #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "#     #                                     ),\n",
    "#     #                                 }, \n",
    "#     #                                 aggregations={\n",
    "#     #                                     'reassign_by_pred_cluster': partial(\n",
    "#     #                                         aggregation_reassign_by_pred_cluster,\n",
    "#     #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "#     #                                     ),\n",
    "#     #                                 },\n",
    "#     #                                )\n",
    "#     # pred_kmeans_t, cluster_ind_voc = results['reassign_by_pred_cluster']\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2771d1a-0cad-4197-96bc-c6c34039a0da",
   "metadata": {},
   "source": [
    "unknown K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8998237-f918-4bfe-975a-cfc5bc221890",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [18:34<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_by_pred_cluster\n",
      "is mutex assignment:: False\n",
      "assignment collision num:: 18\n",
      "linear_assign\n",
      "assignment shape=(252, 20071)\n",
      "instance label acc:: 0.3282763361930847\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/654 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 137\n",
      "iou voc:: 0.3161953727506427\n",
      "cluster acc 0.7031889530888689\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(252, 20071)\n",
      "instance label acc:: 0.3916222155094147\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/654 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 137\n",
      "iou voc:: 0.3161953727506427\n",
      "cluster acc 0.7052802657759666\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(252, 20071)\n",
      "instance label acc:: 0.39301741123199463\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/654 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 137\n",
      "iou voc:: 0.3161953727506427\n",
      "cluster acc 0.7052802657759666\n"
     ]
    }
   ],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)\n",
    "\n",
    "pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/gcd-cluster-estimation/cache/{args.dataset}-clustering_pred-{str(args.estimate_k)}.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "    n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "    iou_voc = n_inter/n_union\n",
    "    n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "    print('missing label::', n_missing_label)\n",
    "    print('iou voc::', iou_voc)\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    # history_set_pred.append(set_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ac4c4-8b8f-4694-9d8e-213515a95464",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-uk{str(args.estimate_k)}.pth', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725cf0b-072d-43cd-a4a0-cbd70c5ea17f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/gcd-cluster-estimation/cache/{args.dataset}-clustering_pred-{str(2*args.estimate_k)}.npy'))\n",
    "# pred_kmeans_t = pred_kmeans\n",
    "# for t in range(3):\n",
    "#     record_pred_kmeans_t = pred_kmeans_t\n",
    "#     all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "#     label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "#     pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "#     set_pred = set(res_ass[1].tolist())\n",
    "#     set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "#     print('missing label::', len(set_gt - set_pred))\n",
    "#     print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "#     # history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd96208-97a8-4f74-bad5-35333967f7b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### search K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff0c013-dcf0-4b28-9e4a-a8519734ed99",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(5):\n",
    "    print('='*20)\n",
    "    print(f'k={k}')\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "    pred_kmeans_t = pred_kmeans\n",
    "    history_set_pred = []\n",
    "    for t in range(3):\n",
    "        record_pred_kmeans_t = pred_kmeans_t\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc[:, :k], voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "        history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57de3f0-5ded-491c-a330-3429a52e1e14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88cbe101-f03f-4da8-bf92-a1f093aa90fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_by_pred_cluster\n",
      "is mutex assignment:: False\n",
      "assignment collision num:: 374\n"
     ]
    }
   ],
   "source": [
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/gcd-cluster-estimation/cache/make_entity30-clustering_pred-206.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}-2k.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/pred_kmeans_t.npy'))\n",
    "# pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/pred_kmeans_t.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "record_pred_kmeans_t = pred_kmeans_t\n",
    "all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff82a35-9c87-4519-a602-85aeb834cbea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}-2k.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bbea9-6dac-4b9e-ae74-215cbdf5831a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "for decay in np.linspace(0.0,1.0,11):\n",
    "    print('='*20)\n",
    "    print(f'decay={decay}')\n",
    "    print('='*20)\n",
    "    model = load_mixture_clip(args, decay=decay)\n",
    "    ### collect variables\n",
    "    prob_k = 5\n",
    "    all_topk_voc = []\n",
    "    all_gt_voc = []\n",
    "    all_label_clu = []\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_f):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(args.device)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual.extract_features(images)\n",
    "                    # logits = model.extract_vfeatures(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits @ classifier.t()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                    all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                    all_gt_voc.append(label_voc)\n",
    "                    all_label_clu.append(label_clu)\n",
    "                    all_vfeatures.append(logits.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "    all_topk_voc = np.concatenate(all_topk_voc)\n",
    "    all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "    all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "\n",
    "    # pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset}-train-clip.npy'))\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "    # pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/pred_kmeans_t.npy'))\n",
    "    # pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/pred_kmeans_t.npy'))\n",
    "    pred_kmeans_t = pred_kmeans\n",
    "    history_set_pred = []\n",
    "    for t in range(3):\n",
    "        record_pred_kmeans_t = pred_kmeans_t\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "        history_set_pred.append(set_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8c5e7-5d14-4f5d-b2a8-a21d66ca7d4d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "\n",
    "### collect variables\n",
    "prob_k = 10\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_idx_img = []\n",
    "all_vfeatures = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_idx_img.append(idx_img.cpu().numpy())\n",
    "                all_vfeatures.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_idx_img = np.concatenate(all_idx_img)\n",
    "all_vfeatures = np.concatenate(all_vfeatures)\n",
    "\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "163393cd-1bee-4ffb-baeb-267853fc265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/inov-vocab-template_9-n_repeat=1-extract_synsets.pkl', 'rb') as f:\n",
    "    offline_db = pickle.load(f)['synsets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbd62535-3f2f-4204-a6ff-22ed498f8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batchify(iterable, batch_size=10000):\n",
    "    N = len(iterable)\n",
    "    for i in range(math.ceil(N/batch_size)):\n",
    "        yield iterable[i*batch_size: min(N, (i+1)*batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ffc613b-df68-4498-b7fe-6827a5679642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "n_knn=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [01:08<00:00, 1935.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion acc=0.6427974240198847\n",
      "====================\n",
      "n_knn=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [01:09<00:00, 1909.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion acc=0.6877113697134034\n",
      "====================\n",
      "n_knn=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [01:09<00:00, 1913.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion acc=0.7108575302225737\n",
      "====================\n",
      "n_knn=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [01:15<00:00, 1753.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion acc=0.7272247956916356\n",
      "====================\n",
      "n_knn=80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [01:19<00:00, 1669.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion acc=0.7312770685045005\n"
     ]
    }
   ],
   "source": [
    "\"\"\" performance w.r.t. @n_knn \"\"\"\n",
    "for n_knn in [5, 10, 20, 50, 80]:\n",
    "    print('='*20)\n",
    "    print(f'n_knn={n_knn}')\n",
    "    # n_knn = 20\n",
    "    all_inst_knn = []\n",
    "    is_in = []\n",
    "    with tqdm(total=len(all_idx_img)) as pbar:\n",
    "        for idx_batch, batch in zip(\n",
    "            batchify(all_idx_img, batch_size=1000), batchify(all_vfeatures, batch_size=1000)\n",
    "        ):\n",
    "            sim = torch.from_numpy(batch)@torch.from_numpy(all_vfeatures).t()\n",
    "            knn_ind = sim.topk(k=n_knn).indices\n",
    "            for idx, i in enumerate(idx_batch):\n",
    "                ind, val = torch.from_numpy(all_topk_voc)[knn_ind[idx]].flatten().unique(return_counts=True)\n",
    "                # scores = torch.arange(all_topk_voc.shape[1]).flip(0).repeat(n_knn) ### scores\n",
    "                inst_knn = ind[val.topk(k=5).indices]\n",
    "                all_inst_knn.append(inst_knn.numpy())\n",
    "                is_in.append( all_gt_voc[i] in inst_knn )\n",
    "\n",
    "                pbar.update(1)\n",
    "    is_in = np.mean(is_in)\n",
    "    print(f'inclusion acc={is_in}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "41f8ba6f-e1f2-4a09-9f84-c7e87756b50e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132765/132765 [02:05<00:00, 1055.65it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_knn = 20\n",
    "use_score = True\n",
    "all_inst_knn = []\n",
    "is_in = []\n",
    "with tqdm(total=len(all_idx_img)) as pbar:\n",
    "    for idx_batch, batch in zip(\n",
    "        batchify(all_idx_img, batch_size=1000), batchify(all_vfeatures, batch_size=1000)\n",
    "    ):\n",
    "        sim = torch.from_numpy(batch)@torch.from_numpy(all_vfeatures).t()\n",
    "        knn_ind = sim.topk(k=n_knn).indices\n",
    "        for idx, i in enumerate(idx_batch):\n",
    "            if use_score:\n",
    "                scores = torch.arange(all_topk_voc.shape[1]).flip(0).repeat(n_knn) ### scores\n",
    "                ind = torch.from_numpy(all_topk_voc)[knn_ind[idx]].flatten()\n",
    "                ind_list = ind.unique().numpy().tolist()\n",
    "                mappings = [dict(zip(range(len(ind_list)), ind_list)), dict(zip(ind_list, range(len(ind_list))))]\n",
    "                ind_scores = torch.zeros(len(ind_list))\n",
    "                add_ind = torch.tensor([ mappings[1][item.item()] for item in ind ])\n",
    "                ind_scores = ind_scores.index_add(0, add_ind, scores.float())\n",
    "                inst_knn = [ mappings[0][item.item()] for item in ind_scores.topk(k=5).indices ]\n",
    "                all_inst_knn.append(inst_knn)\n",
    "                is_in.append( all_gt_voc[i] in inst_knn )\n",
    "            else:\n",
    "                ind, val = torch.from_numpy(all_topk_voc)[knn_ind[idx]].flatten().unique(return_counts=True)\n",
    "                inst_knn = ind[val.topk(k=5).indices]\n",
    "                all_inst_knn.append(inst_knn.numpy())\n",
    "                is_in.append( all_gt_voc[i] in inst_knn )\n",
    "\n",
    "            pbar.update(1)\n",
    "is_in = np.mean(is_in)\n",
    "print(f'inclusion acc={is_in}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ade01121-3909-4315-8f90-4d8aa0f102c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inclusion acc=0.7350807818325613\n"
     ]
    }
   ],
   "source": [
    "print(f'inclusion acc={is_in}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "619a4966-b09a-4e4e-88c7-2240c267b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=None):\n",
    "    \"\"\" build classifier for chatgpt\n",
    "    Args:\n",
    "        all_row_chatgpt_names: [[names]]\n",
    "    \"\"\"\n",
    "    if all_row_key_name is None: ### single name\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 1\n",
    "            templates = json.load(f)['imagenet']\n",
    "    else:\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 2\n",
    "            templates = json.load(f)['imagenet-parent-3']\n",
    "            \n",
    "    len_t = len(templates)\n",
    "    row_classifier = []\n",
    "    # with tqdm(total=len(all_row_chatgpt_names)) as pbar:\n",
    "    for idx, row in enumerate(all_row_chatgpt_names):\n",
    "        len_row = len(row)\n",
    "        if all_row_key_name is None:\n",
    "            row_t = [ t.format(name) for name in row for t in templates ]\n",
    "        else:\n",
    "            row_t = [ t.format(name, pname) for pname, name in zip(all_row_key_name[idx], row) for t in templates ]\n",
    "        row_t = tokenize(row_t).to(args.device)\n",
    "        features = model.encode_text(row_t)\n",
    "        features = features.view(len_row, len_t, -1).float()\n",
    "        features = features/features.norm(dim=-1, keepdim=True)\n",
    "        features = features.mean(dim=1)\n",
    "        features = features/features.norm(dim=-1, keepdim=True)\n",
    "        row_classifier.append(features.cpu())\n",
    "            \n",
    "            # pbar.update(1)\n",
    "    return row_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "48a62cab-369b-4dca-a34c-3583a67661ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/inov-vocab-template_9-n_repeat=1-extract_synsets.pkl', 'rb') as f:\n",
    "    offline_db = pickle.load(f)\n",
    "    \n",
    "offline_db_synsets = offline_db['synsets']\n",
    "offline_db_counter = offline_db['synsets_counter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b0b8c8f4-27f7-48f6-b94e-742d91c684c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 280/132765 [01:57<15:23:06,  2.39it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_range = 5\n",
    "is_correct = []\n",
    "with tqdm(total=len(all_idx_img)) as pbar:\n",
    "    for idx_row, row in zip(\n",
    "        all_idx_img, all_topk_voc\n",
    "    ):\n",
    "        ### build candidate classifier\n",
    "        candidate_concepts = [mapping_vocidx_to_synsets(item.item(), vocab) for item in row]\n",
    "        candidate_concepts = list(map(lambda s: s.name(), reduce(lambda x, y: x+y, candidate_concepts)))\n",
    "        row_key_names = []\n",
    "        row_chatgpt_names = []\n",
    "        row_weights = []\n",
    "        for concept in candidate_concepts:\n",
    "            prompts = list(offline_db_counter[concept].keys())\n",
    "            row_key_names.extend([concept.split('.')[0] for _ in range(len(prompts))])\n",
    "            row_chatgpt_names.extend(prompts)\n",
    "            row_weights.extend( list(offline_db_counter[concept].values()) )\n",
    "        candidate_classifier = build_classifier_chatgpt([row_key_names], model, all_row_key_name=[row_chatgpt_names])\n",
    "        ### classify\n",
    "        list_candidate_concept_names = list(set(row_key_names))\n",
    "        scores_concepts = torch.zeros(len(list_candidate_concept_names))\n",
    "        concept_to_idx = dict(zip(list_candidate_concept_names, range(len(list_candidate_concept_names))))\n",
    "        vfeature = torch.from_numpy(all_vfeatures[idx_row])\n",
    "        sim = vfeature.view(1, -1)@candidate_classifier[0].t()\n",
    "        pred_ind = sim.topk(k=k_range).indices\n",
    "        idx_add = torch.tensor([ concept_to_idx[item] for item in np.array(row_key_names)[pred_ind.flatten().numpy()] ])\n",
    "        scores_concepts = scores_concepts.index_add(0, idx_add, torch.arange(k_range).flip(0) + 1.0)\n",
    "        # val, count = np.unique(np.array(row_key_names)[pred_ind.numpy()], return_counts=True)\n",
    "        pred_ind = scores_concepts.argmax(dim=-1)\n",
    "        pred_ind = vocab.mapping_names_idx[list_candidate_concept_names[pred_ind.item()]]\n",
    "        \n",
    "        is_correct.append(pred_ind==all_gt_voc[idx_row].item())\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f6da3-4078-4bb8-98c5-3554d974791d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a752005-a0fe-48c3-a091-48c78ecab1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in all_topk_voc:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdba9a-541d-460d-b79f-43b5f6fd827c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for row in cluster_row_synsets:\n",
    "    for k, v in row.items():\n",
    "        extracted_chatgpt_synsets.setdefault(k, [])\n",
    "        extracted_chatgpt_synsets[k].extend(v)\n",
    "\n",
    "extracted_chatgpt_synsets_counter = {}\n",
    "for k, v in extracted_chatgpt_synsets.items():\n",
    "    extracted_chatgpt_synsets_counter.setdefault(k, Counter())\n",
    "    extracted_chatgpt_synsets_counter[k] = extracted_chatgpt_synsets_counter[k] + Counter(v)\n",
    "    assert len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de297da-9448-4e43-9a2c-a17963818b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53cce3ec-83c7-49f2-89ee-ce7fc8c50ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./cache/features/vfeatures-{args.dataset}.npy', all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1913536-5442-4636-8d16-fa8e283be7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'all_clu_pred': all_clu_pred,\n",
    "    'label_voc_kmeans': label_voc_kmeans,\n",
    "    'pred_kmeans_t': pred_kmeans_t,\n",
    "    'cluster_ind_voc': cluster_ind_voc,\n",
    "    'record_pred_kmeans_t': record_pred_kmeans_t,\n",
    "    'all_gt_voc': all_gt_voc,\n",
    "    'all_label_clu': all_label_clu,\n",
    "    'all_topk_voc': all_topk_voc,\n",
    "}, f'./topk=1-cache-inov-{args.dataset}-clip.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7dfb287-384e-4c0f-aaf7-bfa31e2257f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(f'./cache-inov-{args.dataset}-clip.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']\n",
    "all_topk_voc = res['all_topk_voc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97be679d-4a37-41db-a3aa-ee0fc0b56bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{\"/\".join(args.clip_checkpoint.split(\"/\")[:-1])}/pred_kmeans_t.npy', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468da9a9-ac21-487e-91f6-61ff4cdd9929",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset}-train-clip.npy'))\n",
    "# pred_kmeans_t = all_label_clu\n",
    "# history_set_pred = []\n",
    "# for t in range(3):\n",
    "#     record_pred_kmeans_t = pred_kmeans_t\n",
    "#     all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "#     label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "#     pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, model, classifier, args.device)\n",
    "#     # results = \\\n",
    "#     # loop_row_collect_results_nograd(loader_f, \n",
    "#     #                                 computations={\n",
    "#     #                                     'reassign_by_pred_cluster': partial(\n",
    "#     #                                         computation_reassign_by_pred_cluster, \n",
    "#     #                                         args=args, \n",
    "#     #                                         model=model, \n",
    "#     #                                         classifier=classifier, \n",
    "#     #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "#     #                                     ),\n",
    "#     #                                 }, \n",
    "#     #                                 aggregations={\n",
    "#     #                                     'reassign_by_pred_cluster': partial(\n",
    "#     #                                         aggregation_reassign_by_pred_cluster,\n",
    "#     #                                         candidate_classifier_ind=label_voc_kmeans.unique().to(args.device),\n",
    "#     #                                     ),\n",
    "#     #                                 },\n",
    "#     #                                )\n",
    "#     # pred_kmeans_t, cluster_ind_voc = results['reassign_by_pred_cluster']\n",
    "#     set_pred = set(res_ass[1].tolist())\n",
    "#     set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "#     print('missing label::', len(set_gt - set_pred))\n",
    "#     print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "#     history_set_pred.append(set_pred)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9986833e-49b3-462d-9edb-05fb17976be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros_like(record_pred_kmeans_t)\n",
    "for i in range(all_clu_pred.size(0)):\n",
    "    a[record_pred_kmeans_t==i] = all_clu_pred.argmax(dim=-1)[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95241c08-1e83-4cfb-847f-a8a227fb3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw acc 0.7491432428359985\n"
     ]
    }
   ],
   "source": [
    "print('raw acc', (a==all_gt_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dec99c-d61b-4121-a428-ff51186c7113",
   "metadata": {},
   "source": [
    "#### CHATGPT request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c97155be-e602-4509-a3fa-e91d4a7a5413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    # completion = openai.Completion.create(\n",
    "    #     model=\"text-davinci-003\",\n",
    "    #     prompt=content,  \n",
    "    #     temperature=0.7,\n",
    "    #     max_tokens=256,\n",
    "    #     top_p=1,\n",
    "    #     frequency_penalty=0,\n",
    "    #     presence_penalty=0,\n",
    "    # )\n",
    "    # result = completion['choices'][0]['text']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c210a91-0cb2-429f-a936-87914e2ccc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@3 = 0.7341269850730896\n"
     ]
    }
   ],
   "source": [
    "all_clu_gt_voc = []\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "\n",
    "all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "k_1 = 3\n",
    "topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "for i in range(k_1):\n",
    "    cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "\n",
    "print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00ad4dbd-f755-4709-812c-f48f96e95801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46253372-83e5-4b5b-bb2b-d813d1795e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/OSZSL/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_13 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible adjective phrases of visual descriptions for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible visual descriptive phrases for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "\n",
    "    \n",
    "template_in_use = template_9_1\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16e8ec-4a49-4493-81d2-925d7a5347cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 13/219 [07:48<2:01:02, 35.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4ad16e8f1cc564e62dc5551bc3e96dd2 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 16/219 [10:01<2:12:04, 39.04s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20293320-5aca-4921-aec9-58806f30cc45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)\n",
    "    \n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-iter=1.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-vocab.pkl', 'wb') as f:\n",
    "#     pickle.dump(data, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "# with open(f'./cache/openai/visual-inov-template=5-k_1={k_1}-repeat={n_repeat}.pkl', 'rb') as f:\n",
    "#     all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14a08e2c-498e-449e-b972-3e9e670aba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-uk{args.estimate_k}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chatgpt_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb5cfa-2ec9-4824-b743-90b31b59a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/visual-inov-template=9-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-iter=1.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "878b68f2-a270-439c-bc5e-6e23c4f7ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/sheng/sssa/ipynb/cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "191fef52-2b54-4946-b3eb-c0a1e1db7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/openai/topk=1-visual-inov-template=9_1-k_1={k_1}-repeat={n_repeat}-data={args.dataset}-uk{args.estimate_k}.pkl', 'rb') as f:\n",
    "    all_chatgpt_res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84ba2f3d-8fb1-4e49-af3a-f7a61f79fced",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "while 1:\n",
    "    \"\"\" integrity check \"\"\"\n",
    "    while 1:\n",
    "        invalid_res = []\n",
    "        for i in range(n_repeat):\n",
    "            for j, row in enumerate(all_chatgpt_res[i]):\n",
    "                extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "                remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "                synsets = extract_synsetid(remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n')))\n",
    "                gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "                try:\n",
    "                    start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "                    synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "                    assert set(synsets)==set(gt_synsets)\n",
    "                except Exception as e:\n",
    "                    print(i, j)\n",
    "                    print(synsets, gt_synsets)\n",
    "                    invalid_res.append((i,j))\n",
    "\n",
    "        if len(invalid_res)==0:\n",
    "            break\n",
    "        else:\n",
    "            for i,j in invalid_res:\n",
    "                print(f'repair {(i,j)}')\n",
    "                content = concept_templates[j]\n",
    "                while 1:\n",
    "                    try:\n",
    "                        res = openai_chatgpt_post(content)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                all_chatgpt_res[i][j] = res\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" extract key-value-list from @chatgpt-res \"\"\"\n",
    "    extracted_chatgpt_res = []\n",
    "    for j, row in enumerate(all_chatgpt_res[0]):\n",
    "        # all_chatgpt_res[0][j] = \n",
    "        chatgpt_row_res = {}\n",
    "        extract_synsetid = lambda r: list(map(lambda x: x.split(': ')[0], r))\n",
    "        remove_space = lambda r: list(filter(lambda x: len(x), r))\n",
    "        extract_synnames = lambda r: list(map(lambda x: x.split(': ')[1].split('; '), r))\n",
    "        for i in range(n_repeat):\n",
    "            row = all_chatgpt_res[i][j]\n",
    "            row_data = remove_space(row.lower().replace('\\n\\n', '\\n').split('\\n'))\n",
    "            synsets = extract_synsetid(row_data)\n",
    "            synnames = extract_synnames(row_data)\n",
    "            gt_synsets = extract_synsetid(reduce(lambda x,y: x+y, cluster_row_synsets[j]))\n",
    "            start_idx = [ synsets[k].find(s) for k, s in enumerate(gt_synsets) ]\n",
    "            synsets = [ synsets[k][start_idx[k]:start_idx[k]+len(gt_synsets[k])] for k, s in enumerate(synsets) ]\n",
    "            for idx_s, s in enumerate(synsets):\n",
    "                chatgpt_row_res.setdefault(s, [])\n",
    "                chatgpt_row_res[s].append( remove_space(synnames[idx_s]) )\n",
    "        extracted_chatgpt_res.append(chatgpt_row_res)\n",
    "\n",
    "    \"\"\" deduplication \"\"\"\n",
    "    use_dedup = True\n",
    "    all_candidates = []\n",
    "    all_candidates_set = []\n",
    "    for i, row in enumerate(extracted_chatgpt_res):\n",
    "        ### flatten multiple results\n",
    "        row_all_synset_names = list(map(lambda x: x.split('.')[0], row.keys()))\n",
    "        row_candidates = {}\n",
    "        row_candidates_set = {}\n",
    "        for k, v in row.items():\n",
    "            candidates = list(reduce(lambda x, y: x+y, v))\n",
    "            candidates = [c for c in candidates if c not in row_all_synset_names] ### remove competing synset names\n",
    "            set_candidates = set(candidates)\n",
    "            k = k.split('.')[0] ### key synset name\n",
    "            row_candidates.setdefault(k, [])\n",
    "            row_candidates_set.setdefault(k, set([]))\n",
    "            row_candidates[k].extend(candidates)\n",
    "            row_candidates_set[k] |= set_candidates\n",
    "        ### collect duplicates\n",
    "        duplicates = set()\n",
    "        for k1, v1 in row.items():\n",
    "            k1 = k1.split('.')[0]\n",
    "            for k2, v2 in row.items():\n",
    "                k2 = k2.split('.')[0]\n",
    "                if k1!=k2:\n",
    "                    duplicates |= row_candidates_set[k1]&row_candidates_set[k2]\n",
    "        ### remove duplication with synset-names (keys)\n",
    "        row_candidates_update = {}\n",
    "        row_candidates_set_update = {}\n",
    "        for k1, v1 in row.items():\n",
    "            k1 = k1.split('.')[0]\n",
    "            for k2, v2 in row.items():\n",
    "                k2 = k2.split('.')[0]\n",
    "            row_candidates_set_update[k1] = row_candidates_set[k1] - duplicates if use_dedup else row_candidates_set[k1]\n",
    "            row_candidates_update[k1] = [item for item in row_candidates[k1] if item not in duplicates ] if row_candidates_set[k1] else row_candidates[k1]\n",
    "\n",
    "        all_candidates.append(row_candidates_update)\n",
    "        all_candidates_set.append(row_candidates_set_update)\n",
    "\n",
    "\n",
    "    ### check non-empty\n",
    "    empty_list = []\n",
    "    for i, line in enumerate(all_candidates_set):\n",
    "        for k, v in line.items():\n",
    "            if len(v)==0:\n",
    "                for j in range(n_repeat):\n",
    "                    empty_list.append(j)\n",
    "                    print(f'repair {i} {j}')\n",
    "                    while 1:\n",
    "                        try:\n",
    "                            res = openai_chatgpt_post(concept_templates[i])\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    all_chatgpt_res[j][i] = res\n",
    "\n",
    "    if len(empty_list)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3695fa28-601f-465e-8862-e405d825c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "{\n",
    "    'all_candidates': all_candidates,\n",
    "    'all_candidates_set': all_candidates_set,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b053c196-dfb3-4491-9ad6-e6d400f4bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" counter sorting \"\"\"\n",
    "all_candidates = data['all_candidates']\n",
    "all_counter_candidates = []\n",
    "all_number_candidates = []\n",
    "for row in all_candidates:\n",
    "    row_counter = {}\n",
    "    total_num = 0\n",
    "    for k, v in row.items():\n",
    "        ct = Counter(v)\n",
    "        row_counter[k] = OrderedDict(sorted(ct.items())) ### order key\n",
    "        total_num += sum(ct.values())\n",
    "    all_counter_candidates.append(OrderedDict(sorted(row_counter.items()))) ### order key\n",
    "    all_number_candidates.append(total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcc250a1-2d48-41ef-a166-649d1662be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### flatten\n",
    "all_row_mapping_idx_synset_name = []\n",
    "all_row_chatgpt_names = []\n",
    "all_row_i_syn = []\n",
    "all_row_weight = []\n",
    "all_row_key_name = []\n",
    "for i in range(len(all_counter_candidates)):\n",
    "    row_synset_names = all_counter_candidates[i].keys()\n",
    "    row_mapping_idx_synset_name = dict(zip(range(len(row_synset_names)), row_synset_names))\n",
    "    row_i_syn = []\n",
    "    row_chatgpt_names = []\n",
    "    row_weight = []\n",
    "    for i_syn, syn in enumerate(row_synset_names):\n",
    "        row_i_syn.extend([i_syn for _ in range(len(all_counter_candidates[i][syn]))])\n",
    "        row_chatgpt_names.extend(list(all_counter_candidates[i][syn]))\n",
    "        row_weight.extend(list(all_counter_candidates[i][syn].values()))\n",
    "    \n",
    "    all_row_mapping_idx_synset_name.append(row_mapping_idx_synset_name)\n",
    "    all_row_chatgpt_names.append(row_chatgpt_names)\n",
    "    all_row_i_syn.append(row_i_syn)\n",
    "    all_row_weight.append(row_weight)\n",
    "    all_row_key_name.append(list(map(lambda x: row_mapping_idx_synset_name[x], row_i_syn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7a821d8-1775-4645-a77a-0269bc30fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=None):\n",
    "    \"\"\" build classifier for chatgpt\n",
    "    Args:\n",
    "        all_row_chatgpt_names: [[names]]\n",
    "    \"\"\"\n",
    "    if all_row_key_name is None: ### single name\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 1\n",
    "            templates = json.load(f)['imagenet']\n",
    "    else:\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 2\n",
    "            templates = json.load(f)[f'{args.dataset}-parent-3']\n",
    "            \n",
    "    len_t = len(templates)\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_row_chatgpt_names)) as pbar:\n",
    "        for idx, row in enumerate(all_row_chatgpt_names):\n",
    "            len_row = len(row)\n",
    "            if all_row_key_name is None:\n",
    "                row_t = [ t.format(name) for name in row for t in templates ]\n",
    "            else:\n",
    "                row_t = [ t.format(pname, name) for pname, name in zip(all_row_key_name[idx], row) for t in templates ]\n",
    "            row_t = tokenize(row_t).to(args.device)\n",
    "            features = model.encode_text(row_t)\n",
    "            features = features.view(len_row, len_t, -1).float()\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            features = features.mean(dim=1)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1145633a-4d2e-4ab6-93dd-1d24d31ee023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [01:26<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "all_row_classifier = build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=all_row_key_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61b61a-000c-43ba-bfa2-d652de176881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/vfeatures-{args.dataset}.npy')\n",
    "# is_correct = []\n",
    "# k_2 = 1\n",
    "# enable_weight = True\n",
    "# instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "# for c in range(len(all_row_classifier)):\n",
    "#     select = (record_pred_kmeans_t==c)\n",
    "#     row_classifier = all_row_classifier[c]\n",
    "#     sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "#     sim_topk = sim.topk(k=k_2)\n",
    "#     # reliable_samples = sim_topk.values.flatten().topk(k=int(0.9*sim.size(0))).indices\n",
    "#     ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "#     count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "#     count_names[ind] = val ### count of each name\n",
    "#     count_smask = []\n",
    "#     smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "#     for s in np.unique(smask):\n",
    "#         if enable_weight:\n",
    "#             row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "#             # row_weight /= row_weight.sum()\n",
    "#             # row_weight = torch.ones(len(all_row_weight[c])).float()\n",
    "#             # row_weight[smask==s] = row_weight[smask==s] / row_weight[smask==s].sum()\n",
    "#             row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "#             row_weight /= row_weight.sum()\n",
    "#             count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "#         else:\n",
    "#             count_smask.append(count_names[smask==s].sum())\n",
    "#     name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "#     name_gt = all_gt_voc[select].mode().values\n",
    "#     name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "#     is_correct.append(name_pred==name_gt)\n",
    "#     instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73726305-326d-4047-b855-daed75f86105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vfeatures = np.load(f'./cache/features/vfeatures-{args.dataset}.npy')\n",
    "vfeatures = all_vfeatures\n",
    "all_clu_pred_chatgpt = torch.zeros_like(all_clu_pred)\n",
    "is_correct = []\n",
    "k_2 = 3\n",
    "enable_weight = True\n",
    "instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "for c in range(len(all_row_classifier)):\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    row_classifier = all_row_classifier[c]\n",
    "    sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "    sim_topk = sim.topk(k=k_2)\n",
    "    ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "    count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "    count_names[ind] = val ### count of each name\n",
    "    count_smask = []\n",
    "    smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "    for s in np.unique(smask):\n",
    "        if enable_weight:\n",
    "            row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "            row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "            row_weight /= row_weight.sum()\n",
    "            count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "        else:\n",
    "            count_smask.append(count_names[smask==s].sum())\n",
    "    name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "    name_gt = all_gt_voc[select].mode().values\n",
    "    name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "    is_correct.append(name_pred==name_gt)\n",
    "    instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]\n",
    "    \n",
    "    val_count = torch.tensor(count_smask)\n",
    "    ind_count = [ all_row_mapping_idx_synset_name[c][ii] for ii in range(k_1) ]\n",
    "    ind_count = torch.tensor([vocab.mapping_names_idx[xx] for xx in ind_count])\n",
    "    all_clu_pred_chatgpt[c, ind_count] = val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c058e853-7b70-4c67-a46d-d689cbc32012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name_acc=0.5119047619047619, instance_acc=0.43116295337677, missing=125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([250]), torch.Size([260]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_acc = np.array(is_correct).mean().item()\n",
    "instance_acc = (instance_pred_voc==all_gt_voc).float().mean().item()\n",
    "missing = all_gt_voc.unique().size(0) - all_gt_voc[(instance_pred_voc==all_gt_voc)].unique().size(0)\n",
    "\n",
    "print(f'name_acc={name_acc}, instance_acc={instance_acc}, missing={missing}')\n",
    "instance_pred_voc.unique().shape, all_gt_voc.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8db0d60-45de-46db-82b8-9cf10ee6c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_voc_clu = dict(zip(instance_pred_voc.unique().numpy().tolist(), range(len(instance_pred_voc))))\n",
    "r_pred_kmeans_t = np.array([mapping_voc_clu[item.item()] for item in instance_pred_voc])\n",
    "\n",
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt-uk{args.estimate_k}.pth', r_pred_kmeans_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfd56e80-c545-4fd7-aa76-033e3b51d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "\n",
    "# with open(f'./cache/openai/inov-cluster_visual_chatgpt-repeat={n_repeat}-k_1={k_1}-dataset={args.dataset}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_chatgpt_res, f)\n",
    "\n",
    "instance_acc = ((instance_pred_voc==all_gt_voc) | (cluster_ind_voc.cpu()==all_gt_voc)).float().mean().item()\n",
    "print(instance_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f541e79a-8854-4c18-8d3d-26ddba25bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_assign\n",
      "assignment shape=(73, 20071)\n",
      "instance label acc:: 0.41768473386764526\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/173 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "a, res_ass = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "r_pred_kmeans_t, r_cluster_ind_voc = reassign_by_pred_cluster(a, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fba0b7de-3708-4a81-ab75-8ef96501a1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 30\n",
      "iou voc:: 0.36893203883495146\n",
      "cluster acc 0.6815302475001132\n"
     ]
    }
   ],
   "source": [
    "set_pred = set(res_ass[1].tolist())\n",
    "set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "iou_voc = n_inter/n_union\n",
    "n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "print('missing label::', n_missing_label)\n",
    "print('iou voc::', iou_voc)\n",
    "print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=r_pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e95f8569-6111-46f5-aa44-73f94a022318",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt-uk206.pth', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ddece1d5-371e-4ea2-b057-45e6a1c52f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'/home/sheng/sssa/ipynb/cache/cluster/topk=1-cache-inov-{args.dataset}-clip-chatgpt.pth', r_pred_kmeans_t.cpu().numpy())\n",
    "# np.save(f'/home/sheng/sssa/ipynb/cache/cluster/cache-inov-{args.dataset}-clip-chatgpt-iter=1.pth', r_pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f089427d-e49d-42d3-b96b-8ec64be6b327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([167, 167, 167, ..., 232, 232, 232])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f'/home/sheng/sssa/ipynb/cache/cluster/cache-inov-{args.dataset}-clip-chatgpt.pth.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158387f-c808-4a68-bb83-a887f9cc62b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### vocab ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f40302f9-1061-44b9-8651-50ba1e4128a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = 3\n",
    "topk_all_clu_pred = (classifier@classifier.t()).topk(k=k_1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d905939-2990-4c9f-88c8-21c341c325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" gather concepts \"\"\"\n",
    "to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "cluster_row_synsets = []\n",
    "for row in topk_all_clu_pred:\n",
    "    row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "    cluster_row_synsets.append(row_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86ecafa0-5463-40b1-9e6a-d577a29377bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" generate concept requests \"\"\"\n",
    "concept_request = []\n",
    "for row in cluster_row_synsets:\n",
    "    ccpts = reduce(lambda x, y: x+y, row)\n",
    "    ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "    ccpts = ', '.join(ccpts)\n",
    "    concept_request.append(ccpts)\n",
    "    \n",
    "\"\"\" generate concept templates \"\"\"\n",
    "template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "with open('/home/sheng/sssa/templates_chatgpt.json', 'r') as f:\n",
    "    template_chatgpt = json.load(f)\n",
    "template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "# template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "    \n",
    "template_in_use = template_9\n",
    "concept_templates = []\n",
    "for row in concept_request:\n",
    "    concept_templates.append(template_in_use(row))\n",
    "    \n",
    "n_repeat = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0d2d4-4343-40bc-be8c-f944383e7988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60/20071 [06:52<38:59:28,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8c6d3b0b746aa518c8842e4e54019d28 in your message.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 126/20071 [15:12<47:16:45,  8.53s/it]"
     ]
    }
   ],
   "source": [
    "\"\"\" collect chatgpt res \"\"\"\n",
    "all_chatgpt_res = [[] for _ in range(n_repeat)]\n",
    "with tqdm(total=len(concept_templates)*n_repeat) as pbar:\n",
    "    for i in range(n_repeat):\n",
    "        for row in concept_templates:\n",
    "            while 1:\n",
    "                try:\n",
    "                    all_chatgpt_res[i].append(openai_chatgpt_post(row))\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b6404-bfff-49b5-a283-313d7efbd14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
