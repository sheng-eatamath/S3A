{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0705be-f973-4ee8-921c-305648207653",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "from my_util_package.evaluate import measure_similarity_bert\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:2'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'imagenet'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 256\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_entity13/sssa/checkpoint-current.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word_L.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "def most_frequent(strings):\n",
    "    counts = Counter(strings)\n",
    "    return max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n",
    "\n",
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483d62cf-3b3b-421b-80c7-2bf564d60cf1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "pet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3680/3680 [01:31<00:00, 40.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score={'nli-bert-large': 0.6053804553842739, 'all-mpnet-base-v2': 0.5405309574212879}\n",
      "({'nli-bert-large': 0.6078760531811934, 'all-mpnet-base-v2': 0.5511760576526917},)\n",
      "{'nli-bert-large': 0.6078760531811934, 'all-mpnet-base-v2': 0.5511760576526917}\n",
      "==============================\n",
      "cifar100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [15:23<00:00, 54.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score={'nli-bert-large': 0.696760649568513, 'all-mpnet-base-v2': 0.5755577055854351}\n",
      "({'nli-bert-large': 0.6936536730685458, 'all-mpnet-base-v2': 0.5930957601378113},)\n",
      "{'nli-bert-large': 0.6930872553317249, 'all-mpnet-base-v2': 0.5799500711106509}\n",
      "==============================\n",
      "sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108754/108754 [1:40:53<00:00, 17.97it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score={'nli-bert-large': 0.7011621361831473, 'all-mpnet-base-v2': 0.5365280876201292}\n",
      "({'nli-bert-large': 0.7067870392192824, 'all-mpnet-base-v2': 0.554087328776058},)\n",
      "{'nli-bert-large': 0.6980152250392371, 'all-mpnet-base-v2': 0.5287831031942599}\n",
      "==============================\n",
      "caltech101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6859/6859 [02:21<00:00, 48.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score={'nli-bert-large': 0.7415422698006823, 'all-mpnet-base-v2': 0.6009537648153793}\n",
      "({'nli-bert-large': 0.7340521192712293, 'all-mpnet-base-v2': 0.6025970067319986},)\n",
      "{'nli-bert-large': 0.7324085820518963, 'all-mpnet-base-v2': 0.5946288760162196}\n"
     ]
    }
   ],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "from sklearn import metrics\n",
    "def measure_similarity_bert(vocab, all_pred_voc, all_gt_voc, base=['nli-bert-large', 'all-mpnet-base-v2'], device='cuda:0'):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    all_scores = {}\n",
    "    for b in base:\n",
    "        model = SentenceTransformer(b, device=device)\n",
    "\n",
    "        set_all_gt_voc = list(set(all_gt_voc))\n",
    "        embeddings_gt = torch.tensor(model.encode(set_all_gt_voc))\n",
    "        mapping_set_all_gt_voc = {}\n",
    "        for i, w in enumerate(set_all_gt_voc):\n",
    "            mapping_set_all_gt_voc[w] = embeddings_gt[i]\n",
    "\n",
    "        if isinstance(all_pred_voc[0], str):\n",
    "            all_pred_voc_names = all_pred_voc\n",
    "        else:\n",
    "            all_pred_voc_names = [vocab.mapping_idx_names[x.item()] for x in all_pred_voc]\n",
    "        set_all_pred_voc = list(set(all_pred_voc_names))\n",
    "        embeddings_pred = torch.tensor(model.encode(set_all_pred_voc))\n",
    "        mapping_set_all_pred_voc = {}\n",
    "        for i, w in enumerate(set_all_pred_voc):\n",
    "            mapping_set_all_pred_voc[w] = embeddings_pred[i]\n",
    "\n",
    "        scores = []\n",
    "        for pred, gt in zip(all_pred_voc_names, all_gt_voc):\n",
    "            sim = util.cos_sim(mapping_set_all_pred_voc[pred], mapping_set_all_gt_voc[gt])\n",
    "            scores.append(sim.item())\n",
    "            \n",
    "        all_scores[b] = np.mean(scores)\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=args.device)\n",
    "for dataset_name in [\n",
    "    'pet', 'cifar100', 'sun', 'caltech101']:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    \n",
    "    dataset_raw = get_datasets_oszsl(args, vocab, is_train=True, transform=None, seed=0)\n",
    "    loader_r = torch.utils.data.DataLoader(dataset_raw, num_workers=4, batch_size=20, shuffle=False)\n",
    "\n",
    "    with open(f'./cache/vlm-{args.dataset}-scd.pkl', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    topk_all_clu_pred = res['topk_all_clu_pred']\n",
    "    pred_kmeans_t = res['pred_kmeans_t']\n",
    "    all_clu_pred = res['all_clu_pred']\n",
    "\n",
    "    to_name = lambda x: x.name().split('.')[0]\n",
    "    cluster_row_synsets = []\n",
    "    for row in topk_all_clu_pred:\n",
    "        row_synsets = [vocab.mapping_idx_names[voc_idx.item()] for voc_idx in row]\n",
    "        cluster_row_synsets.append(row_synsets)\n",
    "\n",
    "    # try:\n",
    "    #     with open(f'./cache/temp-vqa-{args.dataset}.pkl', 'rb') as f:\n",
    "    #         rec = pickle.load(f)\n",
    "    #     flag = min([len(item) for item in rec['all_ans']])\n",
    "    # except Exception as e:\n",
    "    #     flag = 0\n",
    "    \n",
    "    \n",
    "    bsize = 256\n",
    "    q = lambda c: f\"what is the category name of the object in the photo? {c[0]}, {c[1]}, or {c[2]}?\"\n",
    "    # q = lambda c: f\"what is category name of the dog in the photo?  {c[0]}, {c[1]}, or {c[2]}?\"\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(dataset_raw)) as pbar:\n",
    "            batch_label_voc, batch_label_clu = [], []\n",
    "            batch_img, batch_q = [], []\n",
    "            all_ans = []\n",
    "            all_voc, all_clu = [], []\n",
    "            for idx, item in enumerate(dataset_raw):\n",
    "                # if idx<flag:\n",
    "                #     pbar.update(1)\n",
    "                #     continue\n",
    "                image, label_voc, label_clu, idx_img = item[:4]\n",
    "                image = vis_processors[\"eval\"](image)\n",
    "                question = txt_processors[\"eval\"](q(cluster_row_synsets[pred_kmeans_t[idx_img]]))\n",
    "                batch_label_voc.append(label_voc)\n",
    "                batch_label_clu.append(label_clu)\n",
    "                batch_img.append(image)\n",
    "                batch_q.append(question)\n",
    "\n",
    "                if (idx%bsize==(bsize-1)) or (idx==(len(dataset_raw)-1)):\n",
    "                    ans = model.predict_answers(samples={\"image\": torch.stack(batch_img, dim=0).to(args.device), \n",
    "                                                         \"text_input\": batch_q}, inference_method=\"generate\")\n",
    "                    all_ans.append(ans)\n",
    "                    all_voc.extend(batch_label_voc)\n",
    "                    all_clu.extend(batch_label_clu)\n",
    "\n",
    "                    batch_label_voc, batch_label_clu = [], []\n",
    "                    batch_img, batch_q = [], []\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "                # if idx%10000==1:\n",
    "                #     with open(f'./cache/temp-vqa-{args.dataset}.pkl', 'wb') as f:\n",
    "                #         pickle.dump({'all_ans': all_ans, 'all_voc': all_voc, 'all_clu': all_clu}, f)\n",
    "    try:\n",
    "        c_gt = np.array([vocab.mapping_idx_names[x] for x in all_voc])\n",
    "    except Exception as e:\n",
    "        c_gt = np.array(all_voc)\n",
    "    c_pred = np.array(reduce(lambda x, y: x+y, all_ans))\n",
    "    score = measure_similarity_bert(vocab, np.array(all_voc), c_pred, device=args.device)\n",
    "    print(f'score={score}')\n",
    "    res = {\n",
    "        'all_voc': all_voc,\n",
    "        'c_gt': c_gt,\n",
    "        'c_pred': c_pred,\n",
    "    }\n",
    "\n",
    "    c_gt = np.array([' '.join(x.split('_')) for x in c_gt])\n",
    "    for c in pred_kmeans_t.unique():\n",
    "        select = (pred_kmeans_t==c).numpy()\n",
    "        item = most_frequent(c_pred[select])\n",
    "        c_pred[select] = item\n",
    "\n",
    "    score1 = measure_similarity_bert(vocab, c_pred, c_gt, device=args.device),\n",
    "    score2 = measure_similarity_bert(vocab, res['c_pred'], res['c_gt'], device=args.device)\n",
    "    print(score1)\n",
    "    print(score2)\n",
    "\n",
    "    with open(f'./cache/vqav2-{args.dataset}-train.pkl', 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "\n",
    "    with open(f'./cache/vqav2-{args.dataset}-train-score.pkl', 'wb') as f:\n",
    "        pickle.dump([score1, score2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153a1f9-a8cb-412b-bb84-6ca16ef73b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score1)\n",
    "print(score2)\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
