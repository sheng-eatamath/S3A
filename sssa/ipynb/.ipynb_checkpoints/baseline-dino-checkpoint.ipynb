{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f07a74-29eb-49f4-8573-5a5a086d429d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:3'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'make_nonliving26'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    seed = 0\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d05dc9a-2956-45aa-8519-d8167d50baa4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "def read_lvis_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/lvis_imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "        # data = list(map(lambda x: x.split('.')[0], data))\n",
    "    return data\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch, device=args.device)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "if args.dataset == 'lvis':\n",
    "    classes = read_lvis_imagenet21k_classes()\n",
    "    classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(args.input_size, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=torch.tensor(mean),\n",
    "        std=torch.tensor(std))\n",
    "])\n",
    "\n",
    "\n",
    "from my_util_package.dino import vision_transformer as vits\n",
    "from config import dino_pretrain_path\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "\n",
    "dino = vits.vit_base()\n",
    "dino.load_state_dict(torch.load(dino_pretrain_path, map_location='cpu'))\n",
    "dino = dino.to(args.device)\n",
    "\n",
    "for dataset_name in ['make_living17', 'make_nonliving26', 'make_entity13', 'make_entity30']:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "    loader_val = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=256, shuffle=False)\n",
    "    print('dataset size', len(dataset))\n",
    "\n",
    "    all_vfeatures = []\n",
    "    all_clu_label = []\n",
    "    with tqdm(total=len(loader_val)) as pbar:\n",
    "        dino.eval()\n",
    "        for idx_batch, batch in enumerate(loader_val):\n",
    "            images, label_voc, label_clu, idx_img = batch\n",
    "            images = images.to(args.device)\n",
    "            with torch.no_grad():\n",
    "                features = dino(images)\n",
    "                features = F.normalize(features, dim=-1)\n",
    "                all_vfeatures.append(deepcopy(features.cpu().numpy()))\n",
    "                all_clu_label.append(deepcopy(label_clu.numpy()))\n",
    "            pbar.update(1)\n",
    "\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    all_clu_label = np.concatenate(all_clu_label)\n",
    "    \n",
    "    estimate_k = {'make_entity13': 252, 'make_living17': 73, 'make_nonliving26': 101, 'make_entity30': 206}\n",
    "    kmeans = KMeans(n_clusters=estimate_k[args.dataset], random_state=0, n_init=10, max_iter=1000, verbose=1).fit(all_vfeatures)\n",
    "    preds = kmeans.labels_\n",
    "    acc = cluster_acc(all_clu_label, preds)\n",
    "\n",
    "    with open(f'./cache/dino/dino-{args.dataset}-uk.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'all_vfeatures': all_vfeatures,\n",
    "            'all_clu_label': all_clu_label,\n",
    "            'acc': acc,\n",
    "        }, f)\n",
    "    print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d0629-7891-435c-ba37-725191086997",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in ['make_living17', 'make_nonliving26', 'make_entity13', 'make_entity30', 'imagenet1k', 'sdogs']:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    with open(f'./cache/dino/dino-{args.dataset}-uk.pkl', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    print(res['acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
