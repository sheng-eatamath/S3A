{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491112e-bd1d-4f10-afa1-114f75376ac1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/MUST')\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:3'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'imagenet'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 100\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_entity13/sssa/checkpoint-current.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word_L.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "\n",
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n",
    "\n",
    "\n",
    "def load_clip(args):\n",
    "    model, preprocess = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model, preprocess\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch, device=args.device)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "def load_mixture_clip(args, decay=1.0):\n",
    "    model1 = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model1.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model1.to(args.device).eval()\n",
    "    model2 = clip.load(args.arch)\n",
    "    model2.to(args.device).eval()\n",
    "    with torch.no_grad():\n",
    "        msd = model1.state_dict()\n",
    "        for k, ema_v in model2.state_dict().items():\n",
    "            # if needs_module:\n",
    "            #     k = 'module.' + k\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * decay + (1. - decay) * model_v)\n",
    "    return model2\n",
    "\n",
    "def topk_acc(all_pred_voc_topk, all_gt_voc):\n",
    "    acc = []\n",
    "    ### topK accuracy\n",
    "    for i in range(all_pred_voc_topk.size(1)):\n",
    "        vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "        for j in range(i+1):\n",
    "            vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "        print(f'k={i} acc={vec.float().mean()}')\n",
    "        acc.append(vec.float().mean().item())\n",
    "    return acc\n",
    "\n",
    "def semantic_acc(y_pred, y_true, metrics={}):\n",
    "    \"\"\" compute soft semantic acc for @y_pred and @y_true \"\"\"\n",
    "    assert len(metrics)>0\n",
    "    assert y_pred.size(0)==y_true.size(0)\n",
    "    scores = {m:[] for m in metrics.keys()}\n",
    "    with tqdm(total=y_pred.size(0)) as pbar:\n",
    "        for i in range(y_pred.size(0)):\n",
    "            syn_pred = mapping_vocidx_to_synsets(y_pred[i].item(), vocab)\n",
    "            syn_true = mapping_vocidx_to_synsets(y_true[i].item(), vocab)\n",
    "            pairs = list(itertools.product(range(len(syn_pred)), range(len(syn_true))))\n",
    "            for m_name, m in metrics.items():\n",
    "                scores[m_name].append( max([ m(syn_pred[p[0]], syn_true[p[1]]) for p in pairs ]) )\n",
    "            pbar.update(1)\n",
    "    for m_name in metrics.keys():\n",
    "        scores[m_name] = np.array(scores[m_name]).mean()\n",
    "    return scores\n",
    "    \n",
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text)[:75] + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    n_count = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count.append( selected.sum().item() )\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected, :]).ravel(), return_counts=True)\n",
    "        # counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    n_count = torch.tensor(n_count).cpu()\n",
    "    \n",
    "    # all_clu_pred = setdiff_assignment(all_clu_pred)\n",
    "    \n",
    "    all_clu_pred = all_clu_pred/(n_count.view(-1, 1) + 1e-20)\n",
    "    \n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc, return_results=False):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    inst_acc = (label_voc_kmeans==all_gt_voc).float().mean().item()\n",
    "    print('instance label acc::', inst_acc)\n",
    "    if return_results:\n",
    "        return label_voc_kmeans, res_ass, inst_acc\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, model, classifier, device, \n",
    "                             all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                            prob = classifier_selected[similarity.softmax(-1)]\n",
    "                            cluster_ind.append(deepcopy(prob.cpu().argmax(dim=-1)))\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(deepcopy(prob[:, label_voc_kmeans].cpu().argmax(dim=-1)))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    if classifier_selected is not None:\n",
    "        cluster_ind_voc = classifier_selected[cluster_ind]\n",
    "    else:\n",
    "        cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, \n",
    "                             preextracted_vfeatures=None):\n",
    "    \"\"\" given vocab label set @label_voc_kmeans, \n",
    "    Args:\n",
    "        label_voc_kmeans: cluster-assigned label on vocab\n",
    "        ...\n",
    "        preextracted_vfeatures: np.array([N x D])\n",
    "    Returns:\n",
    "        cluster_ind: tensor([N]): re-ordered cluster assignment\n",
    "        cluster_ind_voc: tensor([N]): cluster assignment indiced by vocab\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device).unique()\n",
    "    cluster_ind = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        if preextracted_vfeatures is not None:\n",
    "            N = len(loader_f.dataset)\n",
    "            batch_size = 10000\n",
    "            indices = np.array_split(np.arange(N), N//batch_size)\n",
    "            with torch.no_grad():\n",
    "                for group in indices:\n",
    "                    logits = torch.from_numpy(preextracted_vfeatures[group]).float()\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits@classifier.t().cpu()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    cluster_ind.append(deepcopy(prob[:, label_voc_kmeans.cpu()].argmax(dim=-1)))\n",
    "        else:\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if preextracted_vfeatures is not None:\n",
    "                            logits = torch.from_numpy(preextracted_vfeatures[idx_img.cpu().numpy()]).float().to(device)\n",
    "                        else:\n",
    "                            logits = model.ema.extract_vfeatures(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        similarity = 100 * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(deepcopy(prob[:, label_voc_kmeans].cpu().argmax(dim=-1)))\n",
    "                pbar.update(1)\n",
    "    cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def computation_reassign_by_pred_cluster(row, idx, args, model, classifier, candidate_classifier_ind):\n",
    "    \"\"\"\n",
    "    candidate_classifier_ind = label_voc_kmeans.unique().to(args.device)\n",
    "    \"\"\"\n",
    "    images, label_voc, label_clu, idx_img = row[:4]\n",
    "    images = images.to(args.device)\n",
    "    with amp_autocast():\n",
    "        vfeatures = model.visual(images).float()\n",
    "        # vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "    vfeatures = F.normalize(vfeatures, dim=-1)\n",
    "    batch_sim = 100*vfeatures@classifier[candidate_classifier_ind].t()\n",
    "    cluster_ind = batch_sim.argmax(dim=-1)\n",
    "    cluster_ind_voc = candidate_classifier_ind[cluster_ind].cpu()\n",
    "    return cluster_ind_voc\n",
    "\n",
    "def aggregation_reassign_by_pred_cluster(r, candidate_classifier_ind):\n",
    "    cluster_ind_voc = torch.cat(r, dim=0)\n",
    "    mapping_ind = dict(zip(cluster_ind_voc.unique().numpy(), torch.arange(cluster_ind_voc.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind_voc])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_vfeatures(model, data_loader, device):\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(data_loader)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(data_loader):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(device)\n",
    "            with amp_autocast():\n",
    "                vfeatures = model.visual(images).float()\n",
    "            vfeatures = vfeatures/vfeatures.norm(dim=-1, keepdim=True)\n",
    "            all_vfeatures.append(vfeatures.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    return all_vfeatures\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def loop_row_collect_results_nograd(obj_iter, computations={}, aggregations={}):\n",
    "    \"\"\" compute and aggregate results, looping over @obj_iter \n",
    "    func_computation(@row, @index_row)\n",
    "    aggregations(list(@results_computation))\n",
    "    \"\"\"\n",
    "    assert set(list(computations.keys())) == set(list(aggregations.keys()))\n",
    "    collector = { k:[] for k in computations }\n",
    "    with tqdm(total=len(obj_iter)) as pbar:\n",
    "        for i, row in enumerate(obj_iter):\n",
    "            ### apply computations\n",
    "            for k, func in computations.items():\n",
    "                collector[k].append(func(row, i))\n",
    "            pbar.update(1)\n",
    "    ### aggregate results\n",
    "    results = {}\n",
    "    for k, func_agg in aggregations.items():\n",
    "        results[k] = func_agg(collector[k])\n",
    "    return results\n",
    "\n",
    "\n",
    "import openai\n",
    "def openai_chatgpt_post(content, parameters={'temperature': 0.7}):\n",
    "    openai.api_key = \"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\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo-0301\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "    **parameters,\n",
    "    )\n",
    "    result = completion['choices'][0]['message']['content']\n",
    "    return result\n",
    "\n",
    "\n",
    "import math\n",
    "def batchify(iterable, batch_size=10000):\n",
    "    N = len(iterable)\n",
    "    for i in range(math.ceil(N/batch_size)):\n",
    "        yield iterable[i*batch_size: min(N, (i+1)*batch_size)]\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=None):\n",
    "    \"\"\" build classifier for chatgpt\n",
    "    Args:\n",
    "        all_row_chatgpt_names: [[names]]\n",
    "    \"\"\"\n",
    "    if all_row_key_name is None: ### single name\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 1\n",
    "            templates = json.load(f)['imagenet']\n",
    "    else:\n",
    "        with open('../templates_small.json', 'rb') as f: ### template 2\n",
    "            templates = json.load(f)[f'{args.dataset}-parent-3']\n",
    "            \n",
    "    len_t = len(templates)\n",
    "    row_classifier = []\n",
    "    with tqdm(total=len(all_row_chatgpt_names)) as pbar:\n",
    "        for idx, row in enumerate(all_row_chatgpt_names):\n",
    "            len_row = len(row)\n",
    "            if all_row_key_name is None:\n",
    "                row_t = [ t.format(name) for name in row for t in templates ]\n",
    "            else:\n",
    "                row_t = [ t.format(pname, name) for pname, name in zip(all_row_key_name[idx], row) for t in templates ]\n",
    "            row_t = tokenize(row_t).to(args.device)\n",
    "            features = []\n",
    "            for brow in batchify(row_t, batch_size=101):\n",
    "                features.append(model.encode_text(brow).cpu())\n",
    "            features = torch.cat(features)\n",
    "            features = features.view(len_row, len_t, -1).float()\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            features = features.mean(dim=1)\n",
    "            features = features/features.norm(dim=-1, keepdim=True)\n",
    "            row_classifier.append(features.cpu())\n",
    "            \n",
    "            pbar.update(1)\n",
    "    return row_classifier\n",
    "    \n",
    "    \n",
    "def generate_prompt(category_name: str):\n",
    "    # you can replace the examples with whatever you want; these were random and worked, could be improved\n",
    "    return f\"\"\"Q: What are useful visual features for distinguishing a lemur in a photo?\n",
    "A: There are several useful visual features to tell there is a lemur in a photo:\n",
    "- four-limbed primate\n",
    "- black, grey, white, brown, or red-brown\n",
    "- wet and hairless nose with curved nostrils\n",
    "- long tail\n",
    "- large eyes\n",
    "- furry bodies\n",
    "- clawed hands and feet\n",
    "\n",
    "Q: What are useful visual features for distinguishing a television in a photo?\n",
    "A: There are several useful visual features to tell there is a television in a photo:\n",
    "- electronic device\n",
    "- black or grey\n",
    "- a large, rectangular screen\n",
    "- a stand or mount to support the screen\n",
    "- one or more speakers\n",
    "- a power cord\n",
    "- input ports for connecting to other devices\n",
    "- a remote control\n",
    "\n",
    "Q: What are useful features for distinguishing a {category_name} in a photo?\n",
    "A: There are several useful visual features to tell there is a {category_name} in a photo:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "mean = (0.48145466, 0.4578275, 0.40821073)\n",
    "std = (0.26862954, 0.26130258, 0.27577711)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(args.input_size, interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(args.input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=torch.tensor(mean),\n",
    "        std=torch.tensor(std))\n",
    "])\n",
    "\n",
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "model = load_clip2(args)\n",
    "\n",
    "\n",
    "for dataset_name in ['make_entity13']:\n",
    "    print('='*30)\n",
    "    print(f'dataset={dataset_name}')\n",
    "    args.dataset = dataset_name\n",
    "    # dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "    if subset == 'train':\n",
    "        dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "    elif subset == 'val':\n",
    "        dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=0)\n",
    "    args.nb_classes = dataset_f.num_classes\n",
    "    loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=4, batch_size=args.batch_size, shuffle=False)\n",
    "    classifier = get_classifier(args)\n",
    "    classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "    args.num_voc = classifier.size(0)\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    ### collect variables\n",
    "    prob_k = 1\n",
    "    all_topk_voc = []\n",
    "    all_gt_voc = []\n",
    "    all_label_clu = []\n",
    "    all_vfeatures = []\n",
    "    with tqdm(total=len(loader_f)) as pbar:\n",
    "        if hasattr(model, 'eval'):\n",
    "            model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_f):\n",
    "            images, label_voc, label_clu, idx_img = batch[:4]\n",
    "            images = images.to(args.device)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual.extract_features(images)\n",
    "                    # logits = model.extract_vfeatures(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    similarity = 100 * logits @ classifier.t()\n",
    "                    prob = similarity.softmax(-1)\n",
    "                    prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                    all_topk_voc.append(deepcopy(prob_topk_ind.cpu().numpy()))\n",
    "                    all_gt_voc.append(deepcopy(label_voc))\n",
    "                    all_label_clu.append(deepcopy(label_clu))\n",
    "                    all_vfeatures.append(deepcopy(logits.cpu().numpy()))\n",
    "            pbar.update(1)\n",
    "\n",
    "    all_topk_voc = np.concatenate(all_topk_voc)\n",
    "    all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "    all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "\n",
    "\n",
    "    # pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset}-train-clip.npy'))\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./cache/cluster/kmeans-{args.dataset}.npy'))\n",
    "    # pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/pred_kmeans_t.npy'))\n",
    "    # pred_kmeans = torch.from_numpy(np.load('/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/pred_kmeans_t.npy'))\n",
    "    pred_kmeans_t = pred_kmeans\n",
    "    history_set_pred = []\n",
    "    for t in range(3):\n",
    "        record_pred_kmeans_t = pred_kmeans_t\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "        history_set_pred.append(set_pred)\n",
    "\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "        n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "        iou_voc = n_inter/n_union\n",
    "        n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "        print('missing label::', n_missing_label)\n",
    "        print('iou voc::', iou_voc)\n",
    "\n",
    "\n",
    "    all_clu_gt_voc = []\n",
    "    for c in record_pred_kmeans_t.unique():\n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        all_clu_gt_voc.append(all_gt_voc[select].mode().values)\n",
    "    all_clu_gt_voc = torch.tensor(all_clu_gt_voc)\n",
    "    k_1 = 3\n",
    "    topk_all_clu_pred = all_clu_pred.topk(k=k_1).indices\n",
    "    cluster_is_correct = torch.zeros(topk_all_clu_pred.size(0)).bool()\n",
    "    for i in range(k_1):\n",
    "        cluster_is_correct |= (topk_all_clu_pred[:, i]==all_clu_gt_voc)\n",
    "    print(f'recall@{k_1} = {cluster_is_correct.float().mean()}')\n",
    "    \"\"\" gather concepts \"\"\"\n",
    "    to_name = lambda x: [ s.name() + ': ' + s.definition() for s in x ]\n",
    "    cluster_row_synsets = []\n",
    "    for row in topk_all_clu_pred:\n",
    "        row_synsets = [to_name(mapping_vocidx_to_synsets(voc_idx.item(), vocab)) for voc_idx in row]\n",
    "        cluster_row_synsets.append(row_synsets)\n",
    "\n",
    "    \"\"\" generate concept requests \"\"\"\n",
    "    concept_request = []\n",
    "    for row in cluster_row_synsets:\n",
    "        ccpts = reduce(lambda x, y: x+y, row)\n",
    "        ccpts = list(map(lambda x: \"'\"+x+\".'\", ccpts))\n",
    "        ccpts = ', '.join(ccpts)\n",
    "        concept_request.append(ccpts)\n",
    "\n",
    "    \"\"\" generate concept templates \"\"\"\n",
    "    template_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all alternative concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "    with open('/home/sheng/OSZSL/templates_chatgpt.json', 'r') as f:\n",
    "        template_chatgpt = json.load(f)\n",
    "    template_2 = lambda concept_list: template_chatgpt['pictionary-long'].format(concept_list)\n",
    "    template_3 = lambda concept_list: template_chatgpt['pictionary-short'].format(concept_list)\n",
    "    template_4 = lambda concept_list: template_chatgpt['direct'].format(concept_list)\n",
    "    template_5 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all synonym concept names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "    template_6 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "    template_7 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all parent-type category names for each visual concept. List in the format \\\"{concept name}: {list of names separated by ';'}.\\\"\"\n",
    "    template_8 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible descriptive phrases of image captions for each visual concept. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "    template_9 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "    # template_10 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"List all possible visiual descriptive phrases for each visual concept without duplication. List in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "    template_13 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible adjective phrases of visual descriptions for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "    template_9_1 = lambda concept_list: \"Given visual concepts: \"+ concept_list + \"Please list all possible visual descriptive phrases for each visual concept without duplication. Please list in the format \\\"{concept name}: {all phrases deliminated by semicolons}.\\\" for each concept. No duplication.\"\n",
    "\n",
    "\n",
    "    template_in_use = template_9_1\n",
    "    concept_templates = []\n",
    "    for row in concept_request:\n",
    "        concept_templates.append(template_in_use(row))\n",
    "\n",
    "    n_repeat = 3\n",
    "\n",
    "\n",
    "    request = lambda c: f\"what are useful visual features for distinguishing a {c} in a photo? please list descriptive phrases in bullet points.\"\n",
    "    request = lambda c: f'Please list all possible visual descriptive phrases for {c} in bullet points.'\n",
    "    request = lambda c: generate_prompt(c)\n",
    "\n",
    "    try:\n",
    "        with open('./cache/openai/vcd-total-2.pkl', 'rb') as f:\n",
    "            all_cate_descriptions = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        all_cate_descriptions = {}\n",
    "\n",
    "    with tqdm(total=len(cluster_row_synsets)*n_repeat) as pbar:\n",
    "        for row in cluster_row_synsets:\n",
    "            for cate in row:\n",
    "                cate_name = cate[0].split('.')[0]\n",
    "                if cate_name in all_cate_descriptions.keys():\n",
    "                    continue\n",
    "                else:\n",
    "                    all_cate_descriptions.setdefault(cate_name, [])\n",
    "                    for i in range(n_repeat):\n",
    "                        while 1:\n",
    "                            try:\n",
    "                                response = openai_chatgpt_post(request(cate_name))\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                            break\n",
    "                        all_cate_descriptions[cate_name].append(response)\n",
    "\n",
    "                        pbar.update(1)\n",
    "\n",
    "    all_cate_descriptions_new = {}\n",
    "    for k, v in all_cate_descriptions.items():\n",
    "        all_cate_descriptions_new[k] = \\\n",
    "            list(\n",
    "                map(lambda x: \n",
    "                    list(map(lambda y: y.strip(' ').strip('\\n'), x.split('- ')))[1:], \n",
    "                    v)\n",
    "            )\n",
    "\n",
    "    for k, v in all_cate_descriptions_new.items():\n",
    "        all_cate_descriptions_new[k] = reduce(lambda x, y: x+y, all_cate_descriptions_new[k])\n",
    "\n",
    "    with open(f'./cache/openai/vcd-total-2.pkl', 'wb') as f:\n",
    "        pickle.dump(all_cate_descriptions, f)\n",
    "\n",
    "    with open(f'./cache/openai/vcd-clean-total-2.pkl', 'wb') as f:\n",
    "        pickle.dump(all_cate_descriptions_new, f)\n",
    "\n",
    "\n",
    "    all_row_chatgpt_names = []\n",
    "    all_row_key_name = []\n",
    "    all_row_weight = []\n",
    "    all_row_i_syn = []\n",
    "    for row in cluster_row_synsets:\n",
    "        row_chatgpt_names = []\n",
    "        row_key_name = []\n",
    "        row_i_syn = []\n",
    "        for i, cate in enumerate(row):\n",
    "            cate_name = cate[0].split('.')[0]\n",
    "            desc = all_cate_descriptions_new[cate_name]\n",
    "            if len(desc)==0:\n",
    "                print(f'desc=0 for {cate_name}')\n",
    "                desc = [cate[0].split(':')[-1]]\n",
    "            row_key_name.extend([cate_name]*len(desc))\n",
    "            row_chatgpt_names.extend(desc)\n",
    "            row_i_syn.extend([i]*len(desc))\n",
    "        all_row_chatgpt_names.append(row_chatgpt_names)\n",
    "        all_row_key_name.append(row_key_name)\n",
    "        all_row_weight.append([1]*len(row_chatgpt_names))\n",
    "        all_row_i_syn.append(row_i_syn)\n",
    "\n",
    "    all_row_classifier = build_classifier_chatgpt(all_row_chatgpt_names, model, all_row_key_name=all_row_key_name)\n",
    "\n",
    "    # vfeatures = np.load(f'./cache/features/vfeatures-{args.dataset}.npy')\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        all_clu_pred: shape\n",
    "        all_row_classifier\n",
    "        record_pred_kmeans_t\n",
    "        all_row_weight\n",
    "        all_row_i_syn\n",
    "        vfeatures\n",
    "    \"\"\"\n",
    "    vfeatures = all_vfeatures\n",
    "    all_clu_pred_chatgpt = torch.zeros_like(all_clu_pred)\n",
    "    is_correct = []\n",
    "    all_name_gt = []\n",
    "    k_2 = 3\n",
    "    enable_weight = True\n",
    "    instance_pred_voc = torch.zeros_like(record_pred_kmeans_t)\n",
    "    for c in range(len(all_row_classifier)):\n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        row_classifier = all_row_classifier[c]\n",
    "        sim = torch.from_numpy(vfeatures[select, ...]).to(args.device)@row_classifier.to(args.device).t()\n",
    "        sim_topk = sim.topk(k=k_2)\n",
    "        ind, val = sim_topk.indices.flatten().cpu().unique(return_counts=True)\n",
    "        count_names = torch.zeros(row_classifier.size(0)).long()\n",
    "        count_names[ind] = val ### count of each name\n",
    "        count_smask = []\n",
    "        smask = np.array(all_row_i_syn[c]) ### partition mask\n",
    "        for s in np.unique(smask):\n",
    "            if enable_weight:\n",
    "                row_weight = torch.tensor(all_row_weight[c]).float()\n",
    "                row_weight[smask==s] = row_weight[smask==s] / row_weight[(smask==s)].sum()\n",
    "                row_weight /= row_weight.sum()\n",
    "                count_smask.append((row_weight[smask==s]*count_names[smask==s]).sum().item())\n",
    "            else:\n",
    "                count_smask.append(count_names[smask==s].sum())\n",
    "        # name_pred = all_row_mapping_idx_synset_name[c][np.argmax(count_smask)]\n",
    "        name_gt = all_gt_voc[select].mode().values\n",
    "        name_gt = vocab.mapping_idx_names[name_gt.item()]\n",
    "        all_name_gt.append(name_gt)\n",
    "        # is_correct.append(name_pred==name_gt)\n",
    "        # instance_pred_voc[select] = vocab.mapping_names_idx[name_pred]\n",
    "\n",
    "        val_count = torch.tensor(count_smask)\n",
    "        ind_count = topk_all_clu_pred[c]\n",
    "        # ind_count = torch.tensor([vocab.mapping_names_idx[xx] for xx in ind_count])\n",
    "        all_clu_pred_chatgpt[c, ind_count] = val_count\n",
    "\n",
    "    \n",
    "    classifier = get_classifier(args)\n",
    "    classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "    args.num_voc = classifier.size(0)\n",
    "    a, res_ass = linear_assign(all_clu_pred_chatgpt, record_pred_kmeans_t, all_gt_voc)\n",
    "    r_pred_kmeans_t, r_cluster_ind_voc = reassign_by_pred_cluster(a, loader_f, model, classifier, args.device, preextracted_vfeatures=all_vfeatures)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    n_inter = all_gt_voc[r_cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "    n_union = torch.cat([r_cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "    iou_voc = n_inter/n_union\n",
    "    n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "    print('missing label::', n_missing_label)\n",
    "    print('iou voc::', iou_voc)\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=r_pred_kmeans_t.numpy()))\n",
    "    n_inter = all_gt_voc[cluster_ind_voc.cpu()==all_gt_voc].unique().shape[0]\n",
    "    n_union = torch.cat([cluster_ind_voc.cpu(), all_gt_voc]).unique().shape[0]\n",
    "    iou_voc = n_inter/n_union\n",
    "    n_missing_label = all_gt_voc.unique().shape[0] - n_inter\n",
    "    print('missing label::', n_missing_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a36e7-322e-4947-8daa-ad4e56abade5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# dataset=sdogs\n",
    "# instance label acc:: 0.5203333497047424\n",
    "# missing label:: 25\n",
    "# iou voc:: 0.6551724137931034\n",
    "# cluster acc 0.5601666666666667\n",
    "# missing label:: 25\n",
    "# ==============================\n",
    "# dataset=imagenet\n",
    "# instance label acc:: 0.48275184631347656\n",
    "# missing label:: 46\n",
    "# iou voc:: 0.3698630136986301\n",
    "# cluster acc 0.7910789442630689\n",
    "# missing label:: 46\n",
    "# ==============================\n",
    "# dataset=make_nonliving26\n",
    "# instance label acc:: 0.5403526425361633\n",
    "# missing label:: 36\n",
    "# iou voc:: 0.4857142857142857\n",
    "# cluster acc 0.710093622662258\n",
    "# missing label:: 41\n",
    "# ==============================\n",
    "# dataset=make_living17\n",
    "# instance label acc:: 0.4589158892631531\n",
    "# missing label:: 28\n",
    "# iou voc:: 0.4166666666666667\n",
    "# cluster acc 0.682661418035383\n",
    "# missing label:: 31\n",
    "# ==============================\n",
    "# dataset=make_entity30\n",
    "# instance label acc:: 0.47094058990478516\n",
    "# missing label:: 101\n",
    "# iou voc:: 0.40762463343108507\n",
    "# cluster acc 0.7100492146766937\n",
    "# missing label:: 103\n",
    "# ==============================\n",
    "# dataset=imagenet1k\n",
    "# instance label acc:: 0.3739706873893738\n",
    "# missing label:: 472\n",
    "# iou voc:: 0.3573859768550034\n",
    "# cluster acc 0.5684373697495914\n",
    "# missing label:: 479\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
