{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c30921-4254-49f5-b122-ae92b0d06364",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets, Vocab\n",
    "from data.imagenet_datasets import get_datasets_oszsl\n",
    "from my_util_package.evaluate import measure_similarity_bert\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Config:\n",
    "    device = 'cuda:0'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset = 'imagenet'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = 252\n",
    "    \n",
    "    batch_size = 256\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/baseline-04_22_1/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/chatgpt_init-warmup=2/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_entity13/sssa/checkpoint-current.pth'\n",
    "    f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word_L.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()\n",
    "\n",
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns_with_synset_4.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def get_subsample_vocab(sample_synset_id: set):\n",
    "    vocab = get_vocab()\n",
    "    index = np.array([ i for i in range(len(vocab['synsets'])) if vocab['synsets'][i] in sample_synset_id ]).astype(np.int32)\n",
    "    for k in vocab.keys():\n",
    "        vocab[k] = np.array(vocab[k])[index].tolist()\n",
    "    return vocab\n",
    "\n",
    "def read_imagenet21k_classes():\n",
    "    with open('/home/sheng/dataset/imagenet21k/imagenet21k_wordnet_ids.txt', 'r') as f:\n",
    "        data = f.read()\n",
    "        data = list(filter(lambda x: len(x), data.split('\\n')))\n",
    "    return data\n",
    "\n",
    "def most_frequent(strings):\n",
    "    counts = Counter(strings)\n",
    "    return max(counts, key=counts.get)\n",
    "\n",
    "\n",
    "templates = load_templates(args)\n",
    "vocab = get_vocab()\n",
    "nouns = [ wn.synset(s) for s in vocab['synsets'] ]\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']\n",
    "\n",
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "classes = read_imagenet21k_classes() + os.listdir('/home/sheng/dataset/imagenet-img/')\n",
    "classes = [wn.synset_from_pos_and_offset('n', int(x[1:])).name() for x in classes]\n",
    "classes = set(classes)\n",
    "vocab = get_subsample_vocab(classes)\n",
    "vocab = Vocab(vocab=vocab)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f1e20-bc88-4ef3-a1b1-afa3be586761",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "make_nonliving26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132767/132767 [3:26:37<00:00, 10.71it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score={'nli-bert-large': 0.7419917309974211, 'all-mpnet-base-v2': 0.6072441308347992}\n",
      "({'nli-bert-large': 0.7431732688045526, 'all-mpnet-base-v2': 0.6485160827685106},)\n",
      "{'nli-bert-large': 0.7300223977810174, 'all-mpnet-base-v2': 0.5938833906641051}\n",
      "==============================\n",
      "make_living17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88404/88404 [2:10:34<00:00, 11.28it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score={'nli-bert-large': 0.6349181116374336, 'all-mpnet-base-v2': 0.45561848656285714}\n",
      "({'nli-bert-large': 0.6408435254294588, 'all-mpnet-base-v2': 0.5572600830460754},)\n",
      "{'nli-bert-large': 0.6330885625330619, 'all-mpnet-base-v2': 0.44701261120536484}\n",
      "==============================\n",
      "make_entity13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 108483/334718 [2:39:05<3:06:02, 20.27it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 42%|████▏     | 140673/334718 [3:20:12<1:42:39, 31.50it/s] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 82%|████████▏ | 1044355/1281186 [18:10:30<2:29:04, 26.48it/s] "
     ]
    }
   ],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "from sklearn import metrics\n",
    "def measure_similarity_bert(vocab, all_pred_voc, all_gt_voc, base=['nli-bert-large', 'all-mpnet-base-v2'], device='cuda:0'):\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    all_scores = {}\n",
    "    for b in base:\n",
    "        model = SentenceTransformer(b, device=device)\n",
    "\n",
    "        set_all_gt_voc = list(set(all_gt_voc))\n",
    "        embeddings_gt = torch.tensor(model.encode(set_all_gt_voc))\n",
    "        mapping_set_all_gt_voc = {}\n",
    "        for i, w in enumerate(set_all_gt_voc):\n",
    "            mapping_set_all_gt_voc[w] = embeddings_gt[i]\n",
    "\n",
    "        if isinstance(all_pred_voc[0], str):\n",
    "            all_pred_voc_names = all_pred_voc\n",
    "        else:\n",
    "            all_pred_voc_names = [vocab.mapping_idx_names[x.item()] for x in all_pred_voc]\n",
    "        set_all_pred_voc = list(set(all_pred_voc_names))\n",
    "        embeddings_pred = torch.tensor(model.encode(set_all_pred_voc))\n",
    "        mapping_set_all_pred_voc = {}\n",
    "        for i, w in enumerate(set_all_pred_voc):\n",
    "            mapping_set_all_pred_voc[w] = embeddings_pred[i]\n",
    "\n",
    "        scores = []\n",
    "        for pred, gt in zip(all_pred_voc_names, all_gt_voc):\n",
    "            sim = util.cos_sim(mapping_set_all_pred_voc[pred], mapping_set_all_gt_voc[gt])\n",
    "            scores.append(sim.item())\n",
    "            \n",
    "        all_scores[b] = np.mean(scores)\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=args.device)\n",
    "for dataset_name in [\n",
    "    'make_nonliving26', 'make_living17', 'make_entity13', 'make_entity30', \n",
    "    'imagenet1k']:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    \n",
    "    dataset_raw = get_datasets_oszsl(args, vocab, is_train=True, transform=None, seed=0)\n",
    "    loader_r = torch.utils.data.DataLoader(dataset_raw, num_workers=4, batch_size=20, shuffle=False)\n",
    "\n",
    "    with open(f'./cache/vlm-{args.dataset}-scd.pkl', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    topk_all_clu_pred = res['topk_all_clu_pred']\n",
    "    pred_kmeans_t = res['pred_kmeans_t']\n",
    "    all_clu_pred = res['all_clu_pred']\n",
    "\n",
    "    to_name = lambda x: x.name().split('.')[0]\n",
    "    cluster_row_synsets = []\n",
    "    for row in topk_all_clu_pred:\n",
    "        row_synsets = [vocab.mapping_idx_names[voc_idx.item()] for voc_idx in row]\n",
    "        cluster_row_synsets.append(row_synsets)\n",
    "\n",
    "    # try:\n",
    "    #     with open(f'./cache/temp-vqa-{args.dataset}.pkl', 'rb') as f:\n",
    "    #         rec = pickle.load(f)\n",
    "    #     flag = min([len(item) for item in rec['all_ans']])\n",
    "    # except Exception as e:\n",
    "    #     flag = 0\n",
    "    \n",
    "    \n",
    "    bsize = 256\n",
    "    q = lambda c: f\"what is the category name of the object in the photo? {c[0]}, {c[1]}, or {c[2]}?\"\n",
    "    # q = lambda c: f\"what is category name of the dog in the photo?  {c[0]}, {c[1]}, or {c[2]}?\"\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(dataset_raw)) as pbar:\n",
    "            batch_label_voc, batch_label_clu = [], []\n",
    "            batch_img, batch_q = [], []\n",
    "            all_ans = []\n",
    "            all_voc, all_clu = [], []\n",
    "            for idx, item in enumerate(dataset_raw):\n",
    "                # if idx<flag:\n",
    "                #     pbar.update(1)\n",
    "                #     continue\n",
    "                image, label_voc, label_clu, idx_img = item[:4]\n",
    "                image = vis_processors[\"eval\"](image)\n",
    "                question = txt_processors[\"eval\"](q(cluster_row_synsets[pred_kmeans_t[idx_img]]))\n",
    "                batch_label_voc.append(label_voc)\n",
    "                batch_label_clu.append(label_clu)\n",
    "                batch_img.append(image)\n",
    "                batch_q.append(question)\n",
    "\n",
    "                if (idx%bsize==(bsize-1)) or (idx==(len(dataset_raw)-1)):\n",
    "                    ans = model.predict_answers(samples={\"image\": torch.stack(batch_img, dim=0).to(args.device), \n",
    "                                                         \"text_input\": batch_q}, inference_method=\"generate\")\n",
    "                    all_ans.append(ans)\n",
    "                    all_voc.extend(batch_label_voc)\n",
    "                    all_clu.extend(batch_label_clu)\n",
    "\n",
    "                    batch_label_voc, batch_label_clu = [], []\n",
    "                    batch_img, batch_q = [], []\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # if idx%10000==1:\n",
    "                #     with open(f'./cache/temp-vqa-{args.dataset}.pkl', 'wb') as f:\n",
    "                #         pickle.dump({'all_ans': all_ans, 'all_voc': all_voc, 'all_clu': all_clu}, f)\n",
    "\n",
    "    c_gt = np.array([vocab.mapping_idx_names[x] for x in all_voc])\n",
    "    c_pred = np.array(reduce(lambda x, y: x+y, all_ans))\n",
    "    score = measure_similarity_bert(vocab, torch.tensor(all_voc), c_pred, device=args.device)\n",
    "    print(f'score={score}')\n",
    "    res = {\n",
    "        'all_voc': all_voc,\n",
    "        'c_gt': c_gt,\n",
    "        'c_pred': c_pred,\n",
    "    }\n",
    "    \n",
    "    c_gt = np.array([' '.join(x.split('_')) for x in c_gt])\n",
    "    for c in pred_kmeans_t.unique():\n",
    "        select = (pred_kmeans_t==c).numpy()\n",
    "        item = most_frequent(c_pred[select])\n",
    "        c_pred[select] = item\n",
    "\n",
    "    score1 = measure_similarity_bert(vocab, c_pred, c_gt, device=args.device),\n",
    "    score2 = measure_similarity_bert(vocab, res['c_pred'], res['c_gt'], device=args.device)\n",
    "    print(score1)\n",
    "    print(score2)\n",
    "\n",
    "    with open(f'./cache/vqav2-{args.dataset}-train.pkl', 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "        \n",
    "    with open(f'./cache/vqav2-{args.dataset}-train-score.pkl', 'wb') as f:\n",
    "        pickle.dump([score1, score2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036f09d1-1b99-4c0b-b6c4-525377439e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./cache/vqav2-imagenet-train.pkl', 'rb') as f:\n",
    "    res_imagenet = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac36088f-7595-4833-a284-9ec4718e516c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'nli-bert-large': 0.7431732688045526, 'all-mpnet-base-v2': 0.6485160827685106},), {'nli-bert-large': 0.7300223977810174, 'all-mpnet-base-v2': 0.5938833906641051}]\n",
      "[({'nli-bert-large': 0.6408435254294588, 'all-mpnet-base-v2': 0.5572600830460754},), {'nli-bert-large': 0.6330885625330619, 'all-mpnet-base-v2': 0.44701261120536484}]\n",
      "[({'nli-bert-large': 0.6915193647977609, 'all-mpnet-base-v2': 0.5960451863561586},), {'nli-bert-large': 0.6844698945722419, 'all-mpnet-base-v2': 0.5459808432889766}]\n",
      "[({'nli-bert-large': 0.6964566178256694, 'all-mpnet-base-v2': 0.5817065460034704},), {'nli-bert-large': 0.6887262798780419, 'all-mpnet-base-v2': 0.5295927274779838}]\n",
      "[({'nli-bert-large': 0.6778714557998187, 'all-mpnet-base-v2': 0.558138123263251},), {'nli-bert-large': 0.6711001074876364, 'all-mpnet-base-v2': 0.5111288806844274}]\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in [\n",
    "    'make_nonliving26', 'make_living17', 'make_entity13', 'make_entity30',\n",
    "    'imagenet1k']:\n",
    "    with open(f'./cache/vqav2-{dataset_name}-train-score.pkl', 'rb') as f:\n",
    "        scores = pickle.load(f)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b441fcf-110e-485e-a9d6-ceb4a0872164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_nonliving26\n",
    "# 100%|██████████| 132767/132767 [3:26:37<00:00, 10.71it/s]   \n",
    "# score={'nli-bert-large': 0.7419917309974211, 'all-mpnet-base-v2': 0.6072441308347992}\n",
    "# ({'nli-bert-large': 0.7431732688045526, 'all-mpnet-base-v2': 0.6485160827685106},)\n",
    "# {'nli-bert-large': 0.7300223977810174, 'all-mpnet-base-v2': 0.5938833906641051}\n",
    "# ==============================\n",
    "# make_living17\n",
    "# 100%|██████████| 88404/88404 [2:10:34<00:00, 11.28it/s]   \n",
    "# score={'nli-bert-large': 0.6349181116374336, 'all-mpnet-base-v2': 0.45561848656285714}\n",
    "# ({'nli-bert-large': 0.6408435254294588, 'all-mpnet-base-v2': 0.5572600830460754},)\n",
    "# {'nli-bert-large': 0.6330885625330619, 'all-mpnet-base-v2': 0.44701261120536484}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80794e43-4a23-4e15-a303-d8a8a024c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.models import load_model_and_preprocess\n",
    "from my_util_package.evaluate import measure_similarity_bert\n",
    "\n",
    "model, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_vqa\", model_type=\"vqav2\", is_eval=True, device=args.device)\n",
    "for dataset_name in [\n",
    "    'make_nonliving26', 'make_living17', 'make_entity13', 'make_entity30', \n",
    "    'imagenet1k']:\n",
    "    print('='*30)\n",
    "    print(dataset_name)\n",
    "    args.dataset = dataset_name\n",
    "    \n",
    "    dataset_raw = get_datasets_oszsl(args, vocab, is_train=True, transform=None, seed=0)\n",
    "    loader_r = torch.utils.data.DataLoader(dataset_raw, num_workers=4, batch_size=20, shuffle=False)\n",
    "\n",
    "    with open(f'./cache/vlm-{args.dataset}-scd.pkl', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    topk_all_clu_pred = res['topk_all_clu_pred']\n",
    "    pred_kmeans_t = res['pred_kmeans_t']\n",
    "    all_clu_pred = res['all_clu_pred']\n",
    "\n",
    "    to_name = lambda x: x.name().split('.')[0]\n",
    "    cluster_row_synsets = []\n",
    "    for row in topk_all_clu_pred:\n",
    "        row_synsets = [vocab.mapping_idx_names[voc_idx.item()] for voc_idx in row]\n",
    "        cluster_row_synsets.append(row_synsets)\n",
    "\n",
    "    try:\n",
    "        with open(f'./cache/temp-vqa-{args.dataset}.pkl', 'rb') as f:\n",
    "            rec = pickle.load(f)\n",
    "        flag = min([len(item) for item in rec['all_ans']])\n",
    "    except Exception as e:\n",
    "        flag = 0\n",
    "    \n",
    "    \n",
    "    bsize = 256\n",
    "    q = lambda c: f\"what is the category name of the object in the photo? {c[0]}, {c[1]}, or {c[2]}?\"\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(dataset_raw)) as pbar:\n",
    "            batch_label_voc, batch_label_clu = [], []\n",
    "            batch_img, batch_q = [], []\n",
    "            all_ans = []\n",
    "            all_voc, all_clu = [], []\n",
    "            for idx, item in enumerate(dataset_raw):\n",
    "                image, label_voc, label_clu, idx_img = item[:4]\n",
    "                image = vis_processors[\"eval\"](image)\n",
    "                question = txt_processors[\"eval\"](q(cluster_row_synsets[pred_kmeans_t[idx_img]]))\n",
    "                batch_label_voc.append(label_voc)\n",
    "                batch_label_clu.append(label_clu)\n",
    "                batch_img.append(image)\n",
    "                batch_q.append(question)\n",
    "\n",
    "                if idx%bsize==(bsize-1):\n",
    "                    ans = model.predict_answers(samples={\"image\": torch.stack(batch_img, dim=0).to(args.device), \n",
    "                                                         \"text_input\": batch_q}, inference_method=\"generate\")\n",
    "                    all_ans.append(ans)\n",
    "                    all_voc.extend(batch_label_voc)\n",
    "                    all_clu.extend(batch_label_clu)\n",
    "\n",
    "                    batch_label_voc, batch_label_clu = [], []\n",
    "                    batch_img, batch_q = [], []\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "    c_gt = np.array([vocab.mapping_idx_names[x] for x in all_voc])\n",
    "    c_pred = np.array(reduce(lambda x, y: x+y, all_ans))\n",
    "    score = measure_similarity_bert(vocab, torch.tensor(all_voc), c_pred, device=args.device)\n",
    "    print(f'score={score}')\n",
    "    res = {\n",
    "        'all_voc': all_voc,\n",
    "        'c_gt': c_gt,\n",
    "        'c_pred': c_pred,\n",
    "    }\n",
    "    \n",
    "    c_gt = np.array([' '.join(x.split('_')) for x in c_gt])\n",
    "    for c in pred_kmeans_t.unique():\n",
    "        select = (pred_kmeans_t==c).numpy()\n",
    "        item = most_frequent(c_pred[select])\n",
    "        c_pred[select] = item\n",
    "\n",
    "    score1 = measure_similarity_bert(vocab, c_pred, c_gt, device=args.device),\n",
    "    score2 = measure_similarity_bert(vocab, res['c_pred'], res['c_gt'], device=args.device)\n",
    "    print(score1)\n",
    "    print(score2)\n",
    "\n",
    "    with open(f'./cache/vqav2-{args.dataset}-train.pkl', 'wb') as f:\n",
    "        pickle.dump(res, f)\n",
    "        \n",
    "    with open(f'./cache/vqav2-{args.dataset}-train-score.pkl', 'wb') as f:\n",
    "        pickle.dump([score1, score2], f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
