{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae430ea7-ab57-477f-8a7d-1890d97d2898",
   "metadata": {},
   "source": [
    "- observe image KNN\n",
    "\n",
    "- observe initial assignment performance\n",
    "\n",
    "- observe text distribution\n",
    "\n",
    "- observe vocab filtering performance w.r.t. different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c187d77c-026e-4cf7-9000-103a8718cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/OSZSL/')\n",
    "sys.path.append('/home/sheng/OSZSL/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier\n",
    "import clip\n",
    "from data.datasets import get_datasets_oszsl, build_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184f5eb3-cd49-42e6-87b1-cb7e50d0d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cuda:2'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset_name = 'imagenet'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    \n",
    "    batch_size = 1024\n",
    "    use_def = False\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/classifier=false-em=model_ema-w_fair=0.2-conf=0.5-B128x2-test/checkpoint-current.pth'\n",
    "    clip_checkpoint = None\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b6c3e7-a7cf-4d0b-b4f6-2c8f2ae6daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../templates.json', 'rb') as f:\n",
    "    templates = json.load(f)\n",
    "\n",
    "if args.use_def:\n",
    "    with open('../templates-def.json', 'rb') as f:\n",
    "        templates = json.load(f)\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    # with open('/home/sheng/dataset/wordnet_nouns_no_abstract.pkl', 'rb') as f:\n",
    "    #     vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab()\n",
    "templates = templates['imagenet']\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c3b89-edaa-4054-a0f6-a78a82ced191",
   "metadata": {},
   "source": [
    "### build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a86114b-bc50-45ab-9c15-03cfdab3e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\" classname indexed vocab \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.indexing()\n",
    "        ### {global_synset: global_names}\n",
    "        self.mapping_ids_names = dict(zip(vocab['ids'], vocab['names']))\n",
    "        # self.mapping_names_ids = dict(zip(vocab['names'], vocab['ids']))\n",
    "        ### {local_idx: local_names}\n",
    "        self.mapping_idx_names = dict(zip(range(len(self.classnames)), self.classnames))\n",
    "        ### {local_names: local_idx}\n",
    "        self.mapping_names_idx = dict(zip(self.classnames, range(len(self.classnames))))\n",
    "        ### {global_synset: global_idx}\n",
    "        self.mapping_ids_global_idx = dict(zip(vocab['ids'], range(len(vocab['ids']))))\n",
    "        self.mapping_global_idx_ids = dict(zip(range(len(vocab['ids'])), vocab['ids']))\n",
    "        \n",
    "        ### {global_names: [global_idx]}\n",
    "        self.vocab_mapping_names_idx = defaultdict(list)\n",
    "        for k, v in zip(vocab['names'], range(len(vocab['names']))):\n",
    "            self.vocab_mapping_names_idx[k].append(v)\n",
    "            \n",
    "        ### {local_idx: global_idx}\n",
    "        self.mapping_idx_global_idx = {}\n",
    "        for i in range(len(self)):\n",
    "            self.mapping_idx_global_idx[i] = self.vocab_mapping_names_idx[self.mapping_idx_names[i]]\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.classnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.classnames[idx]\n",
    "    \n",
    "    def indexing(self):\n",
    "        self.classnames = sorted(list(set(self.vocab['names'])))\n",
    "        ### global index retained\n",
    "        # self.classnames = list(self.vocab['names'])\n",
    "        return\n",
    "    \n",
    "def calibrate_vocab(vocab, method, **kwargs):\n",
    "    if method == 'topk':\n",
    "        K = kwargs['K']\n",
    "    elif method == 'upperbound':\n",
    "        synids = kwargs['classids']\n",
    "        synids\n",
    "        \n",
    "def subsample_vocab(vocab, sampling_idx):\n",
    "    \"\"\" [to revise] subsample vocab based on @sampling_idx on @vocab \"\"\"\n",
    "    assert len(sampling_idx)>0\n",
    "    set_sampling_idx = set(sampling_idx)\n",
    "    sub_vocab = deepcopy(vocab)\n",
    "    sub_vocab.mapping_idx_ids = {k: v for k, v in vocab.mapping_idx_ids.items() if k in set_sampling_idx}\n",
    "    sub_vocab.mapping_ids_idx = {v: k for k, v in sub_vocab.mapping_idx_ids.items()}\n",
    "    set_sampling_ids = set(sub_vocab.mapping_ids_idx)\n",
    "    sub_vocab.mapping_ids_names = {k: v for k, v in vocab.mapping_ids_names.items() if k in set_sampling_ids}\n",
    "    sub_vocab.mapping_names_ids = {v: k for k, v in sub_vocab.mapping_ids_names.items()}\n",
    "    return sub_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a896e47-a890-4740-8b0b-1b06aeb77d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b7acf-c7cf-4eca-823c-0122d0d1ab04",
   "metadata": {},
   "source": [
    "### load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b61826-1398-4f56-9a19-7a62f3263af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18adb3f5-7744-4582-9eaf-29ecbe3799f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3cbe7-3f1e-4996-8e97-1c3fb735b846",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e5fcd-f94c-42dc-833b-5954236d5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset_raw = get_datasets_oszsl(args, None, is_train=True, transform=transform_val, seed=1)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_val, seed=1)\n",
    "\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8c0ad-37f2-4426-809c-49f272e2430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.labels)==len(dataset.samples), \\\n",
    "np.unique(dataset.labels), np.unique(dataset.labels_transformed), \\\n",
    "dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508fb34-0a52-4437-bb37-542ad364d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_raw.labels)==len(dataset_raw.samples), \\\n",
    "np.unique(dataset_raw.labels), np.unique(dataset_raw.labels_transformed), \\\n",
    "dataset_raw.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ef66f-e287-475e-bd63-c7adf92816c3",
   "metadata": {},
   "source": [
    "### build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50813d-9642-4b31-9a0c-ebe4456e9fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if args.use_def:\n",
    "#     batch_size = 64\n",
    "#     with torch.no_grad():\n",
    "#         zeroshot_weights = []\n",
    "#         with tqdm(total=len(vocab.classnames)//batch_size) as pbar:\n",
    "#             for idx_set, classname_set in zip(np.array_split(np.arange(len(vocab.classnames)), len(vocab.classnames)//batch_size), \n",
    "#                                      np.array_split(vocab.classnames, len(vocab.classnames)//batch_size)):\n",
    "#                 # idx_set = [vocab.mapping_idx_global_idx[ii] for ii in idx_set]\n",
    "#                 texts = [template.format(classname, defs[idx])[:77] for idx, classname in zip(idx_set, classname_set) for template in templates] #format with class\n",
    "#                 texts = tokenize(texts).to(args.device) #tokenize\n",
    "#                 class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "#                 class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "#                 class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "#                 class_embedding = class_embeddings.mean(dim=1)\n",
    "#                 class_embedding /= class_embedding.norm()\n",
    "#                 zeroshot_weights.append(class_embedding.cpu())\n",
    "\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#     classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "#     torch.save(classifier, './cache/wordnet_classifier_def.pth') \n",
    "# else:\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "    zeroshot_weights = []\n",
    "    with tqdm(total=len(vocab.classnames)//batch_size) as pbar:\n",
    "        for classname_set in np.array_split(vocab.classnames, len(vocab.classnames)//batch_size):\n",
    "            texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "            texts = tokenize(texts).to(args.device) #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=1)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding.cpu())\n",
    "            # break\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "torch.save(classifier, './cache/wordnet_classifier_no_abstract.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655360ed-29e5-4151-99b7-3d9057ddf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(args):\n",
    "    if args.use_def:\n",
    "        classifier = torch.load('./cache/wordnet_classifier_def.pth')\n",
    "    else:\n",
    "        classifier = torch.load('./cache/wordnet_classifier.pth')\n",
    "    classifier = classifier.to(args.device)\n",
    "    return classifier\n",
    "\n",
    "classifier = get_classifier(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93642e9a-d14a-4b10-aaba-c55e8bbd82a4",
   "metadata": {},
   "source": [
    "### CLIP performance investigation -- initial assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949eb4f-c18f-432d-902a-e94e36d1b35c",
   "metadata": {},
   "source": [
    "raw performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdfe8b-fa58-40bc-8674-2cef03d64910",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "all_pred_voc_topk = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_pred_voc_topk.append(prob.topk(k=5, dim=-1).indices.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_pred_voc_topk = torch.cat(all_pred_voc_topk, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8342549-3719-42d7-a44b-92385bfda05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb01d5-b93b-4d9f-8978-09e2b899f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "### topK accuracy\n",
    "for i in range(all_pred_voc_topk.size(1)):\n",
    "    vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "    for j in range(i+1):\n",
    "        vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "    print(f'k={i} acc={vec.float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded307e8-bfd9-4f56-881a-e9321be3929d",
   "metadata": {},
   "source": [
    "topK performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e62b16-2ad3-4799-b813-db5c545c8ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_soft = True\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "topK = 5\n",
    "voc_beta = 5000\n",
    "mapping_classifier = None\n",
    "\n",
    "for voc_beta in [10000, 5000, 1000, 500]:\n",
    "    \"\"\" compute distribution over classifier\n",
    "    \"\"\"\n",
    "    vec_count = torch.zeros(classifier.size(0))\n",
    "    all_gt_labels = []\n",
    "    with tqdm(total=len(loader_val)) as pbar:\n",
    "        model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_val):\n",
    "            images, label_voc, label_clu, idx_img = batch\n",
    "            images = images.to(args.device)\n",
    "            all_gt_labels.append(label_voc)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                    similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                    prob = similarity.softmax(-1)\n",
    "\n",
    "                    topk_prob = prob.topk(k=topK, dim=-1)\n",
    "                    topk_ind = topk_prob.indices\n",
    "                    topk_val = topk_prob.values.flatten().cpu().numpy()\n",
    "                    batch_counter = Counter(topk_ind.flatten().cpu().numpy())\n",
    "                    if use_soft:\n",
    "                        for i, k in enumerate(topk_ind.flatten().cpu().numpy()):\n",
    "                            vec_count[k] += topk_val[i]\n",
    "                    else:\n",
    "                        for k, v in batch_counter.items():\n",
    "                            vec_count[k] += v\n",
    "            pbar.update(1)\n",
    "\n",
    "    all_gt_labels = torch.cat(all_gt_labels, dim=0)\n",
    "    all_valid_idx = vec_count.topk(k=voc_beta).indices\n",
    "    if mapping_classifier is None:\n",
    "        mapping_classifier = torch.tensor(sorted(all_valid_idx), device=args.device)\n",
    "    else:\n",
    "        mapping_classifier = torch.gather(mapping_classifier, 0, torch.tensor(sorted(all_valid_idx), device=args.device))\n",
    "    mask = torch.zeros(classifier.size(0), device=args.device)\n",
    "    mask = torch.scatter(mask, 0, torch.tensor(sorted(all_valid_idx), device=args.device), 1)\n",
    "    classifier = classifier[mask.bool()]\n",
    "\n",
    "    \"\"\" assignment on selected vocab\n",
    "    \"\"\"\n",
    "    all_pred_voc = []\n",
    "    all_gt_voc = []\n",
    "    with tqdm(total=len(loader_val)) as pbar:\n",
    "        model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_val):\n",
    "            images, label_voc, label_clu, idx_img = batch\n",
    "            images = images.to(args.device)\n",
    "            all_gt_voc.append(label_voc)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                    similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                    prob = similarity.softmax(-1)\n",
    "\n",
    "                    pred_label = prob.argmax(dim=-1)\n",
    "                    pred_label = torch.gather(mapping_classifier, 0, pred_label)\n",
    "                    all_pred_voc.append(pred_label.cpu())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "        all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "        print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')\n",
    "\n",
    "        print(f'missing={len(set(all_gt_labels.cpu().numpy()) - set(mapping_classifier.cpu().numpy()))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7586ee-6282-4a15-8375-bf77fc869308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(vec_count.topk(k=voc_beta*dataset.num_classes).values.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381a3c8-f1a3-429f-b4b6-13a312c5edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d205833-451c-4539-b20f-227ed64045d9",
   "metadata": {},
   "source": [
    "upperbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bc5cb-e5af-4528-aabb-a6854c51db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "mask = torch.zeros(classifier.size(0), device=args.device)\n",
    "mapping_classifier = torch.tensor(sorted(set(dataset.labels)), device=args.device)\n",
    "mask = torch.scatter(mask, 0, mapping_classifier, 1)\n",
    "classifier = classifier[mask.bool()]\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "\n",
    "all_pred_voc = torch.gather(mapping_classifier.cpu(), 0, all_pred_voc)\n",
    "\n",
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52897615-fcd6-4c26-ac22-b75be99ff3a1",
   "metadata": {},
   "source": [
    "hierarchy accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef9118-91fc-47ad-9de9-a44437a83c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                # label_voc = torch.tensor(list(map(lambda x: vocab.mapping_names_idx[x], label_voc)))\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9519a02-6a92-4b2d-b350-05d5f4d83c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_i2gi = vocab.mapping_idx_global_idx\n",
    "isin = lambda x, y: np.array([xx in y for xx in x])\n",
    "all_pred_hier = []\n",
    "for i in range(len(all_gt_voc)):\n",
    "    cond1 = isin(mapping_i2gi[all_gt_voc[i].item()], reduce(lambda x,y: x|y, [parents[p] for p in mapping_i2gi[all_pred_voc[i].item()]])).any()\n",
    "    cond2 = isin(mapping_i2gi[all_pred_voc[i].item()], reduce(lambda x,y: x|y, [parents[p] for p in mapping_i2gi[all_gt_voc[i].item()]])).any()\n",
    "    pred_hier = cond1 | cond2\n",
    "    all_pred_hier.append(pred_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b82639-e18e-4da9-a0b7-36f32dfc184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_pred_hier).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a45ccd-0b81-4faa-833d-27d9ee4e4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_pred_voc==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d1a80-6c76-41d2-97d3-3863a16490c0",
   "metadata": {},
   "source": [
    "KNN performance investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb43e8-525b-4d08-a46d-96f8c19d655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "# classifier = F.normalize(classifier, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deebbc7-f951-40f6-bbe2-19ea353341c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = classifier@classifier.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ab39c-83e4-4414-9282-85f5db9b1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "topk_ind = similarity.topk(k=K+1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5457c3-7439-43d6-88f0-56949b23ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(map(lambda x: list(map(lambda y: classnames[y], x)), topk_ind.cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040aa32-b9f8-49d0-b48f-14063c0d670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae789b-5b77-4a51-8964-7959c853c6d6",
   "metadata": {},
   "source": [
    "### SSL performance investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d03d08de-9921-4117-995a-acdb887815ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sheng/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "arch = 'vit_dino'\n",
    "# arch = 'resnet_dino'\n",
    "\n",
    "def load_checkpoint(arch):\n",
    "    if arch == 'resnet_dino':\n",
    "        ckpt = torch.load('/home/sheng/dino/cache/make_nonliving26-resnet50_test_E500/checkpoint.pth', map_location='cpu')\n",
    "    elif arch == 'vit_dino':\n",
    "        # ckpt = torch.load('/home/sheng/SimGCD/cache/checkpoints/checkpoint.pth', map_location='cpu')\n",
    "        ckpt = torch.load('/home/sheng/OSZSL/cache/nonliving26_vitb16.pth', map_location='cpu')\n",
    "\n",
    "    state_dict = {k[len('module.'):]: v for k, v in ckpt['student'].items() if k.startswith('module.')}\n",
    "    state_dict = {k[len('backbone.'):]: v for k, v in state_dict.items() if k.startswith('backbone.')}\n",
    "    state_dict = torch.load('/home/sheng/dino/checkpoint/dino_vitbase16_pretrain.pth', map_location='cpu')\n",
    "    return state_dict\n",
    "\n",
    "state_dict = load_checkpoint(arch)\n",
    "if arch == 'resnet_dino':\n",
    "    modelf = torchvision.models.resnet50()\n",
    "    modelf.load_state_dict(state_dict, strict=False)\n",
    "    modelf.fc = nn.Identity()\n",
    "elif arch == 'vit_dino':\n",
    "    modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "    modelf.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff27d33-f618-42fb-93a7-e8229194573e",
   "metadata": {},
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55f9a3-8c54-47a7-87c9-abfef3e09bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SSL clustering acc \"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_idx_img = []\n",
    "modelf = modelf.to(args.device)\n",
    "modelf.eval()\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    for batch in loader_f:\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = modelf(images)\n",
    "            features = F.normalize(features, dim=-1)\n",
    "            \n",
    "        all_features.append(features.detach().cpu())\n",
    "        all_labels.append(label_clu)\n",
    "        all_idx_img.append(idx_img)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_idx_img = torch.cat(all_idx_img, dim=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(all_labels.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_predict(all_features.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c1b69-d8de-4416-8465-feecb0d731ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1000000, 5).storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feef416-3f3f-4068-9aab-867435ad5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1000000, 5)\n",
    "x.size * x.itemsize, torch.from_numpy(x).storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea0560-9495-459f-9295-40090979c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu-{args.dataset_name}-train-{arch}.npy', pred_clu)\n",
    "# pred_clu = np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ee39-4248-4c84-95f9-61f6e8baf7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features = all_features.to(args.device)\n",
    "sim = all_features@all_features.t()\n",
    "np.save(f'./knn_ind-{args.dataset_name}-train-{arch}.npy', sim.topk(k=300).indices.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e20167-6022-4ee0-8e95-ddd93dde38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72e71e-2619-4f46-884d-630ce5ca068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d59f21-7e3a-4d8d-9d92-564e0b290224",
   "metadata": {},
   "source": [
    "CLIP clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b1f66-2810-4e10-a66f-1e5e8ef58010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clu = np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bf01e-46bb-4aaa-ae59-260bfc32baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CLIP clustering acc \"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_idx_img = []\n",
    "modelf = modelf.to(args.device)\n",
    "modelf.eval()\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    for batch in loader_f:\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images)\n",
    "                features = F.normalize(features, dim=-1)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "        all_features.append(features.detach().cpu())\n",
    "        all_labels.append(label_clu)\n",
    "        all_idx_img.append(idx_img)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_idx_img = torch.cat(all_idx_img, dim=0)\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())\n",
    "\n",
    "# kmeans = KMeans(n_clusters=len(all_labels.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "# pred_clu = kmeans.fit_predict(all_features.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f4feb-e99d-4d4c-aa7c-5c4a801f030c",
   "metadata": {},
   "source": [
    "Cluster navigation (depends on clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de786f98-6bb8-4643-b070-de7187cd46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_labels = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cdc6a-707c-45cf-8bda-d76f80bed2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "### per predicted-cluster voting\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "\n",
    "pred_kmeans_t = pred_kmeans\n",
    "# for it in range(10):\n",
    "#     print(f'iteration {it}')\n",
    "\n",
    "# cluster agg\n",
    "all_clu_pred = []\n",
    "for i in range(len(all_gt_voc.unique())):\n",
    "    selected = (pred_kmeans==i)\n",
    "    clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "    all_clu_pred.append(clu_pred)\n",
    "all_clu_pred = torch.stack(all_clu_pred, dim=0)\n",
    "\n",
    "# linear assignment\n",
    "print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "\n",
    "cost_mat = all_clu_pred.cpu().numpy()\n",
    "res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "label_kmeans_voc = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "\n",
    "print('instance label acc::', (label_kmeans_voc==all_gt_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4979ece-f872-406c-ad93-72b2970f0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_labels = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f56c2b-1baf-434d-a4fe-7a35fc495272",
   "metadata": {},
   "outputs": [],
   "source": [
    "### subset vocab\n",
    "col_subset = all_clu_pred.nonzero()[:, 1]\n",
    "col_subset = col_subset.unique().sort().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788bf5fc-fcbb-49df-a2c0-a6155255e9c3",
   "metadata": {},
   "source": [
    "KNN investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52721295-8e5c-4baf-9e70-0434168d1867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neighborhood_size = np.arange(5, 500, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    topk_res = similarity.topk(k=K+1)\n",
    "    topk_ind = topk_res.indices[:, 1:]\n",
    "    topk_match = torch.gather(label_match, 1, topk_ind)\n",
    "    topk_acc = topk_match.float().mean(dim=-1).mean()\n",
    "    print(f'K={K} acc={topk_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519d41-07c4-44e5-b649-5b170e3e1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.graph import compute_consensus_on_features\n",
    "neighborhood_size = np.arange(5, 50, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    _, _, pred_affinity = compute_consensus_on_features(all_features, k=K+1, q=0.8)\n",
    "    acc = ((pred_affinity & label_match).float().sum(1)/(pred_affinity.float().sum(1)+1e-10)).mean()\n",
    "    n_nn = pred_affinity.float().sum(1).mean()\n",
    "    print(f'K={K} acc={acc} n_nn={n_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970ede6-7bd9-408d-a903-30200acf6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_size = np.arange(5, 50, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    _, _, pred_affinity = compute_consensus_on_features(all_features, k=K+1, q=0.5)\n",
    "    acc = ((pred_affinity & label_match).float().sum(1)/(pred_affinity.float().sum(1)+1e-10)).mean()\n",
    "    n_nn = pred_affinity.float().sum(1).mean()\n",
    "    print(f'K={K} acc={acc} n_nn={n_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a399a6e-531d-41bd-b16e-c8b0b40f9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" KNN matrix output \"\"\"\n",
    "neighborhood_size = 315\n",
    "similarity = all_features@all_features.T\n",
    "label_match = (all_labels.view(-1, 1)==all_labels.view(1, -1))\n",
    "K = neighborhood_size\n",
    "topk_res = similarity.topk(k=K+1)\n",
    "topk_ind = topk_res.indices\n",
    "\n",
    "torch.save(topk_ind, f'./cache/{args.dataset_name}-clip-knn-{neighborhood_size}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcd275-c353-44f0-b13c-e47a1729d4c6",
   "metadata": {},
   "source": [
    "### SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0a1f64-e602-447c-9b56-045118530fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sheng/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b73b47b-eb67-48ce-9d9e-b0fc1c8c17b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04ab17-8a37-4f5f-8ea1-19472a568a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [02:59<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "# initial topK prediction from CLIP\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_prob = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                # all_prob.append(prob.cpu())\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "# all_prob = torch.cat(all_prob, dim=0)\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a6e4-5e06-43cb-ba7e-82de5b1cf268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster agg\n",
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc):\n",
    "    all_clu_pred = []\n",
    "    for i in range(args.nb_classes):\n",
    "        selected = (pred_kmeans==i)\n",
    "        clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0)\n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc):\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    print('instance label acc::', (label_voc_kmeans==all_gt_voc).float().mean().item())\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args, all_prob=None):\n",
    "    label_voc_kmeans = label_voc_kmeans.to(args.device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch\n",
    "                images = images.to(args.device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        logits = model.visual(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                        similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99ca3405-7460-4d17-9208-dcad1edcc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count = selected.sum().item()\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected]).ravel(), return_counts=True)\n",
    "        counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    print('instance label acc::', (label_voc_kmeans==all_gt_voc).float().mean().item())\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, all_prob=None):\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        logits = model.visual(images)\n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        similarity = 100 * logits @ classifier.t()\n",
    "                        prob = similarity.softmax(-1)\n",
    "                        cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3b3caf2-b17b-4590-8dfe-ec8e6a0a5695",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_by_pred_cluster\n",
      "is mutex assignment:: False\n",
      "assignment collision num:: 3\n",
      "linear_assign\n",
      "instance label acc:: 0.370339035987854\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 83/126 [04:00<02:00,  2.80s/it]"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "for t in range(3):\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e4806-1027-4a44-a160-a637db4fdf6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_kmeans_t = pred_kmeans\n",
    "# for t in range(5):\n",
    "#     all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t, all_topk_voc)\n",
    "#     label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "#     pred_kmeans_t = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args, all_prob=None)\n",
    "#     set_pred = set(res_ass[1].tolist())\n",
    "#     set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "#     print('missing label::', len(set_gt - set_pred))\n",
    "#     print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "\n",
    "\n",
    "\"\"\" get confident prediction \"\"\"\n",
    "th = 0.5\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "label_voc_kmeans_t = label_voc_kmeans_t.to(args.device)\n",
    "cluster_ind = []\n",
    "selected_ind = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity[:, label_voc_kmeans_t].softmax(dim=-1)\n",
    "                selected = (prob.amax(dim=-1)>th)\n",
    "                selected_ind.append(selected.cpu())\n",
    "        pbar.update(1)\n",
    "selected_ind = torch.cat(selected_ind, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b57c9c7-64e1-423b-837f-30751c506af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cluster_acc(y_true=all_label_clu[selected_ind].numpy(), y_pred=pred_kmeans_t[selected_ind].numpy())\n",
    "recall = selected_ind.mean()\n",
    "print(f'confidence selection precision={precision} recall={recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f944bd-0cb4-459f-8cd1-1a61aed38507",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688f3cb-d754-4c42-bf39-242029dbd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "1. inverse entropy of prototype\n",
    "\n",
    "2. top1 sim of proto-image\n",
    "\n",
    "3. top1 sim of image-proto\n",
    "\n",
    "\"\"\"\n",
    "# candidate_ind = res_ass[1].unique()\n",
    "# cls_proto_similarity = torch.zeros([len(dataset_f), candidate_ind.size()])\n",
    "all_sim_proto_image_pred = []\n",
    "all_sim_proto_image_gt = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        label_voc = label_voc.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(dim=-1)\n",
    "                all_sim_proto_image_pred.append(similarity[:, prob.argmax(dim=-1)].cpu())\n",
    "                all_sim_proto_image_gt.append(similarity[:, label_voc].cpu())\n",
    "        pbar.update(1)\n",
    "        \n",
    "all_sim_proto_image_pred = torch.cat(all_sim_proto_image_pred, dim=0)\n",
    "all_sim_proto_image_gt = torch.cat(all_sim_proto_image_gt, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c4e5b-8db6-4a06-8901-cd8a9d43086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sim_proto_image_pred = torch.cat(all_sim_proto_image_pred, dim=0)\n",
    "all_sim_proto_image_gt = torch.cat(all_sim_proto_image_gt, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bc41f-987d-4d7e-b046-fdf11ab15f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f7279-c6e2-47d1-b081-271e3994db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_match = all_label_clu.view(-1, 1)@all_label_clu.view(1, -1)\n",
    "pred_match_init = pred_kmeans.view(-1, 1)@pred_kmeans.view(1, -1)\n",
    "pred_match = pred_kmeans_t.view(-1, 1)@pred_kmeans_t.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbc974-008f-4dfa-9c4b-618bcbf358f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_consensus = (pred_match_init==pred_match) \n",
    "((pred_consensus & label_match).float().sum(dim=-1) / (pred_consensus.sum(dim=-1)+1e-20)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfae22d-5c05-4e6d-ac26-d3fd78c69dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred_consensus & label_match).float().sum(dim=-1).bool().float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67817627-f7fa-41a6-b783-4986beea5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clu_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c306e3-a6a3-4731-84d6-cd0e5d845ea4",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fde93f02-13a3-498c-ba18-67de692b8e3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sheng/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))\n",
    "    \n",
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d793092-23ab-497e-9759-714c91e55e51",
   "metadata": {},
   "source": [
    "collect variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3641aa10-d6ea-4434-b4b3-57058a9e1f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac099527-5211-4e10-a50f-1ebc926834c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "# initial topK prediction from CLIP\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_prob = []\n",
    "all_max_ind = []\n",
    "all_topk_vocinds = []\n",
    "all_label_clu = []\n",
    "all_topk_vals = []\n",
    "all_topk_inds = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                # all_prob.append(prob.cpu())\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_max_ind.append(prob.argmax(dim=-1).cpu())\n",
    "                all_topk_vocinds.append(prob.topk(k=10, dim=-1).indices.cpu())\n",
    "                \n",
    "                batch_topk_res = prob.topk(k=20, dim=-1)\n",
    "                all_topk_vals.append(batch_topk_res.values.cpu())\n",
    "                all_topk_inds.append(batch_topk_res.indices.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "# all_prob = torch.cat(all_prob, dim=0)\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_max_ind = torch.cat(all_max_ind, dim=0)\n",
    "all_topk_vocinds = torch.cat(all_topk_vocinds, dim=0)\n",
    "all_topk_vals = torch.cat(all_topk_vals, dim=0)\n",
    "all_topk_inds = torch.cat(all_topk_inds, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b4a31-0d72-45d8-bcdd-97c69944fecb",
   "metadata": {},
   "source": [
    "text proto inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159430a-4362-40b9-9181-1c08c1de63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN proto analysis\n",
    "text_sim = classifier[all_gt_voc.unique(), :]@classifier.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5c99a-edbd-485e-9082-ab819e27db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_topk = text_sim.topk(k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a41ba7-f29d-4697-ae19-399d222cc970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(np.array([vocab.mapping_idx_names[t.item()] for t in text_topk.indices[:, :].flatten().cpu()]).reshape(text_sim.size(0), -1).tolist(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f20703-1fc0-4791-9242-591d6a7b10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.distplot(all_topk_vals[:, 0].cpu().numpy(), bins=200)\n",
    "sns.distplot(all_topk_vals[:, 1].cpu().numpy(), bins=200)\n",
    "sns.distplot(all_topk_vals[:, 2].cpu().numpy(), bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075cda2-8944-48de-b2f5-8e9517de5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = (all_topk_vals[:, 0]>0.7)\n",
    "'top1 acc', (all_gt_voc[selected] == all_max_ind[selected]).float().mean(), \\\n",
    "'selected percentile', selected.float().mean(), \\\n",
    "'class diversity', len(set(all_gt_voc.unique().numpy()) - set(all_max_ind[selected].unique().numpy())), \\\n",
    "'topk inclusion', torch.stack([all_topk_vocinds[selected, i]==all_gt_voc[selected] for i in range(all_topk_vocinds.size(1))], dim=1).float().sum(dim=-1).bool().float().mean(), \\\n",
    "'selected sample pred voc size', len(all_max_ind[selected].unique()), \\\n",
    "'average selected instance number per class', selected.sum()/len(all_gt_voc.unique()),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888780d-22f0-47f2-a216-d5b78a5dbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e444ff3-7837-4e21-a5eb-d21c6111fd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" initial clip assignment \"\"\"\n",
    "list(filter(lambda x: x[1]<100, [(i.item(), (all_max_ind==i).sum().item()) for i in all_gt_voc.unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15937caa-ab71-4efd-b334-634f64485d73",
   "metadata": {},
   "source": [
    "text to image entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9165942-c523-4821-9ddf-da7e57c3e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [04:16<00:00,  2.04s/it]\n",
      "100%|██████████| 126/126 [04:55<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "### get all label and predicted label\n",
    "all_label_voc = []\n",
    "all_pred_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                pred_voc = prob.argmax(dim=-1)\n",
    "        all_label_voc.append(label_voc)\n",
    "        all_pred_voc.append(pred_voc.cpu())\n",
    "        all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "all_label_voc = torch.cat(all_label_voc, dim=0)\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "                \n",
    "### compute entropy\n",
    "set_all_label_voc = all_label_voc.unique()\n",
    "set_all_pred_voc = all_pred_voc.unique()\n",
    "selected_classifier_ind = torch.cat([set_all_label_voc, set_all_pred_voc], dim=0).unique()\n",
    "all_selected_sim = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        label_voc = label_voc.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                all_selected_sim.append(similarity[:, selected_classifier_ind].cpu())\n",
    "        pbar.update(1)\n",
    "        \n",
    "all_selected_sim = torch.cat(all_selected_sim, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23ded316-eb1c-43f6-93a0-b65682197242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans_t = torch.from_numpy(np.load(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy'))\n",
    "classwise_all_selected_sim = []\n",
    "for c in pred_kmeans_t.unique():\n",
    "    subset = (pred_kmeans_t==c)\n",
    "    classwise_all_selected_sim.append(all_selected_sim[subset, :].mean(dim=0).cpu())\n",
    "classwise_all_selected_sim = torch.stack(classwise_all_selected_sim, dim=0)\n",
    "p = classwise_all_selected_sim.float().softmax(dim=0)\n",
    "ent = (-p*(p+1e-10).log2()).sum(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056f95cb-d240-4f8e-bff3-89d1ee2b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = torch.nan_to_num(ent)\n",
    "ent_gt = ent[torch.isin(selected_classifier_ind, set_all_label_voc)]\n",
    "ent_pred = ent[torch.isin(selected_classifier_ind, set_all_pred_voc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1d2bd40-54aa-4cd3-8b70-9818908b212a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 768x512 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=128)\n",
    "sns.distplot(ent_gt.numpy(), bins=100)\n",
    "sns.distplot(ent_pred.numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b5420f-f0cd-4283-b407-23b711156895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14), tensor(9), tensor(3176), tensor(4867))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ent_gt>2).sum(), (ent_gt>3).sum(), \\\n",
    "(ent_pred<2).sum(), (ent_pred<3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2423501f-0554-4cfc-8457-5d5b2deb434c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14), tensor(9), tensor(3176), tensor(4867))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ent_gt>2).sum(), (ent_gt>3).sum(), \\\n",
    "(ent_pred<2).sum(), (ent_pred<3).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
