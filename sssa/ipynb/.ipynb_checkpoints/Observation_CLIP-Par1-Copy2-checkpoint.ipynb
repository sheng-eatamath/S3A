{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae430ea7-ab57-477f-8a7d-1890d97d2898",
   "metadata": {},
   "source": [
    "- observe image KNN\n",
    "\n",
    "- observe initial assignment performance\n",
    "\n",
    "- observe text distribution\n",
    "\n",
    "- observe vocab filtering performance w.r.t. different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c187d77c-026e-4cf7-9000-103a8718cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/OSZSL/')\n",
    "sys.path.append('/home/sheng/OSZSL/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier\n",
    "import clip\n",
    "from data.datasets import get_datasets_oszsl, build_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184f5eb3-cd49-42e6-87b1-cb7e50d0d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    device = 'cuda:1'\n",
    "    arch = 'ViT-B/16'\n",
    "    dataset_name = 'make_entity13'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/make_nonliving26/classifier=false-em=model_ema-w_fair=0.2-conf=0.5-B128x2-test/checkpoint-current.pth'\n",
    "    # clip_checkpoint = '/home/sheng/MUST-output/imagenet/imagenet-ssl_cluster=true-B128/checkpoint-current.pth'\n",
    "    clip_checkpoint = None\n",
    "    f_classifier = None #'./cache/wordnet_classifier_small_templates.pth'\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b6c3e7-a7cf-4d0b-b4f6-2c8f2ae6daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../templates.json', 'rb') as f:\n",
    "    templates = json.load(f)\n",
    "\n",
    "if args.use_def:\n",
    "    with open('../templates-def.json', 'rb') as f:\n",
    "        templates = json.load(f)\n",
    "\n",
    "def get_vocab():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocab: {`names`: list, `ids`: synset ids, `parents`: [{synset ids}]}\n",
    "    \"\"\"\n",
    "    with open('/home/sheng/dataset/wordnet_nouns.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    # with open('/home/sheng/dataset/wordnet_nouns_no_abstract.pkl', 'rb') as f:\n",
    "    #     vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab()\n",
    "templates = templates['imagenet']\n",
    "classnames = vocab['names']\n",
    "parents = vocab['parents']\n",
    "defs = vocab['def']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c3b89-edaa-4054-a0f6-a78a82ced191",
   "metadata": {},
   "source": [
    "### build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a86114b-bc50-45ab-9c15-03cfdab3e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\" classname indexed vocab \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.indexing()\n",
    "        ### {global_synset: global_names}\n",
    "        self.mapping_ids_names = dict(zip(vocab['ids'], vocab['names']))\n",
    "        # self.mapping_names_ids = dict(zip(vocab['names'], vocab['ids']))\n",
    "        ### {local_idx: local_names}\n",
    "        self.mapping_idx_names = dict(zip(range(len(self.classnames)), self.classnames))\n",
    "        ### {local_names: local_idx}\n",
    "        self.mapping_names_idx = dict(zip(self.classnames, range(len(self.classnames))))\n",
    "        ### {global_synset: global_idx}\n",
    "        self.mapping_ids_global_idx = dict(zip(vocab['ids'], range(len(vocab['ids']))))\n",
    "        self.mapping_global_idx_ids = dict(zip(range(len(vocab['ids'])), vocab['ids']))\n",
    "        \n",
    "        ### {global_names: [global_idx]}\n",
    "        self.vocab_mapping_names_idx = defaultdict(list)\n",
    "        for k, v in zip(vocab['names'], range(len(vocab['names']))):\n",
    "            self.vocab_mapping_names_idx[k].append(v)\n",
    "            \n",
    "        ### {local_idx: global_idx}\n",
    "        self.mapping_idx_global_idx = {}\n",
    "        for i in range(len(self)):\n",
    "            self.mapping_idx_global_idx[i] = self.vocab_mapping_names_idx[self.mapping_idx_names[i]]\n",
    "\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.classnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.classnames[idx]\n",
    "    \n",
    "    def indexing(self):\n",
    "        self.classnames = sorted(list(set(self.vocab['names'])))\n",
    "        ### global index retained\n",
    "        # self.classnames = list(self.vocab['names'])\n",
    "        return\n",
    "    \n",
    "def calibrate_vocab(vocab, method, **kwargs):\n",
    "    if method == 'topk':\n",
    "        K = kwargs['K']\n",
    "    elif method == 'upperbound':\n",
    "        synids = kwargs['classids']\n",
    "        synids\n",
    "        \n",
    "def subsample_vocab(vocab, sampling_idx):\n",
    "    \"\"\" [to revise] subsample vocab based on @sampling_idx on @vocab \"\"\"\n",
    "    assert len(sampling_idx)>0\n",
    "    set_sampling_idx = set(sampling_idx)\n",
    "    sub_vocab = deepcopy(vocab)\n",
    "    sub_vocab.mapping_idx_ids = {k: v for k, v in vocab.mapping_idx_ids.items() if k in set_sampling_idx}\n",
    "    sub_vocab.mapping_ids_idx = {v: k for k, v in sub_vocab.mapping_idx_ids.items()}\n",
    "    set_sampling_ids = set(sub_vocab.mapping_ids_idx)\n",
    "    sub_vocab.mapping_ids_names = {k: v for k, v in vocab.mapping_ids_names.items() if k in set_sampling_ids}\n",
    "    sub_vocab.mapping_names_ids = {v: k for k, v in sub_vocab.mapping_ids_names.items()}\n",
    "    return sub_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a896e47-a890-4740-8b0b-1b06aeb77d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b7acf-c7cf-4eca-823c-0122d0d1ab04",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b61826-1398-4f56-9a19-7a62f3263af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18adb3f5-7744-4582-9eaf-29ecbe3799f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3cbe7-3f1e-4996-8e97-1c3fb735b846",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4e5fcd-f94c-42dc-833b-5954236d5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset_raw = get_datasets_oszsl(args, None, is_train=True, transform=transform_val, seed=1)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_val, seed=1)\n",
    "\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8c0ad-37f2-4426-809c-49f272e2430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.labels)==len(dataset.samples), \\\n",
    "np.unique(dataset.labels), np.unique(dataset.labels_transformed), \\\n",
    "dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508fb34-0a52-4437-bb37-542ad364d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_raw.labels)==len(dataset_raw.samples), \\\n",
    "np.unique(dataset_raw.labels), np.unique(dataset_raw.labels_transformed), \\\n",
    "dataset_raw.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ef66f-e287-475e-bd63-c7adf92816c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50813d-9642-4b31-9a0c-ebe4456e9fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if args.use_def:\n",
    "#     batch_size = 64\n",
    "#     with torch.no_grad():\n",
    "#         zeroshot_weights = []\n",
    "#         with tqdm(total=len(vocab.classnames)//batch_size) as pbar:\n",
    "#             for idx_set, classname_set in zip(np.array_split(np.arange(len(vocab.classnames)), len(vocab.classnames)//batch_size), \n",
    "#                                      np.array_split(vocab.classnames, len(vocab.classnames)//batch_size)):\n",
    "#                 # idx_set = [vocab.mapping_idx_global_idx[ii] for ii in idx_set]\n",
    "#                 texts = [template.format(classname, defs[idx])[:77] for idx, classname in zip(idx_set, classname_set) for template in templates] #format with class\n",
    "#                 texts = tokenize(texts).to(args.device) #tokenize\n",
    "#                 class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "#                 class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "#                 class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "#                 class_embedding = class_embeddings.mean(dim=1)\n",
    "#                 class_embedding /= class_embedding.norm()\n",
    "#                 zeroshot_weights.append(class_embedding.cpu())\n",
    "\n",
    "#                 pbar.update(1)\n",
    "\n",
    "#     classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "#     torch.save(classifier, './cache/wordnet_classifier_def.pth') \n",
    "# else:\n",
    "batch_size = 64\n",
    "with torch.no_grad():\n",
    "    zeroshot_weights = []\n",
    "    with tqdm(total=len(vocab.classnames)//batch_size) as pbar:\n",
    "        for classname_set in np.array_split(vocab.classnames, len(vocab.classnames)//batch_size):\n",
    "            texts = [template.format(classname) for classname in classname_set for template in templates] #format with class\n",
    "            texts = tokenize(texts).to(args.device) #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings = class_embeddings.view(-1, len(templates), class_embeddings.size(-1))\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=1)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding.cpu())\n",
    "            # break\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "classifier = torch.cat(zeroshot_weights, dim=0)\n",
    "torch.save(classifier, './cache/wordnet_classifier_no_abstract.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655360ed-29e5-4151-99b7-3d9057ddf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(args):\n",
    "    if args.use_def:\n",
    "        classifier = torch.load('./cache/wordnet_classifier_def.pth')\n",
    "    else:\n",
    "        classifier = torch.load('./cache/wordnet_classifier.pth')\n",
    "    classifier = classifier.to(args.device)\n",
    "    return classifier\n",
    "\n",
    "classifier = get_classifier(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93642e9a-d14a-4b10-aaba-c55e8bbd82a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CLIP performance investigation -- initial assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949eb4f-c18f-432d-902a-e94e36d1b35c",
   "metadata": {},
   "source": [
    "raw performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdfe8b-fa58-40bc-8674-2cef03d64910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "all_pred_voc_topk = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_pred_voc_topk.append(prob.topk(k=5, dim=-1).indices.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_pred_voc_topk = torch.cat(all_pred_voc_topk, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8342549-3719-42d7-a44b-92385bfda05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb01d5-b93b-4d9f-8978-09e2b899f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "### topK accuracy\n",
    "for i in range(all_pred_voc_topk.size(1)):\n",
    "    vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "    for j in range(i+1):\n",
    "        vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "    print(f'k={i} acc={vec.float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded307e8-bfd9-4f56-881a-e9321be3929d",
   "metadata": {},
   "source": [
    "topK performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e62b16-2ad3-4799-b813-db5c545c8ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_soft = True\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "topK = 5\n",
    "voc_beta = 5000\n",
    "mapping_classifier = None\n",
    "\n",
    "for voc_beta in [10000, 5000, 1000, 500]:\n",
    "    \"\"\" compute distribution over classifier\n",
    "    \"\"\"\n",
    "    vec_count = torch.zeros(classifier.size(0))\n",
    "    all_gt_labels = []\n",
    "    with tqdm(total=len(loader_val)) as pbar:\n",
    "        model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_val):\n",
    "            images, label_voc, label_clu, idx_img = batch\n",
    "            images = images.to(args.device)\n",
    "            all_gt_labels.append(label_voc)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                    similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                    prob = similarity.softmax(-1)\n",
    "\n",
    "                    topk_prob = prob.topk(k=topK, dim=-1)\n",
    "                    topk_ind = topk_prob.indices\n",
    "                    topk_val = topk_prob.values.flatten().cpu().numpy()\n",
    "                    batch_counter = Counter(topk_ind.flatten().cpu().numpy())\n",
    "                    if use_soft:\n",
    "                        for i, k in enumerate(topk_ind.flatten().cpu().numpy()):\n",
    "                            vec_count[k] += topk_val[i]\n",
    "                    else:\n",
    "                        for k, v in batch_counter.items():\n",
    "                            vec_count[k] += v\n",
    "            pbar.update(1)\n",
    "\n",
    "    all_gt_labels = torch.cat(all_gt_labels, dim=0)\n",
    "    all_valid_idx = vec_count.topk(k=voc_beta).indices\n",
    "    if mapping_classifier is None:\n",
    "        mapping_classifier = torch.tensor(sorted(all_valid_idx), device=args.device)\n",
    "    else:\n",
    "        mapping_classifier = torch.gather(mapping_classifier, 0, torch.tensor(sorted(all_valid_idx), device=args.device))\n",
    "    mask = torch.zeros(classifier.size(0), device=args.device)\n",
    "    mask = torch.scatter(mask, 0, torch.tensor(sorted(all_valid_idx), device=args.device), 1)\n",
    "    classifier = classifier[mask.bool()]\n",
    "\n",
    "    \"\"\" assignment on selected vocab\n",
    "    \"\"\"\n",
    "    all_pred_voc = []\n",
    "    all_gt_voc = []\n",
    "    with tqdm(total=len(loader_val)) as pbar:\n",
    "        model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_val):\n",
    "            images, label_voc, label_clu, idx_img = batch\n",
    "            images = images.to(args.device)\n",
    "            all_gt_voc.append(label_voc)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                    similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                    prob = similarity.softmax(-1)\n",
    "\n",
    "                    pred_label = prob.argmax(dim=-1)\n",
    "                    pred_label = torch.gather(mapping_classifier, 0, pred_label)\n",
    "                    all_pred_voc.append(pred_label.cpu())\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "        all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "        print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')\n",
    "\n",
    "        print(f'missing={len(set(all_gt_labels.cpu().numpy()) - set(mapping_classifier.cpu().numpy()))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7586ee-6282-4a15-8375-bf77fc869308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(vec_count.topk(k=voc_beta*dataset.num_classes).values.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381a3c8-f1a3-429f-b4b6-13a312c5edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d205833-451c-4539-b20f-227ed64045d9",
   "metadata": {},
   "source": [
    "upperbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bc5cb-e5af-4528-aabb-a6854c51db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "mask = torch.zeros(classifier.size(0), device=args.device)\n",
    "mapping_classifier = torch.tensor(sorted(set(dataset.labels)), device=args.device)\n",
    "mask = torch.scatter(mask, 0, mapping_classifier, 1)\n",
    "classifier = classifier[mask.bool()]\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "\n",
    "all_pred_voc = torch.gather(mapping_classifier.cpu(), 0, all_pred_voc)\n",
    "\n",
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52897615-fcd6-4c26-ac22-b75be99ff3a1",
   "metadata": {},
   "source": [
    "hierarchy accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef9118-91fc-47ad-9de9-a44437a83c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                # label_voc = torch.tensor(list(map(lambda x: vocab.mapping_names_idx[x], label_voc)))\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9519a02-6a92-4b2d-b350-05d5f4d83c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_i2gi = vocab.mapping_idx_global_idx\n",
    "isin = lambda x, y: np.array([xx in y for xx in x])\n",
    "all_pred_hier = []\n",
    "for i in range(len(all_gt_voc)):\n",
    "    cond1 = isin(mapping_i2gi[all_gt_voc[i].item()], reduce(lambda x,y: x|y, [parents[p] for p in mapping_i2gi[all_pred_voc[i].item()]])).any()\n",
    "    cond2 = isin(mapping_i2gi[all_pred_voc[i].item()], reduce(lambda x,y: x|y, [parents[p] for p in mapping_i2gi[all_gt_voc[i].item()]])).any()\n",
    "    pred_hier = cond1 | cond2\n",
    "    all_pred_hier.append(pred_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b82639-e18e-4da9-a0b7-36f32dfc184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_pred_hier).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a45ccd-0b81-4faa-833d-27d9ee4e4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_pred_voc==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d1a80-6c76-41d2-97d3-3863a16490c0",
   "metadata": {},
   "source": [
    "KNN performance investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb43e8-525b-4d08-a46d-96f8c19d655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "# classifier = F.normalize(classifier, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deebbc7-f951-40f6-bbe2-19ea353341c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = classifier@classifier.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ab39c-83e4-4414-9282-85f5db9b1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "topk_ind = similarity.topk(k=K+1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5457c3-7439-43d6-88f0-56949b23ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(map(lambda x: list(map(lambda y: classnames[y], x)), topk_ind.cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040aa32-b9f8-49d0-b48f-14063c0d670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae789b-5b77-4a51-8964-7959c853c6d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SSL performance investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03d08de-9921-4117-995a-acdb887815ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sheng/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "arch = 'vit_dino'\n",
    "# arch = 'resnet_dino'\n",
    "\n",
    "def load_checkpoint(arch, model_fpath=None):\n",
    "    if arch == 'resnet_dino':\n",
    "        ckpt = torch.load('/home/sheng/dino/cache/make_nonliving26-resnet50_test_E500/checkpoint.pth', map_location='cpu')\n",
    "    elif arch == 'vit_dino':\n",
    "        # ckpt = torch.load('/home/sheng/SimGCD/cache/checkpoints/checkpoint.pth', map_location='cpu')\n",
    "        ckpt = torch.load('/home/sheng/OSZSL/cache/nonliving26_vitb16.pth', map_location='cpu')\n",
    "\n",
    "    state_dict = {k[len('module.'):]: v for k, v in ckpt['student'].items() if k.startswith('module.')}\n",
    "    state_dict = {k[len('backbone.'):]: v for k, v in state_dict.items() if k.startswith('backbone.')}\n",
    "    if model_fpath is None:\n",
    "        state_dict = torch.load('/home/sheng/dino/checkpoint/dino_vitbase16_pretrain.pth', map_location='cpu')\n",
    "    else:\n",
    "        state_dict = torch.load(model_fpath, map_location='cpu')\n",
    "        state_dict = {k[2:]:v for k, v in state_dict['model'].items() if k.startswith('0.')}\n",
    "    return state_dict\n",
    "\n",
    "state_dict = load_checkpoint(arch, '/home/sheng/SimGCD/cache/make_nonliving26/log/debug/checkpoints/model.pt')\n",
    "if arch == 'resnet_dino':\n",
    "    modelf = torchvision.models.resnet50()\n",
    "    modelf.load_state_dict(state_dict, strict=False)\n",
    "    modelf.fc = nn.Identity()\n",
    "elif arch == 'vit_dino':\n",
    "    modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "    msg = modelf.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=0)\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff27d33-f618-42fb-93a7-e8229194573e",
   "metadata": {},
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55f9a3-8c54-47a7-87c9-abfef3e09bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SSL clustering acc \"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_idx_img = []\n",
    "modelf = modelf.to(args.device)\n",
    "modelf.eval()\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    for batch in loader_f:\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = modelf(images)\n",
    "            features = F.normalize(features, dim=-1)\n",
    "            \n",
    "        all_features.append(features.detach().cpu())\n",
    "        all_labels.append(label_clu)\n",
    "        all_idx_img.append(idx_img)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_idx_img = torch.cat(all_idx_img, dim=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(all_labels.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_predict(all_features.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25c32183-8922-497a-b997-7c6126112d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu-{args.dataset_name}-train-{arch}-dino_stage1.npy', pred_clu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea0560-9495-459f-9295-40090979c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu-{args.dataset_name}-train-{arch}.npy', pred_clu)\n",
    "# pred_clu = np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ee39-4248-4c84-95f9-61f6e8baf7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features = all_features.to(args.device)\n",
    "sim = all_features@all_features.t()\n",
    "np.save(f'./knn_ind-{args.dataset_name}-train-{arch}.npy', sim.topk(k=300).indices.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e20167-6022-4ee0-8e95-ddd93dde38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72e71e-2619-4f46-884d-630ce5ca068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d59f21-7e3a-4d8d-9d92-564e0b290224",
   "metadata": {},
   "source": [
    "CLIP clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b1f66-2810-4e10-a66f-1e5e8ef58010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clu = np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bf01e-46bb-4aaa-ae59-260bfc32baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [06:22<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" CLIP clustering acc \"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_idx_img = []\n",
    "modelf = modelf.to(args.device)\n",
    "modelf.eval()\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    for batch in loader_f:\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "        all_features.append(features.detach().cpu())\n",
    "        all_labels.append(label_clu)\n",
    "        all_idx_img.append(idx_img)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "all_features = torch.cat(all_features, dim=0).float().numpy()\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_idx_img = torch.cat(all_idx_img, dim=0)\n",
    "\n",
    "# cluster_acc(pred_clu, all_labels.numpy())\n",
    "\n",
    "# clip_store = torch.load(f'./cache/clip_store-{args.dataset_name}.pth')\n",
    "# all_gt_label_clu = clip_store['all_gt_label_clu']\n",
    "# all_features = clip_store['all_features']\n",
    "\n",
    "# kmeans = KMeans(n_clusters=len(all_gt_label_clu.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "kmeans = KMeans(n_clusters=5000, n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_predict(all_features)\n",
    "kmeans = MiniBatchKMeans(n_clusters=5000, n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_transform(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e9cb5-a4fb-47b0-959e-a023164cc2b8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=5000, n_init=20, max_iter=1000, random_state=43, verbose=2, batch_size=20000)\n",
    "pred_clu = kmeans.fit_predict(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d553326-8716-4ad4-b895-c507c99fb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu-{args.dataset_name}-train-clip-C=5k.npy', pred_clu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32176f36-cfdd-4d52-8a77-e4f24082da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### overclustering\n",
    "from sklearn.cluster import KMeans\n",
    "clip_store = torch.load(f'./cache/clip_store-{args.dataset_name}.pth')\n",
    "all_gt_label_clu = clip_store['all_gt_label_clu']\n",
    "all_features = clip_store['all_features']\n",
    "alpha_over_clustering = 10\n",
    "kmeans = KMeans(n_clusters=alpha_over_clustering*len(all_gt_label_clu.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_predict(all_features)\n",
    "np.save(f'./pred_clu-{args.dataset_name}-train-clip-over_{alpha_over_clustering}.npy', pred_clu)\n",
    "\n",
    "clip_store = torch.load(f'./cache/clip_store-{args.dataset_name}.pth')\n",
    "all_gt_label_clu = clip_store['all_gt_label_clu']\n",
    "all_features = clip_store['all_features']\n",
    "alpha_over_clustering = 5\n",
    "kmeans = KMeans(n_clusters=alpha_over_clustering*len(all_gt_label_clu.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_predict(all_features)\n",
    "np.save(f'./pred_clu-{args.dataset_name}-train-clip-over_{alpha_over_clustering}.npy', pred_clu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f765c-949b-4807-b2a9-12c0e3565e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "006f4feb-e99d-4d4c-aa7c-5c4a801f030c",
   "metadata": {},
   "source": [
    "Cluster navigation (depends on clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de786f98-6bb8-4643-b070-de7187cd46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_labels = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cdc6a-707c-45cf-8bda-d76f80bed2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "### per predicted-cluster voting\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "\n",
    "pred_kmeans_t = pred_kmeans\n",
    "# for it in range(10):\n",
    "#     print(f'iteration {it}')\n",
    "\n",
    "# cluster agg\n",
    "all_clu_pred = []\n",
    "for i in range(len(all_gt_voc.unique())):\n",
    "    selected = (pred_kmeans==i)\n",
    "    clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "    all_clu_pred.append(clu_pred)\n",
    "all_clu_pred = torch.stack(all_clu_pred, dim=0)\n",
    "\n",
    "# linear assignment\n",
    "print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "\n",
    "cost_mat = all_clu_pred.cpu().numpy()\n",
    "res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "label_kmeans_voc = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "\n",
    "print('instance label acc::', (label_kmeans_voc==all_gt_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4979ece-f872-406c-ad93-72b2970f0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_labels = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f56c2b-1baf-434d-a4fe-7a35fc495272",
   "metadata": {},
   "outputs": [],
   "source": [
    "### subset vocab\n",
    "col_subset = all_clu_pred.nonzero()[:, 1]\n",
    "col_subset = col_subset.unique().sort().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788bf5fc-fcbb-49df-a2c0-a6155255e9c3",
   "metadata": {},
   "source": [
    "KNN investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52721295-8e5c-4baf-9e70-0434168d1867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neighborhood_size = np.arange(5, 500, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    topk_res = similarity.topk(k=K+1)\n",
    "    topk_ind = topk_res.indices[:, 1:]\n",
    "    topk_match = torch.gather(label_match, 1, topk_ind)\n",
    "    topk_acc = topk_match.float().mean(dim=-1).mean()\n",
    "    print(f'K={K} acc={topk_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519d41-07c4-44e5-b649-5b170e3e1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.graph import compute_consensus_on_features\n",
    "neighborhood_size = np.arange(5, 50, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    _, _, pred_affinity = compute_consensus_on_features(all_features, k=K+1, q=0.8)\n",
    "    acc = ((pred_affinity & label_match).float().sum(1)/(pred_affinity.float().sum(1)+1e-10)).mean()\n",
    "    n_nn = pred_affinity.float().sum(1).mean()\n",
    "    print(f'K={K} acc={acc} n_nn={n_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970ede6-7bd9-408d-a903-30200acf6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_size = np.arange(5, 50, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    _, _, pred_affinity = compute_consensus_on_features(all_features, k=K+1, q=0.5)\n",
    "    acc = ((pred_affinity & label_match).float().sum(1)/(pred_affinity.float().sum(1)+1e-10)).mean()\n",
    "    n_nn = pred_affinity.float().sum(1).mean()\n",
    "    print(f'K={K} acc={acc} n_nn={n_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a399a6e-531d-41bd-b16e-c8b0b40f9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" KNN matrix output \"\"\"\n",
    "neighborhood_size = 315\n",
    "similarity = all_features@all_features.T\n",
    "label_match = (all_labels.view(-1, 1)==all_labels.view(1, -1))\n",
    "K = neighborhood_size\n",
    "topk_res = similarity.topk(k=K+1)\n",
    "topk_ind = topk_res.indices\n",
    "\n",
    "torch.save(topk_ind, f'./cache/{args.dataset_name}-clip-knn-{neighborhood_size}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcd275-c353-44f0-b13c-e47a1729d4c6",
   "metadata": {},
   "source": [
    "### SCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0a1f64-e602-447c-9b56-045118530fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sheng/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b73b47b-eb67-48ce-9d9e-b0fc1c8c17b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603352d-d18b-4722-a319-c3589a93ac6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" topk prediction from CLIP \"\"\"\n",
    "# classifier = get_classifier(args)\n",
    "# use_norm = True\n",
    "# amp_autocast = torch.cuda.amp.autocast\n",
    "# classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "# # initial topK prediction from CLIP\n",
    "# prob_k = 5\n",
    "# all_topk_voc = []\n",
    "# all_gt_voc = []\n",
    "# all_prob = []\n",
    "# all_label_clu = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch\n",
    "#         images = images.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "#                 similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "#                 prob = similarity.softmax(-1)\n",
    "#                 prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "#                 pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "#                                                                 device=args.device), 1, prob_topk_ind, 1)\n",
    "#                 # all_prob.append(prob.cpu())\n",
    "#                 all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "#                 all_gt_voc.append(label_voc)\n",
    "#                 all_label_clu.append(label_clu)\n",
    "#         pbar.update(1)\n",
    "\n",
    "# # all_prob = torch.cat(all_prob, dim=0)\n",
    "# all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "# all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "# all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "# # cluster agg\n",
    "# def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc):\n",
    "#     all_clu_pred = []\n",
    "#     for i in range(args.nb_classes):\n",
    "#         selected = (pred_kmeans==i)\n",
    "#         clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "#         all_clu_pred.append(clu_pred)\n",
    "#     all_clu_pred = torch.stack(all_clu_pred, dim=0)\n",
    "#     print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "#     print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "#     return all_clu_pred\n",
    "\n",
    "# def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc):\n",
    "#     cost_mat = all_clu_pred.cpu().numpy()\n",
    "#     res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "#     label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "#     print('instance label acc::', (label_voc_kmeans==all_gt_voc).float().mean().item())\n",
    "#     return label_voc_kmeans, res_ass\n",
    "\n",
    "# def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args, all_prob=None):\n",
    "#     label_voc_kmeans = label_voc_kmeans.to(args.device)\n",
    "#     if all_prob is None:\n",
    "#         cluster_ind = []\n",
    "#         with tqdm(total=len(loader_f)) as pbar:\n",
    "#             model.eval()\n",
    "#             for idx_batch, batch in enumerate(loader_f):\n",
    "#                 images, label_voc, label_clu, idx_img = batch\n",
    "#                 images = images.to(args.device)\n",
    "#                 with amp_autocast():\n",
    "#                     with torch.no_grad():\n",
    "#                         logits = model.visual(images)\n",
    "#                         logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "#                         similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "#                         prob = similarity.softmax(-1)\n",
    "#                         cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "#                 pbar.update(1)\n",
    "#         cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "#     else:\n",
    "#         all_prob = all_prob[:, label_voc_kmeans]\n",
    "#         cluster_ind = all_prob.argmax(dim=-1)\n",
    "#     mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "#     cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "#     return cluster_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ca3405-7460-4d17-9208-dcad1edcc3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count = selected.sum().item()\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected]).ravel(), return_counts=True)\n",
    "        counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    print('instance label acc::', (label_voc_kmeans==all_gt_voc).float().mean().item())\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\" [revised at 2023-02-17 12 AM]\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        \n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)[:, label_voc_kmeans].argmax(dim=-1)\n",
    "                            cluster_ind.append(prob.cpu())\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    # if classifier_selected is not None:\n",
    "    #     print(cluster_ind.shape, classifier_selected.shape, cluster_ind.max())\n",
    "    #     cluster_ind_voc = classifier_selected.index_select(0, cluster_ind)\n",
    "    # else:\n",
    "    cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b3caf2-b17b-4590-8dfe-ec8e6a0a5695",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [06:24<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg_by_pred_cluster\n",
      "is mutex assignment:: False\n",
      "assignment collision num:: 17\n",
      "linear_assign\n",
      "assignment shape=(260, 67174)\n",
      "instance label acc:: 0.29067155718803406\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [09:23<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 149\n",
      "cluster acc 0.696413099982672\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(260, 67174)\n",
      "instance label acc:: 0.3429902195930481\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [09:22<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 150\n",
      "cluster acc 0.7016144933944395\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(260, 67174)\n",
      "instance label acc:: 0.34697267413139343\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [09:27<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 150\n",
      "cluster acc 0.7015876050884625\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(260, 67174)\n",
      "instance label acc:: 0.34697866439819336\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [09:26<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 150\n",
      "cluster acc 0.7015876050884625\n",
      "agg_by_pred_cluster\n",
      "is mutex assignment:: True\n",
      "assignment collision num:: 0\n",
      "linear_assign\n",
      "assignment shape=(260, 67174)\n",
      "instance label acc:: 0.34697866439819336\n",
      "reassign_by_pred_cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [09:25<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing label:: 150\n",
      "cluster acc 0.7015876050884625\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "# pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino-dino_stage1.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-clip.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(5):\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a9841fa-ea1c-4611-a2b2-917de4161bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'all_clu_pred': all_clu_pred,\n",
    "    'label_voc_kmeans': label_voc_kmeans,\n",
    "    'pred_kmeans_t': pred_kmeans_t,\n",
    "    'cluster_ind_voc': cluster_ind_voc,\n",
    "    'record_pred_kmeans_t': record_pred_kmeans_t,\n",
    "    'all_gt_voc': all_gt_voc,\n",
    "    'all_label_clu': all_label_clu,\n",
    "}, f'./cache-{args.dataset_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5c141-2170-475b-b4b5-b08d7e723cac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = torch.load(f'./cache-{args.dataset_name}.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387260b5-a0c9-4894-a212-67f7148a4af4",
   "metadata": {},
   "source": [
    "### improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf8bea-46f8-4957-9b49-4f36d34249e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f971261-a759-4f8f-aabf-9475d786c1d8",
   "metadata": {},
   "source": [
    "#### substructure ensemblingpred_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c78df095-2862-47f6-8229-58cb609f803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_knn_info = torch.load(f'./cache/ssl_knn_info-{args.dataset_name}.pth')\n",
    "sim_top100 = ssl_knn_info['sim_top100']\n",
    "sim_top100_val = ssl_knn_info['sim_top100_val']\n",
    "label_sim_top100 = ssl_knn_info['label_sim_top100']\n",
    "label_match_sim_top100 = ssl_knn_info['label_match_sim_top100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08138939-efce-459a-94dd-b50d463a90e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [02:29<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_img_features = []\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_topk_voc_10 = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                \n",
    "                all_img_features.append(logits.cpu().numpy())\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_topk_voc_10.append(prob.topk(k=10, dim=-1).indices.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "all_img_features = np.concatenate(all_img_features)\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_topk_voc_10 = np.concatenate(all_topk_voc_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6708e-a0e0-4322-b935-b15005d84fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_method = ['ensembling', 'ensembling-threshold'][1]\n",
    "\n",
    "if knn_method == 'ensembling':\n",
    "    def row_unique_topk(r, k=5):\n",
    "        ind, count = np.unique(r, return_counts=True)\n",
    "        return ind[count.argsort()[-k:]]\n",
    "    refined_all_topk_voc = \\\n",
    "        np.apply_along_axis(\n",
    "            func1d=lambda x: row_unique_topk(x), \n",
    "            axis=1, \n",
    "            arr=np.take(all_topk_voc, sim_top100.numpy(), 0).reshape(-1, 100*5),\n",
    "        )\n",
    "elif knn_method == 'ensembling-threshold':\n",
    "    def row_unique_topk(r, knn_mask, k=5):\n",
    "        r = r[knn_mask]\n",
    "        ind, count = np.unique(r, return_counts=True)\n",
    "        return ind[count.argsort()[-k:]]\n",
    "    th_1 = 0.5\n",
    "    nn = 100\n",
    "    knn_mask = (torch.from_numpy(sim_top100_val)[:, :nn]>th_1)#.unsqueeze(-1).repeat(1,1,5).flatten(1)\n",
    "    knn_mask = knn_mask.unsqueeze(-1).repeat(1,1,5).flatten(1)\n",
    "    indexed_topk_voc = np.take(all_topk_voc, sim_top100[:, :nn], axis=0).reshape(-1, nn*5)\n",
    "    refined_all_topk_voc = np.stack([ row_unique_topk(r=indexed_topk_voc[i, :], knn_mask=knn_mask[i, :])\n",
    "                                     for i in range(indexed_topk_voc.shape[0]) ], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b843b-9e2a-444f-81cc-6d2f669554db",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino-dino_stage1.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(5):\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), refined_all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e19ff45-2faf-4660-b239-28f8a4d0f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_confusion = (classifier[all_gt_voc]@classifier.t()).topk(k=2).values[:, 1].cpu().numpy()\n",
    "sns.distplot(sim_confusion, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61984ad2-0b83-4996-9672-920f7fecffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sim_confusion = (classifier[all_gt_voc]@classifier.t()).topk(k=2).values[:, 1].cpu().numpy()\n",
    "sns.distplot(sim_confusion, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8d8542e-2eba-4e92-9f56-a514e7c6da59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9835119"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_confusion.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb77bc74-7e9b-4f55-a31c-7f0e37872234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786451"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_confusion.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9019cc82-7583-4c34-8ec7-845bd6ac85aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4423), tensor(0.2617))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.from_numpy(refined_all_topk_voc)[:, 4]==all_gt_voc).float().mean(), (torch.from_numpy(all_topk_voc)[:, 0]==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2361a5-3e99-49de-853e-79449af3417c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### adaptive weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef42873-e025-43cf-82a3-0c462c826b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute @missing on class-wise agg prediction @all_clu_pred \"\"\"\n",
    "classwise_topk_ind = all_clu_pred.topk(k=5).indices\n",
    "for j in range(classwise_topk_ind.size(-1)):\n",
    "    pred_label_set = set(classwise_topk_ind[:, :j+1].flatten().unique().cpu().numpy())\n",
    "    print(f'j={j} missing={len(set_gt - pred_label_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3cd12-30f4-4166-b525-3ab37d5f205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(label_voc_kmeans==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0bfd4c-761a-4392-a151-cb60a7f8ad1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_class = all_clu_pred.topk(k=5).values\n",
    "### 0-1 normalization\n",
    "weight_class = (val_class - val_class.min())/(val_class.max() - val_class.min()) - 0.5\n",
    "weight_class = weight_class.softmax(dim=-1)#/all_clu_pred.topk(k=5).values.softmax(dim=-1)[:, 0].view(-1, 1)\n",
    "ind_class = all_clu_pred.topk(k=5).indices\n",
    "\n",
    "### class pred to instance pred\n",
    "instance_pred = torch.zeros([pred_kmeans_t.size(0), 5]).long()\n",
    "instance_weight = torch.zeros([pred_kmeans_t.size(0), 5])\n",
    "for c in pred_kmeans_t.unique():\n",
    "    selected = (pred_kmeans_t==c)\n",
    "    instance_pred[selected] = ind_class[c].view(-1, 5)\n",
    "    instance_weight[selected] = weight_class[c].view(-1, 5)\n",
    "\n",
    "weight_normalize = lambda x: x/x[:, 0].view(-1, 1)\n",
    "\n",
    "idx = 3\n",
    "weight_normalize(instance_weight)[(instance_pred[:, 0]==all_gt_voc)][:, idx].mean(), \\\n",
    "weight_normalize(instance_weight)[(instance_pred[:, idx]==all_gt_voc)][:, idx].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29748d25-401e-46d8-8bec-7dadbb051999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0782)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(instance_pred[:, 0]==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6086c-c4f7-40aa-adf7-8456702d768c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" visual reranking computation based on spatial features \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "instance_topk_voc = torch.gather(all_clu_pred, 0, pred_kmeans_t.view(-1, 1))\n",
    "all_spatial_label_pred = []\n",
    "all_label_voc = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images, return_spatial=True)\n",
    "                features = features @ model.visual.proj.unsqueeze(0)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "                spatial_similarity = model.logit_scale.exp() * (features @ classifier[instance_topk_voc[idx_img]].permute(0,2,1))\n",
    "                spatial_label_pred = spatial_similarity.topk(k=10, dim=1).values.mean(dim=1).argmax(dim=-1)\n",
    "                all_spatial_label_pred.append(spatial_label_pred.cpu())\n",
    "                all_label_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_spatial_label_pred = torch.cat(all_spatial_label_pred, dim=0)\n",
    "all_label_voc = torch.cat(all_label_voc, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e6634-c549-416e-89ff-34041414068b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" SCD with shrinked vocab \"\"\"\n",
    "# classifier_selected = all_clu_pred.topk(k=5).indices.flatten().unique()\n",
    "# classifier = get_classifier(args)\n",
    "# classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "# args.num_voc = classifier.size(0)\n",
    "# amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "# ### collect variables\n",
    "# prob_k = 5\n",
    "# all_topk_voc = []\n",
    "# all_gt_voc = []\n",
    "# all_label_clu = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     if hasattr(model, 'eval'):\n",
    "#         model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "#         images = images.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "#                 prob = similarity.softmax(-1)\n",
    "#                 prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "#                 ### mapping @selected to vocab ind\n",
    "#                 B, C = prob_topk_ind.shape\n",
    "#                 prob_topk_ind = classifier_selected[prob_topk_ind.view(-1)].view(B, C)\n",
    "#                 all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "#                 all_gt_voc.append(label_voc)\n",
    "#                 all_label_clu.append(label_clu)\n",
    "#         pbar.update(1)\n",
    "\n",
    "# all_topk_voc = np.concatenate(all_topk_voc)\n",
    "# all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "# all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "# pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino-dino_stage1.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None, classifier_selected=classifier_selected)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17d409fc-0177-4e54-b270-7f7d0f452870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_all_clu_pred = all_clu_pred\n",
    "# len(set_gt - set(final_all_clu_pred.topk(k=2).indices.flatten().unique().numpy().tolist()))\n",
    "\n",
    "# len(set_gt - set(all_clu_pred.topk(k=3).indices.flatten().unique().numpy().tolist()))\n",
    "\n",
    "# select_correct = (cluster_ind_voc.cpu()==all_gt_voc)\n",
    "\n",
    "# all_topk_val = torch.from_numpy(all_topk_val)#[select_correct]\n",
    "# prob_all_topk_val = torch.cat([all_topk_val, 1-all_topk_val.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "\n",
    "# ent = - (prob_all_topk_val * (prob_all_topk_val+1e-30).log()).sum(dim=-1)\n",
    "\n",
    "# # import seaborn as sns\n",
    "# # sns.distplot(prob_all_topk_val[select_correct, 0], bins=100)\n",
    "# # sns.distplot(prob_all_topk_val[~select_correct, 0], bins=100)\n",
    "# # sns.scatterplot(x=prob_all_topk_val[:, 0], y=select_correct.float(), s=3, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5c119e-54f4-4a5d-b6c7-2a2feafb8318",
   "metadata": {},
   "source": [
    "collect variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfb1e2-f2da-4787-b872-d08922d43bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_topk_val = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                topk_res = prob.topk(k=prob_k, dim=-1)\n",
    "                prob_topk_ind = topk_res.indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_topk_val.append(topk_res.values.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_topk_val = np.concatenate(all_topk_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc944ec3-21e1-4577-a721-d969514552e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifier = get_classifier(args)\n",
    "# classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "# args.num_voc = classifier.size(0)\n",
    "# amp_autocast = torch.cuda.amp.autocast\n",
    "# ### collect variables\n",
    "# prob_k = 5\n",
    "# all_topk_voc = []\n",
    "# all_gt_voc = []\n",
    "# all_label_clu = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     if hasattr(model, 'eval'):\n",
    "#         model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "#         images = images.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = 100 * logits @ classifier.t()\n",
    "#                 prob = similarity.softmax(-1)\n",
    "#                 prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "#                 all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "#                 all_gt_voc.append(label_voc)\n",
    "#                 all_label_clu.append(label_clu)\n",
    "#         pbar.update(1)\n",
    "\n",
    "# all_topk_voc = np.concatenate(all_topk_voc)\n",
    "# all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "# all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "use_confidence = True\n",
    "th_confidence = 0.5\n",
    "pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "\n",
    "if use_confidence:\n",
    "    # ### SSL feature extraction\n",
    "    # ssl_prototypes = torch.zeros([pred_kmeans.unique().size(0), 768], device=args.device, dtype=torch.float64) ### C x D\n",
    "    # ssl_counter = torch.zeros(pred_kmeans.unique().size(0))\n",
    "    # with tqdm(total=len(loader_f)) as pbar:\n",
    "    #     modelf.eval()\n",
    "    #     modelf.to(args.device)\n",
    "    #     for idx_batch, batch in enumerate(loader_f):\n",
    "    #         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "    #         images = images.to(args.device)\n",
    "    #         with torch.no_grad():\n",
    "    #             features = modelf(images.float())\n",
    "    #             features = F.normalize(features, dim=-1)\n",
    "    #             for p in range(idx_img.size(0)):\n",
    "    #                 ssl_prototypes[pred_kmeans[p].long()] += features.to(torch.float64)[p]\n",
    "    #             # ssl_prototypes = torch.scatter_add(ssl_prototypes, 0, pred_kmeans[idx_img.long()].to(args.device).long(), features.to(torch.float64))\n",
    "    #             counter_voc_ind, counter_val = pred_kmeans[idx_img].unique(return_counts=True)\n",
    "    #             ssl_counter[counter_voc_ind.long()] += counter_val\n",
    "    #         pbar.update(1)\n",
    "    # ssl_prototypes = ssl_prototypes/ssl_counter.to(args.device).unsqueeze(-1)\n",
    "    # ssl_prototypes = F.normalize(ssl_prototypes, dim=-1)\n",
    "    # ### select confident instances\n",
    "    # all_prob = []\n",
    "    # all_sim = []\n",
    "    # with tqdm(total=len(loader_f)) as pbar:\n",
    "    #     modelf.eval()\n",
    "    #     modelf.to(args.device)\n",
    "    #     for idx_batch, batch in enumerate(loader_f):\n",
    "    #         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "    #         images = images.to(args.device)\n",
    "    #         with torch.no_grad():\n",
    "    #             features = modelf(images)\n",
    "    #             features = F.normalize(features, dim=-1)\n",
    "    #             sim = features@ssl_prototypes.float().t()\n",
    "    #             prob = (sim/1.0).amax(dim=-1)\n",
    "    #             all_prob.append(prob.cpu())\n",
    "    #             all_sim.append(sim.cpu())\n",
    "    #         pbar.update(1)\n",
    "    # all_prob = torch.cat(all_prob, dim=0)\n",
    "    # all_sim = torch.cat(all_sim, dim=0)\n",
    "    ### confidence thresholding\n",
    "    q = np.quantile(all_prob.numpy(), q=0.5)\n",
    "    selected = (all_prob>q)\n",
    "    ### computing\n",
    "    pred_kmeans_t = pred_kmeans[selected]\n",
    "    for t in range(3):\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc[selected], voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc[selected])\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, \n",
    "                                                                  all_prob=None, instance_selected=selected)\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu[selected].numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "else:\n",
    "    pred_kmeans_t = pred_kmeans\n",
    "    for t in range(3):\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cac48-7b09-47ac-b5f6-d67a19966a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" inspect cluster topk assigned classes \"\"\"\n",
    "# topk_cluster_label = all_clu_pred.topk(k=5).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc40ee62-0996-46bf-8e3b-40d9b0fc1997",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pred_kmeans_t = pred_kmeans\n",
    "# # for t in range(5):\n",
    "# #     all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t, all_topk_voc)\n",
    "# #     label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "# #     pred_kmeans_t = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args, all_prob=None)\n",
    "# #     set_pred = set(res_ass[1].tolist())\n",
    "# #     set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "# #     print('missing label::', len(set_gt - set_pred))\n",
    "# #     print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "\n",
    "\n",
    "# \"\"\" get confident prediction \"\"\"\n",
    "# th = 0.5\n",
    "# amp_autocast = torch.cuda.amp.autocast\n",
    "# label_voc_kmeans_t = label_voc_kmeans_t.to(args.device)\n",
    "# cluster_ind = []\n",
    "# selected_ind = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     if hasattr(model, 'eval'):\n",
    "#         model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "#         images = images.to(device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = 100 * logits @ classifier.t()\n",
    "#                 prob = similarity[:, label_voc_kmeans_t].softmax(dim=-1)\n",
    "#                 selected = (prob.amax(dim=-1)>th)\n",
    "#                 selected_ind.append(selected.cpu())\n",
    "#         pbar.update(1)\n",
    "# selected_ind = torch.cat(selected_ind, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# # precision = cluster_acc(y_true=all_label_clu[selected_ind].numpy(), y_pred=pred_kmeans_t[selected_ind].numpy())\n",
    "# # recall = selected_ind.mean()\n",
    "# print(f'confidence selection precision={precision} recall={recall}')\n",
    "\n",
    "# # np.save(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688f3cb-d754-4c42-bf39-242029dbd35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" \n",
    "# 1. inverse entropy of prototype\n",
    "\n",
    "# 2. top1 sim of proto-image\n",
    "\n",
    "# 3. top1 sim of image-proto\n",
    "\n",
    "# \"\"\"\n",
    "# # candidate_ind = res_ass[1].unique()\n",
    "# # cls_proto_similarity = torch.zeros([len(dataset_f), candidate_ind.size()])\n",
    "# all_sim_proto_image_pred = []\n",
    "# all_sim_proto_image_gt = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch\n",
    "#         images = images.to(args.device)\n",
    "#         label_voc = label_voc.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "#                 prob = similarity.softmax(dim=-1)\n",
    "#                 all_sim_proto_image_pred.append(similarity[:, prob.argmax(dim=-1)].cpu())\n",
    "#                 all_sim_proto_image_gt.append(similarity[:, label_voc].cpu())\n",
    "#         pbar.update(1)\n",
    "        \n",
    "# all_sim_proto_image_pred = torch.cat(all_sim_proto_image_pred, dim=0)\n",
    "# all_sim_proto_image_gt = torch.cat(all_sim_proto_image_gt, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05625353-4773-4ed7-951c-d8974dfb3b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_sim_proto_image_pred = torch.cat(all_sim_proto_image_pred, dim=0)\n",
    "# all_sim_proto_image_gt = torch.cat(all_sim_proto_image_gt, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# label_match = all_label_clu.view(-1, 1)@all_label_clu.view(1, -1)\n",
    "# pred_match_init = pred_kmeans.view(-1, 1)@pred_kmeans.view(1, -1)\n",
    "# pred_match = pred_kmeans_t.view(-1, 1)@pred_kmeans_t.view(1, -1)\n",
    "\n",
    "# pred_consensus = (pred_match_init==pred_match) \n",
    "# ((pred_consensus & label_match).float().sum(dim=-1) / (pred_consensus.sum(dim=-1)+1e-20)).mean()\n",
    "\n",
    "# (pred_consensus & label_match).float().sum(dim=-1).bool().float().mean()\n",
    "\n",
    "# all_clu_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c306e3-a6a3-4731-84d6-cd0e5d845ea4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde93f02-13a3-498c-ba18-67de692b8e3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))\n",
    "    \n",
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d793092-23ab-497e-9759-714c91e55e51",
   "metadata": {},
   "source": [
    "collect variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3641aa10-d6ea-4434-b4b3-57058a9e1f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac099527-5211-4e10-a50f-1ebc926834c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "# initial topK prediction from CLIP\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_prob = []\n",
    "all_max_ind = []\n",
    "all_topk_vocinds = []\n",
    "all_label_clu = []\n",
    "all_topk_vals = []\n",
    "all_topk_inds = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                # all_prob.append(prob.cpu())\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_max_ind.append(prob.argmax(dim=-1).cpu())\n",
    "                all_topk_vocinds.append(prob.topk(k=10, dim=-1).indices.cpu())\n",
    "                \n",
    "                batch_topk_res = prob.topk(k=20, dim=-1)\n",
    "                all_topk_vals.append(batch_topk_res.values.cpu())\n",
    "                all_topk_inds.append(batch_topk_res.indices.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "# all_prob = torch.cat(all_prob, dim=0)\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_max_ind = torch.cat(all_max_ind, dim=0)\n",
    "all_topk_vocinds = torch.cat(all_topk_vocinds, dim=0)\n",
    "all_topk_vals = torch.cat(all_topk_vals, dim=0)\n",
    "all_topk_inds = torch.cat(all_topk_inds, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b4a31-0d72-45d8-bcdd-97c69944fecb",
   "metadata": {},
   "source": [
    "text proto inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159430a-4362-40b9-9181-1c08c1de63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN proto analysis\n",
    "text_sim = classifier[all_gt_voc.unique(), :]@classifier.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5c99a-edbd-485e-9082-ab819e27db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_topk = text_sim.topk(k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a41ba7-f29d-4697-ae19-399d222cc970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(np.array([vocab.mapping_idx_names[t.item()] for t in text_topk.indices[:, :].flatten().cpu()]).reshape(text_sim.size(0), -1).tolist(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f20703-1fc0-4791-9242-591d6a7b10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.distplot(all_topk_vals[:, 0].cpu().numpy(), bins=200)\n",
    "sns.distplot(all_topk_vals[:, 1].cpu().numpy(), bins=200)\n",
    "sns.distplot(all_topk_vals[:, 2].cpu().numpy(), bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075cda2-8944-48de-b2f5-8e9517de5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = (all_topk_vals[:, 0]>0.7)\n",
    "'top1 acc', (all_gt_voc[selected] == all_max_ind[selected]).float().mean(), \\\n",
    "'selected percentile', selected.float().mean(), \\\n",
    "'class diversity', len(set(all_gt_voc.unique().numpy()) - set(all_max_ind[selected].unique().numpy())), \\\n",
    "'topk inclusion', torch.stack([all_topk_vocinds[selected, i]==all_gt_voc[selected] for i in range(all_topk_vocinds.size(1))], dim=1).float().sum(dim=-1).bool().float().mean(), \\\n",
    "'selected sample pred voc size', len(all_max_ind[selected].unique()), \\\n",
    "'average selected instance number per class', selected.sum()/len(all_gt_voc.unique()),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888780d-22f0-47f2-a216-d5b78a5dbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e444ff3-7837-4e21-a5eb-d21c6111fd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" initial clip assignment \"\"\"\n",
    "list(filter(lambda x: x[1]<100, [(i.item(), (all_max_ind==i).sum().item()) for i in all_gt_voc.unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15937caa-ab71-4efd-b334-634f64485d73",
   "metadata": {},
   "source": [
    "text to image entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9165942-c523-4821-9ddf-da7e57c3e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "### get all label and predicted label\n",
    "all_label_voc = []\n",
    "all_pred_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                pred_voc = prob.argmax(dim=-1)\n",
    "        all_label_voc.append(label_voc)\n",
    "        all_pred_voc.append(pred_voc.cpu())\n",
    "        all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "all_label_voc = torch.cat(all_label_voc, dim=0)\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "                \n",
    "### compute entropy\n",
    "set_all_label_voc = all_label_voc.unique()\n",
    "set_all_pred_voc = all_pred_voc.unique()\n",
    "selected_classifier_ind = torch.cat([set_all_label_voc, set_all_pred_voc], dim=0).unique()\n",
    "all_similarity = []\n",
    "all_selected_sim = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        label_voc = label_voc.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                all_selected_sim.append(similarity[:, selected_classifier_ind].cpu())\n",
    "                all_similarity.append(similarity.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "        \n",
    "all_selected_sim = torch.cat(all_selected_sim, dim=0)\n",
    "all_similarity = np.concatenate(all_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ded316-eb1c-43f6-93a0-b65682197242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans_t = torch.from_numpy(np.load(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy'))\n",
    "classwise_all_selected_sim = []\n",
    "for c in pred_kmeans_t.unique():\n",
    "    subset = (pred_kmeans_t==c)\n",
    "    classwise_all_selected_sim.append(all_selected_sim[subset, :].mean(dim=0).cpu())\n",
    "classwise_all_selected_sim = torch.stack(classwise_all_selected_sim, dim=0)\n",
    "p = classwise_all_selected_sim.float().softmax(dim=0)\n",
    "ent = (-p*(p+1e-10).log2()).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446d82a-1e89-4e68-a08f-2924614cc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "softmax(all_similarity, axis=0)\n",
    "pred_kmeans_t = torch.from_numpy(np.load(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy'))\n",
    "classwise_all_sim = []\n",
    "for c in pred_kmeans_t.unique():\n",
    "    subset = (pred_kmeans_t==c)\n",
    "    classwise_all_sim.append(all_selected_sim[subset, :].mean(dim=0).cpu())\n",
    "classwise_all_sim = torch.stack(classwise_all_sim, dim=0)\n",
    "p_all = classwise_all_sim.float().softmax(dim=0)\n",
    "ent_all = (-p_all*(p_all+1e-10).log2()).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056f95cb-d240-4f8e-bff3-89d1ee2b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = torch.nan_to_num(ent)\n",
    "ent_gt = ent[torch.isin(selected_classifier_ind, set_all_label_voc)]\n",
    "ent_pred = ent[torch.isin(selected_classifier_ind, set_all_pred_voc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2bd40-54aa-4cd3-8b70-9818908b212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=128)\n",
    "sns.distplot(ent_gt.numpy(), bins=100)\n",
    "sns.distplot(ent_pred.numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b5420f-f0cd-4283-b407-23b711156895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14), tensor(9), tensor(3176), tensor(4867))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ent_gt>2).sum(), (ent_gt>3).sum(), \\\n",
    "(ent_pred<2).sum(), (ent_pred<3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2423501f-0554-4cfc-8457-5d5b2deb434c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14), tensor(9), tensor(3176), tensor(4867))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ent_gt>2).sum(), (ent_gt>3).sum(), \\\n",
    "(ent_pred<2).sum(), (ent_pred<3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abb5cf-b0ee-4e10-a740-b506dc495449",
   "metadata": {},
   "source": [
    "### Classifier Distinguishement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fc225fb-d00e-4edc-b5c5-5c2645e5e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load(f'./cache-{args.dataset_name}.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']\n",
    "\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b9fdbfc-eb0f-4a1b-91fa-6a75c81b757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordnet_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e965ea79-d879-4a45-90d9-39ee249cb3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cls = (classifier[all_gt_voc.unique(), :] @ classifier.t())\n",
    "sim_cls_topk = sim_cls.topk(k=10)\n",
    "sim_cls_topk_ind = sim_cls_topk.indices\n",
    "sim_cls_topk_val = sim_cls_topk.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5243b583-f26f-4119-8f2e-c85d39d1eb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9226, 0.9470, 0.8229, 0.9689, 0.9503, 0.9278, 0.9405, 0.9569, 0.8933,\n",
       "         0.9786, 0.8696, 0.8784, 0.8492, 0.9287, 0.9550, 0.9381, 0.9333, 0.9223,\n",
       "         0.9375, 0.8907, 0.9537, 0.9489, 0.9119, 0.8437, 0.9601, 0.8123, 0.9727,\n",
       "         0.7702, 0.9134, 0.9464, 0.8488, 0.7621, 0.7794, 0.9110, 0.8408, 0.8109,\n",
       "         0.8916, 0.8436, 0.8203, 0.8179, 0.9077, 0.9552, 0.8176, 0.9212, 0.9691,\n",
       "         0.8465, 0.8818, 0.8904, 0.9088, 0.8446, 0.9524, 0.8530, 0.7941, 0.8238,\n",
       "         0.9566, 0.7968, 0.9786, 0.9124, 0.9291, 0.8696, 0.9337, 0.9249, 0.9274,\n",
       "         0.9662, 0.9138, 0.9043, 0.8892, 0.9450, 0.8370, 0.9197, 0.9375, 0.9237,\n",
       "         0.9637, 0.9213, 0.8963, 0.9562, 0.9110, 0.8527, 0.9163, 0.9402, 0.9619,\n",
       "         0.9393, 0.9423, 0.9157, 0.9424, 0.8922, 0.9498, 0.8936, 0.9217, 0.8915,\n",
       "         0.9252, 0.9184, 0.9449, 0.8407, 0.9376, 0.9724, 0.9436, 0.9644, 0.9296],\n",
       "        device='cuda:1'),\n",
       " tensor([15240, 51703, 65422, 32021, 53004, 54715,  6397, 20858, 27740,  7485,\n",
       "          8907,  7694,  7752, 26650,  1223, 43438,  5973, 32538, 10563, 66034,\n",
       "         57945, 12444, 35247,  6492, 15597, 11279, 13570, 17521, 17241, 55501,\n",
       "         34043, 24548,  6114, 58093, 38578, 36599, 23415, 18738, 18111, 57301,\n",
       "          7040, 20878, 32545, 11992, 29101, 29516, 35352, 42218, 65556, 54507,\n",
       "          8321, 65422, 36599, 53398, 59190,  1774, 63917, 31995, 61582, 15596,\n",
       "          8765, 61900, 39850, 22350, 38342,  8557, 43274, 63250, 54473, 44576,\n",
       "         45375, 18059, 15703, 26868, 37410, 43417, 21197,  5833, 64016, 14913,\n",
       "         52342,  2382, 53388,  5163, 54780, 56914, 34640, 18738, 51641, 58779,\n",
       "          5380, 27608,  9356, 19760, 64037, 13373, 44372, 65530, 57065],\n",
       "        device='cuda:1'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_cls_topk_val[:, 1], sim_cls_topk_ind[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd23e819-befc-4f8e-b69f-6b75674ab8b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_names = [ mapping_vocidx_to_synsets(r.item(), vocab)[0] for r in sim_cls_topk_ind[:, 1] ]\n",
    "\n",
    "gt_voc_names = [ mapping_vocidx_to_synsets(r.item(), vocab)[0] for r in sim_cls_topk_ind[:, 0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15df7e9b-8eb6-46c3-9650-ceb507c877e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_synset_to_synset_names = lambda x: list(map(lambda y: y.name(), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6ff359d-3642-4ba5-99e1-714036e8d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_distance = np.array([int(1/tree_distance(x, y, 'path_similarity')) for (x, y) in zip(gt_voc_names, knn_names)])\n",
    "compare_names = np.array([[x, y] for (x, y) in zip(mapping_synset_to_synset_names(gt_voc_names), mapping_synset_to_synset_names(knn_names))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2aa2286-a2b6-43ad-ad22-ff1c3f876c32",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['agaric.n.02', 'royal_agaric.n.01'],\n",
       "       ['airedale.n.01', 'welsh_terrier.n.01'],\n",
       "       ['airliner.n.01', 'jetliner.n.01'],\n",
       "       ['backpack.n.01', 'schoolbag.n.01'],\n",
       "       ['bathing_cap.n.01', 'shower_cap.n.01'],\n",
       "       ['bittern.n.01', 'european_bittern.n.01'],\n",
       "       ['buckle.n.01', 'belt_buckle.n.01'],\n",
       "       ['canoe.n.01', 'kayak.n.01'],\n",
       "       ['crane.n.05', 'bird.n.01'],\n",
       "       ['custard_apple.n.02', 'cherimoya.n.02'],\n",
       "       ['desktop_computer.n.01', 'computer.n.01'],\n",
       "       ['dhole.n.01', 'dingo.n.01'],\n",
       "       ['entlebucher.n.01', 'bernese_mountain_dog.n.01'],\n",
       "       ['folding_chair.n.01', 'straight_chair.n.01'],\n",
       "       ['greater_swiss_mountain_dog.n.01', 'dog.n.01'],\n",
       "       ['hand_blower.n.01', 'blower.n.01'],\n",
       "       ['hare.n.01', 'european_hare.n.01'],\n",
       "       ['harmonica.n.01', 'kazoo.n.01'],\n",
       "       ['home_theater.n.01', 'cinema.n.02'],\n",
       "       ['jay.n.02', 'old_world_jay.n.01'],\n",
       "       ['killer_whale.n.01', 'whale.n.02'],\n",
       "       ['lacewing.n.01', 'brown_lacewing.n.01'],\n",
       "       ['lakeland_terrier.n.01', 'welsh_terrier.n.01'],\n",
       "       ['langur.n.01', 'mangabey.n.01'],\n",
       "       ['leatherback_turtle.n.01', 'sea_turtle.n.01'],\n",
       "       ['lorikeet.n.01', 'varied_lorikeet.n.01'],\n",
       "       ['measuring_cup.n.01', 'cup.n.01'],\n",
       "       ['minibus.n.01', 'bus.n.01'],\n",
       "       ['mud_turtle.n.01', 'musk_turtle.n.01'],\n",
       "       ['parallel_bars.n.01', 'uneven_parallel_bars.n.01'],\n",
       "       ['pekinese.n.01', 'shih-tzu.n.01'],\n",
       "       ['pillow.n.01', 'cushion.n.03'],\n",
       "       ['printer.n.03', 'page_printer.n.01'],\n",
       "       ['recreational_vehicle.n.01', 'vehicle.n.01'],\n",
       "       ['spotlight.n.02', 'light.n.02'],\n",
       "       ['stingray.n.01', 'roughtail_stingray.n.01'],\n",
       "       ['tick.n.02', 'hard_tick.n.01'],\n",
       "       ['tripod.n.01', 'camera_tripod.n.01'],\n",
       "       ['wallaby.n.01', 'common_wallaby.n.01'],\n",
       "       ['weevil.n.01', 'pea_weevil.n.01'],\n",
       "       ['whiptail.n.01', 'western_whiptail.n.01'],\n",
       "       ['wooden_spoon.n.02', 'spoon.n.01']], dtype='<U31')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_names[(path_distance<5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5b44745-2727-4e9f-9167-006798d947e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "sns.distplot(path_distance, bins=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(sim_cls_topk_val[:, 1].cpu().numpy(), bins=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
