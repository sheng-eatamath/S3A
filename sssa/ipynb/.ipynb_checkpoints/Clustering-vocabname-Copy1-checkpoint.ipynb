{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c187d77c-026e-4cf7-9000-103a8718cd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheng/anaconda3/envs/gcd/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "sys.path.append('/home/sheng/sssa/')\n",
    "# sys.path.append('/home/sheng/sssa/CLIP/')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "from typing import Union, List\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from functools import reduce, partial\n",
    "from itertools import zip_longest\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from wordnet_utils import *\n",
    "import scipy.io\n",
    "from PIL import Image\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from ipynb_utils import get_hier_datasets, get_classifier, MCMF_assign_labels\n",
    "# import clip\n",
    "import model as clip\n",
    "from data.datasets import build_transform, get_hier_datasets\n",
    "from data.vocab import get_vocab_with_classnames\n",
    "# from data.imagenet_datasets import get_datasets_oszsl\n",
    "from data.imagenet_datasets_namevocab import get_datasets_oszsl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184f5eb3-cd49-42e6-87b1-cb7e50d0d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    exp = 'classifier_3d'\n",
    "    vocabname = 'concat2' ### ['in21k', 'concat3', 'concat3+lvis']\n",
    "    \n",
    "    device = 'cuda:3'\n",
    "    arch = 'ViT-B/16'\n",
    "    \n",
    "    dataset = 'make_entity30'\n",
    "    n_sampled_classes = 100\n",
    "    input_size = 224\n",
    "    estimate_k = -1\n",
    "    \n",
    "    batch_size = 512\n",
    "    use_def = False\n",
    "    clip_checkpoint = None\n",
    "    # f_classifier = './cache/wordnet_classifier_in21k_word.pth'\n",
    "    f_classifier = './cache/classifier_3d-concat2.pth'\n",
    "    templates_name = 'templates_small'\n",
    "    seed = 0\n",
    "    \n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b6c3e7-a7cf-4d0b-b4f6-2c8f2ae6daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_templates(args):\n",
    "    with open(f'../{args.templates_name}.json', 'rb') as f:\n",
    "        templates = json.load(f)['imagenet']\n",
    "    return templates\n",
    "\n",
    "templates = load_templates(args)\n",
    "\n",
    "def load_clip(args):\n",
    "    model, preprocess = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model, preprocess\n",
    "\n",
    "def load_clip2(args):\n",
    "    model = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model.to(args.device).eval()\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size)\n",
    "    return model\n",
    "\n",
    "def load_mixture_clip(args, decay=1.0):\n",
    "    model1 = clip.load(args.arch)\n",
    "    if args.clip_checkpoint:\n",
    "        model1.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model_ema'].items()}, strict=False)\n",
    "    model1.to(args.device).eval()\n",
    "    model2 = clip.load(args.arch)\n",
    "    model2.to(args.device).eval()\n",
    "    with torch.no_grad():\n",
    "        msd = model1.state_dict()\n",
    "        for k, ema_v in model2.state_dict().items():\n",
    "            # if needs_module:\n",
    "            #     k = 'module.' + k\n",
    "            model_v = msd[k].detach()\n",
    "            ema_v.copy_(ema_v * decay + (1. - decay) * model_v)\n",
    "    return model2\n",
    "\n",
    "def topk_acc(all_pred_voc_topk, all_gt_voc):\n",
    "    acc = []\n",
    "    ### topK accuracy\n",
    "    for i in range(all_pred_voc_topk.size(1)):\n",
    "        vec = torch.zeros(all_pred_voc_topk.size(0)).bool()\n",
    "        for j in range(i+1):\n",
    "            vec |= (all_pred_voc_topk[:, j]==all_gt_voc)\n",
    "        print(f'k={i} acc={vec.float().mean()}')\n",
    "        acc.append(vec.float().mean().item())\n",
    "    return acc\n",
    "\n",
    "def semantic_acc(y_pred, y_true, metrics={}):\n",
    "    \"\"\" compute soft semantic acc for @y_pred and @y_true \"\"\"\n",
    "    assert len(metrics)>0\n",
    "    assert y_pred.size(0)==y_true.size(0)\n",
    "    scores = {m:[] for m in metrics.keys()}\n",
    "    with tqdm(total=y_pred.size(0)) as pbar:\n",
    "        for i in range(y_pred.size(0)):\n",
    "            syn_pred = mapping_vocidx_to_synsets(y_pred[i].item(), vocab)\n",
    "            syn_true = mapping_vocidx_to_synsets(y_true[i].item(), vocab)\n",
    "            pairs = list(itertools.product(range(len(syn_pred)), range(len(syn_true))))\n",
    "            for m_name, m in metrics.items():\n",
    "                scores[m_name].append( max([ m(syn_pred[p[0]], syn_true[p[1]]) for p in pairs ]) )\n",
    "            pbar.update(1)\n",
    "    for m_name in metrics.keys():\n",
    "        scores[m_name] = np.array(scores[m_name]).mean()\n",
    "    return scores\n",
    "    \n",
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2818f39d-9340-4999-857d-2b086c9b0ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_vocab concat2\n",
      "dataset size 307835\n",
      "missing keys:\n",
      "['visual.projection_head.0.weight', 'visual.projection_head.0.bias', 'visual.projection_head.2.weight', 'visual.projection_head.2.bias']\n",
      "Model parameters: 150,408,193\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "\"\"\" prepare dataset and load CLIP \"\"\"\n",
    "vocab = get_vocab_with_classnames(args.vocabname)\n",
    "\n",
    "transform_val = build_transform(is_train=False, args=args, train_config=None)\n",
    "dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_val, seed=0)\n",
    "loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "print('dataset size', len(dataset))\n",
    "\n",
    "# model, preprocess = load_clip(args)\n",
    "model = load_clip2(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499fb6d7-7d39-4a90-8c65-0af9c5c9374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602/602 [08:26<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "all_vfeatures = []\n",
    "all_clu_label = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual.extract_features(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                all_vfeatures.append(deepcopy(logits.cpu().numpy()))\n",
    "                all_clu_label.append(deepcopy(label_clu.numpy()))\n",
    "        pbar.update(1)\n",
    "\n",
    "all_vfeatures = np.concatenate(all_vfeatures)\n",
    "all_clu_label = np.concatenate(all_clu_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2312a8-a968-419e-91d9-e06238fee3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307835"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81986868-0008-4917-a1e4-0d6890a35ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281186"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a064680-71f5-46fe-a46d-4c846a6ef891",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./cache/features/vfeatures-{args.dataset}.npy', all_vfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0c3d403-474b-41c0-a098-9487fa8d524d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=240\n",
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from my_util_package_oszsl.evaluation import cluster_acc\n",
    "K = dataset.num_classes\n",
    "print(f'K={K}')\n",
    "print(np.unique(all_clu_label).shape)\n",
    "# kmeans = MiniBatchKMeans(n_clusters=10*K, batch_size=2048, random_state=0, n_init=10, max_iter=500, verbose=2).fit(all_vfeatures)\n",
    "# preds = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7c3a4-71f7-4229-af50-4159c0c001ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 127192.765625.\n",
      "Iteration 1, inertia 81555.3984375.\n",
      "Iteration 2, inertia 79980.984375.\n",
      "Iteration 3, inertia 79440.2109375.\n",
      "Iteration 4, inertia 79163.9765625.\n",
      "Iteration 5, inertia 79013.953125.\n",
      "Iteration 6, inertia 78914.40625.\n",
      "Iteration 7, inertia 78825.3671875.\n",
      "Iteration 8, inertia 78736.984375.\n",
      "Iteration 9, inertia 78670.921875.\n",
      "Iteration 10, inertia 78626.984375.\n",
      "Iteration 11, inertia 78594.734375.\n",
      "Iteration 12, inertia 78567.1953125.\n",
      "Iteration 13, inertia 78540.640625.\n",
      "Iteration 14, inertia 78514.0390625.\n",
      "Iteration 15, inertia 78489.2578125.\n",
      "Iteration 16, inertia 78467.34375.\n",
      "Iteration 17, inertia 78447.40625.\n",
      "Iteration 18, inertia 78427.5859375.\n",
      "Iteration 19, inertia 78412.03125.\n",
      "Iteration 20, inertia 78401.140625.\n",
      "Iteration 21, inertia 78392.2578125.\n",
      "Iteration 22, inertia 78384.9296875.\n",
      "Iteration 23, inertia 78378.9765625.\n",
      "Iteration 24, inertia 78374.140625.\n",
      "Iteration 25, inertia 78369.7890625.\n",
      "Iteration 26, inertia 78365.2890625.\n",
      "Iteration 27, inertia 78361.2109375.\n",
      "Iteration 28, inertia 78357.1953125.\n",
      "Iteration 29, inertia 78353.5078125.\n",
      "Iteration 30, inertia 78350.046875.\n",
      "Iteration 31, inertia 78346.65625.\n",
      "Iteration 32, inertia 78343.3125.\n",
      "Iteration 33, inertia 78340.0234375.\n",
      "Iteration 34, inertia 78336.671875.\n",
      "Iteration 35, inertia 78333.34375.\n",
      "Iteration 36, inertia 78330.4296875.\n",
      "Iteration 37, inertia 78328.765625.\n",
      "Iteration 38, inertia 78327.453125.\n",
      "Iteration 39, inertia 78326.28125.\n",
      "Iteration 40, inertia 78325.171875.\n",
      "Iteration 41, inertia 78324.171875.\n",
      "Iteration 42, inertia 78323.2890625.\n",
      "Iteration 43, inertia 78322.4140625.\n",
      "Iteration 44, inertia 78321.75.\n",
      "Iteration 45, inertia 78321.296875.\n",
      "Iteration 46, inertia 78320.9609375.\n",
      "Iteration 47, inertia 78320.640625.\n",
      "Iteration 48, inertia 78320.3828125.\n",
      "Iteration 49, inertia 78320.1875.\n",
      "Iteration 50, inertia 78320.0078125.\n",
      "Iteration 51, inertia 78319.7421875.\n",
      "Iteration 52, inertia 78319.5703125.\n",
      "Iteration 53, inertia 78319.3984375.\n",
      "Iteration 54, inertia 78319.234375.\n",
      "Iteration 55, inertia 78319.0390625.\n",
      "Iteration 56, inertia 78318.78125.\n",
      "Iteration 57, inertia 78318.53125.\n",
      "Iteration 58, inertia 78318.3046875.\n",
      "Iteration 59, inertia 78318.0234375.\n",
      "Iteration 60, inertia 78317.6328125.\n",
      "Iteration 61, inertia 78317.234375.\n",
      "Iteration 62, inertia 78316.8203125.\n",
      "Iteration 63, inertia 78316.28125.\n",
      "Iteration 64, inertia 78315.8125.\n",
      "Iteration 65, inertia 78315.5078125.\n",
      "Iteration 66, inertia 78315.078125.\n",
      "Iteration 67, inertia 78314.6015625.\n",
      "Iteration 68, inertia 78314.21875.\n",
      "Iteration 69, inertia 78313.875.\n",
      "Iteration 70, inertia 78313.5859375.\n",
      "Iteration 71, inertia 78313.328125.\n",
      "Iteration 72, inertia 78313.171875.\n",
      "Iteration 73, inertia 78313.0234375.\n",
      "Iteration 74, inertia 78312.921875.\n",
      "Iteration 75, inertia 78312.84375.\n",
      "Iteration 76, inertia 78312.7734375.\n",
      "Iteration 77, inertia 78312.71875.\n",
      "Iteration 78, inertia 78312.703125.\n",
      "Iteration 79, inertia 78312.6640625.\n",
      "Iteration 80, inertia 78312.625.\n",
      "Iteration 81, inertia 78312.6171875.\n",
      "Iteration 82, inertia 78312.578125.\n",
      "Iteration 83, inertia 78312.5859375.\n",
      "Iteration 84, inertia 78312.5546875.\n",
      "Iteration 85, inertia 78312.5625.\n",
      "Iteration 86, inertia 78312.5703125.\n",
      "Iteration 87, inertia 78312.5546875.\n",
      "Iteration 88, inertia 78312.5390625.\n",
      "Iteration 89, inertia 78312.546875.\n",
      "Iteration 90, inertia 78312.53125.\n",
      "Iteration 91, inertia 78312.53125.\n",
      "Converged at iteration 91: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 128317.3046875.\n",
      "Iteration 1, inertia 82136.1484375.\n",
      "Iteration 2, inertia 80516.71875.\n",
      "Iteration 3, inertia 79921.59375.\n",
      "Iteration 4, inertia 79598.34375.\n",
      "Iteration 5, inertia 79354.9921875.\n",
      "Iteration 6, inertia 79182.0390625.\n",
      "Iteration 7, inertia 79061.78125.\n",
      "Iteration 8, inertia 78984.859375.\n",
      "Iteration 9, inertia 78923.0703125.\n",
      "Iteration 10, inertia 78868.6015625.\n",
      "Iteration 11, inertia 78824.234375.\n",
      "Iteration 12, inertia 78789.4609375.\n",
      "Iteration 13, inertia 78762.453125.\n",
      "Iteration 14, inertia 78743.015625.\n",
      "Iteration 15, inertia 78725.4296875.\n",
      "Iteration 16, inertia 78706.7109375.\n",
      "Iteration 17, inertia 78688.7578125.\n",
      "Iteration 18, inertia 78672.265625.\n",
      "Iteration 19, inertia 78657.1796875.\n",
      "Iteration 20, inertia 78644.421875.\n",
      "Iteration 21, inertia 78631.421875.\n",
      "Iteration 22, inertia 78618.2734375.\n",
      "Iteration 23, inertia 78606.2890625.\n",
      "Iteration 24, inertia 78593.5.\n",
      "Iteration 25, inertia 78582.015625.\n",
      "Iteration 26, inertia 78573.578125.\n",
      "Iteration 27, inertia 78564.484375.\n",
      "Iteration 28, inertia 78553.0.\n",
      "Iteration 29, inertia 78536.578125.\n",
      "Iteration 30, inertia 78515.5.\n",
      "Iteration 31, inertia 78500.390625.\n",
      "Iteration 32, inertia 78495.796875.\n",
      "Iteration 33, inertia 78492.984375.\n",
      "Iteration 34, inertia 78490.5078125.\n",
      "Iteration 35, inertia 78488.3359375.\n",
      "Iteration 36, inertia 78486.4609375.\n",
      "Iteration 37, inertia 78484.90625.\n",
      "Iteration 38, inertia 78483.7265625.\n",
      "Iteration 39, inertia 78482.7890625.\n",
      "Iteration 40, inertia 78481.9609375.\n",
      "Iteration 41, inertia 78481.2578125.\n",
      "Iteration 42, inertia 78480.5234375.\n",
      "Iteration 43, inertia 78479.7578125.\n",
      "Iteration 44, inertia 78478.9609375.\n",
      "Iteration 45, inertia 78477.71875.\n",
      "Iteration 46, inertia 78476.0703125.\n",
      "Iteration 47, inertia 78473.078125.\n",
      "Iteration 48, inertia 78468.4140625.\n",
      "Iteration 49, inertia 78462.796875.\n",
      "Iteration 50, inertia 78459.0703125.\n",
      "Iteration 51, inertia 78457.3828125.\n",
      "Iteration 52, inertia 78456.359375.\n",
      "Iteration 53, inertia 78455.59375.\n",
      "Iteration 54, inertia 78455.1484375.\n",
      "Iteration 55, inertia 78454.8046875.\n",
      "Iteration 56, inertia 78454.5078125.\n",
      "Iteration 57, inertia 78454.265625.\n",
      "Iteration 58, inertia 78454.1015625.\n",
      "Iteration 59, inertia 78454.0078125.\n",
      "Iteration 60, inertia 78453.8671875.\n",
      "Iteration 61, inertia 78453.7421875.\n",
      "Iteration 62, inertia 78453.625.\n",
      "Iteration 63, inertia 78453.5078125.\n",
      "Iteration 64, inertia 78453.421875.\n",
      "Iteration 65, inertia 78453.375.\n",
      "Iteration 66, inertia 78453.3203125.\n",
      "Iteration 67, inertia 78453.265625.\n",
      "Iteration 68, inertia 78453.140625.\n",
      "Iteration 69, inertia 78452.9609375.\n",
      "Iteration 70, inertia 78452.796875.\n",
      "Iteration 71, inertia 78452.5546875.\n",
      "Iteration 72, inertia 78452.2421875.\n",
      "Iteration 73, inertia 78451.9765625.\n",
      "Iteration 74, inertia 78451.625.\n",
      "Iteration 75, inertia 78451.21875.\n",
      "Iteration 76, inertia 78450.8359375.\n",
      "Iteration 77, inertia 78450.3359375.\n",
      "Iteration 78, inertia 78449.9140625.\n",
      "Iteration 79, inertia 78449.421875.\n",
      "Iteration 80, inertia 78448.828125.\n",
      "Iteration 81, inertia 78448.296875.\n",
      "Iteration 82, inertia 78447.8359375.\n",
      "Iteration 83, inertia 78447.46875.\n",
      "Iteration 84, inertia 78447.1796875.\n",
      "Iteration 85, inertia 78446.921875.\n",
      "Iteration 86, inertia 78446.6875.\n",
      "Iteration 87, inertia 78446.5078125.\n",
      "Iteration 88, inertia 78446.3203125.\n",
      "Iteration 89, inertia 78446.1484375.\n",
      "Iteration 90, inertia 78445.9453125.\n",
      "Iteration 91, inertia 78445.734375.\n",
      "Iteration 92, inertia 78445.40625.\n",
      "Iteration 93, inertia 78445.0625.\n",
      "Iteration 94, inertia 78444.7734375.\n",
      "Iteration 95, inertia 78444.4296875.\n",
      "Iteration 96, inertia 78444.0625.\n",
      "Iteration 97, inertia 78443.5234375.\n",
      "Iteration 98, inertia 78443.09375.\n",
      "Iteration 99, inertia 78442.765625.\n",
      "Iteration 100, inertia 78442.359375.\n",
      "Iteration 101, inertia 78442.0234375.\n",
      "Iteration 102, inertia 78441.796875.\n",
      "Iteration 103, inertia 78441.5703125.\n",
      "Iteration 104, inertia 78441.375.\n",
      "Iteration 105, inertia 78441.1796875.\n",
      "Iteration 106, inertia 78441.046875.\n",
      "Iteration 107, inertia 78440.9765625.\n",
      "Iteration 108, inertia 78440.9453125.\n",
      "Iteration 109, inertia 78440.9140625.\n",
      "Iteration 110, inertia 78440.8984375.\n",
      "Iteration 111, inertia 78440.8671875.\n",
      "Iteration 112, inertia 78440.8515625.\n",
      "Iteration 113, inertia 78440.828125.\n",
      "Iteration 114, inertia 78440.8046875.\n",
      "Iteration 115, inertia 78440.796875.\n",
      "Iteration 116, inertia 78440.796875.\n",
      "Iteration 117, inertia 78440.7890625.\n",
      "Iteration 118, inertia 78440.7734375.\n",
      "Iteration 119, inertia 78440.7890625.\n",
      "Iteration 120, inertia 78440.7890625.\n",
      "Converged at iteration 120: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 128689.1796875.\n",
      "Iteration 1, inertia 81893.6015625.\n",
      "Iteration 2, inertia 80275.8125.\n",
      "Iteration 3, inertia 79657.59375.\n",
      "Iteration 4, inertia 79318.0625.\n",
      "Iteration 5, inertia 79114.3515625.\n",
      "Iteration 6, inertia 78979.6640625.\n",
      "Iteration 7, inertia 78880.40625.\n",
      "Iteration 8, inertia 78800.34375.\n",
      "Iteration 9, inertia 78741.0859375.\n",
      "Iteration 10, inertia 78701.6640625.\n",
      "Iteration 11, inertia 78673.40625.\n",
      "Iteration 12, inertia 78649.421875.\n",
      "Iteration 13, inertia 78624.75.\n",
      "Iteration 14, inertia 78602.9453125.\n",
      "Iteration 15, inertia 78586.515625.\n",
      "Iteration 16, inertia 78573.828125.\n",
      "Iteration 17, inertia 78562.3515625.\n",
      "Iteration 18, inertia 78550.140625.\n",
      "Iteration 19, inertia 78537.1875.\n",
      "Iteration 20, inertia 78526.625.\n",
      "Iteration 21, inertia 78517.1796875.\n",
      "Iteration 22, inertia 78508.984375.\n",
      "Iteration 23, inertia 78501.203125.\n",
      "Iteration 24, inertia 78495.046875.\n",
      "Iteration 25, inertia 78489.9140625.\n",
      "Iteration 26, inertia 78485.0234375.\n",
      "Iteration 27, inertia 78480.1328125.\n",
      "Iteration 28, inertia 78475.8203125.\n",
      "Iteration 29, inertia 78472.2265625.\n",
      "Iteration 30, inertia 78469.8046875.\n",
      "Iteration 31, inertia 78467.9765625.\n",
      "Iteration 32, inertia 78466.515625.\n",
      "Iteration 33, inertia 78465.5546875.\n",
      "Iteration 34, inertia 78464.7734375.\n",
      "Iteration 35, inertia 78464.1171875.\n",
      "Iteration 36, inertia 78463.421875.\n",
      "Iteration 37, inertia 78462.7109375.\n",
      "Iteration 38, inertia 78462.1015625.\n",
      "Iteration 39, inertia 78461.453125.\n",
      "Iteration 40, inertia 78460.8671875.\n",
      "Iteration 41, inertia 78460.2734375.\n",
      "Iteration 42, inertia 78459.640625.\n",
      "Iteration 43, inertia 78458.8828125.\n",
      "Iteration 44, inertia 78458.0390625.\n",
      "Iteration 45, inertia 78456.9609375.\n",
      "Iteration 46, inertia 78455.28125.\n",
      "Iteration 47, inertia 78452.3671875.\n",
      "Iteration 48, inertia 78446.9609375.\n",
      "Iteration 49, inertia 78440.1875.\n",
      "Iteration 50, inertia 78430.921875.\n",
      "Iteration 51, inertia 78423.6875.\n",
      "Iteration 52, inertia 78420.671875.\n",
      "Iteration 53, inertia 78418.8515625.\n",
      "Iteration 54, inertia 78417.2890625.\n",
      "Iteration 55, inertia 78415.9921875.\n",
      "Iteration 56, inertia 78415.078125.\n",
      "Iteration 57, inertia 78414.140625.\n",
      "Iteration 58, inertia 78413.546875.\n",
      "Iteration 59, inertia 78413.03125.\n",
      "Iteration 60, inertia 78412.6171875.\n",
      "Iteration 61, inertia 78412.203125.\n",
      "Iteration 62, inertia 78411.8125.\n",
      "Iteration 63, inertia 78411.5703125.\n",
      "Iteration 64, inertia 78411.3828125.\n",
      "Iteration 65, inertia 78411.25.\n",
      "Iteration 66, inertia 78411.15625.\n",
      "Iteration 67, inertia 78411.0625.\n",
      "Iteration 68, inertia 78410.9921875.\n",
      "Iteration 69, inertia 78410.8828125.\n",
      "Iteration 70, inertia 78410.7578125.\n",
      "Iteration 71, inertia 78410.6484375.\n",
      "Iteration 72, inertia 78410.5546875.\n",
      "Iteration 73, inertia 78410.5.\n",
      "Iteration 74, inertia 78410.4765625.\n",
      "Iteration 75, inertia 78410.40625.\n",
      "Iteration 76, inertia 78410.390625.\n",
      "Iteration 77, inertia 78410.3515625.\n",
      "Iteration 78, inertia 78410.3125.\n",
      "Iteration 79, inertia 78410.25.\n",
      "Iteration 80, inertia 78410.203125.\n",
      "Iteration 81, inertia 78410.1640625.\n",
      "Iteration 82, inertia 78410.09375.\n",
      "Iteration 83, inertia 78410.0546875.\n",
      "Iteration 84, inertia 78410.015625.\n",
      "Iteration 85, inertia 78409.953125.\n",
      "Iteration 86, inertia 78409.9296875.\n",
      "Iteration 87, inertia 78409.90625.\n",
      "Iteration 88, inertia 78409.859375.\n",
      "Iteration 89, inertia 78409.8203125.\n",
      "Iteration 90, inertia 78409.734375.\n",
      "Iteration 91, inertia 78409.6796875.\n",
      "Iteration 92, inertia 78409.625.\n",
      "Iteration 93, inertia 78409.5546875.\n",
      "Iteration 94, inertia 78409.4921875.\n",
      "Iteration 95, inertia 78409.421875.\n",
      "Iteration 96, inertia 78409.328125.\n",
      "Iteration 97, inertia 78409.234375.\n",
      "Iteration 98, inertia 78409.125.\n",
      "Iteration 99, inertia 78409.015625.\n",
      "Iteration 100, inertia 78408.84375.\n",
      "Iteration 101, inertia 78408.6171875.\n",
      "Iteration 102, inertia 78408.40625.\n",
      "Iteration 103, inertia 78408.0703125.\n",
      "Iteration 104, inertia 78407.734375.\n",
      "Iteration 105, inertia 78407.375.\n",
      "Iteration 106, inertia 78407.0078125.\n",
      "Iteration 107, inertia 78406.71875.\n",
      "Iteration 108, inertia 78406.40625.\n",
      "Iteration 109, inertia 78406.0859375.\n",
      "Iteration 110, inertia 78405.65625.\n",
      "Iteration 111, inertia 78405.234375.\n",
      "Iteration 112, inertia 78404.84375.\n",
      "Iteration 113, inertia 78404.5.\n",
      "Iteration 114, inertia 78404.28125.\n",
      "Iteration 115, inertia 78404.0546875.\n",
      "Iteration 116, inertia 78403.90625.\n",
      "Iteration 117, inertia 78403.75.\n",
      "Iteration 118, inertia 78403.6015625.\n",
      "Iteration 119, inertia 78403.484375.\n",
      "Iteration 120, inertia 78403.390625.\n",
      "Iteration 121, inertia 78403.296875.\n",
      "Iteration 122, inertia 78403.21875.\n",
      "Iteration 123, inertia 78403.171875.\n",
      "Iteration 124, inertia 78403.15625.\n",
      "Iteration 125, inertia 78403.1328125.\n",
      "Iteration 126, inertia 78403.109375.\n",
      "Iteration 127, inertia 78403.0859375.\n",
      "Iteration 128, inertia 78403.0625.\n",
      "Iteration 129, inertia 78403.015625.\n",
      "Iteration 130, inertia 78403.0.\n",
      "Iteration 131, inertia 78402.96875.\n",
      "Iteration 132, inertia 78402.953125.\n",
      "Iteration 133, inertia 78402.9375.\n",
      "Iteration 134, inertia 78402.9453125.\n",
      "Iteration 135, inertia 78402.9296875.\n",
      "Iteration 136, inertia 78402.8984375.\n",
      "Iteration 137, inertia 78402.8984375.\n",
      "Iteration 138, inertia 78402.890625.\n",
      "Iteration 139, inertia 78402.8671875.\n",
      "Iteration 140, inertia 78402.875.\n",
      "Iteration 141, inertia 78402.8984375.\n",
      "Converged at iteration 141: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 127502.03125.\n",
      "Iteration 1, inertia 81982.1796875.\n",
      "Iteration 2, inertia 80403.140625.\n",
      "Iteration 3, inertia 79759.015625.\n",
      "Iteration 4, inertia 79414.2421875.\n",
      "Iteration 5, inertia 79217.125.\n",
      "Iteration 6, inertia 79081.96875.\n",
      "Iteration 7, inertia 78963.21875.\n",
      "Iteration 8, inertia 78852.328125.\n",
      "Iteration 9, inertia 78775.5625.\n",
      "Iteration 10, inertia 78724.7578125.\n",
      "Iteration 11, inertia 78683.375.\n",
      "Iteration 12, inertia 78648.1875.\n",
      "Iteration 13, inertia 78613.703125.\n",
      "Iteration 14, inertia 78575.2109375.\n",
      "Iteration 15, inertia 78540.46875.\n",
      "Iteration 16, inertia 78514.0390625.\n",
      "Iteration 17, inertia 78495.15625.\n",
      "Iteration 18, inertia 78478.5546875.\n",
      "Iteration 19, inertia 78464.3125.\n",
      "Iteration 20, inertia 78452.515625.\n",
      "Iteration 21, inertia 78442.9375.\n",
      "Iteration 22, inertia 78433.1796875.\n",
      "Iteration 23, inertia 78423.3671875.\n",
      "Iteration 24, inertia 78413.6328125.\n",
      "Iteration 25, inertia 78404.1015625.\n",
      "Iteration 26, inertia 78395.3984375.\n",
      "Iteration 27, inertia 78388.4375.\n",
      "Iteration 28, inertia 78381.359375.\n",
      "Iteration 29, inertia 78370.5703125.\n",
      "Iteration 30, inertia 78351.859375.\n",
      "Iteration 31, inertia 78334.546875.\n",
      "Iteration 32, inertia 78325.7265625.\n",
      "Iteration 33, inertia 78320.4453125.\n",
      "Iteration 34, inertia 78316.59375.\n",
      "Iteration 35, inertia 78313.0625.\n",
      "Iteration 36, inertia 78309.4140625.\n",
      "Iteration 37, inertia 78305.9296875.\n",
      "Iteration 38, inertia 78302.4375.\n",
      "Iteration 39, inertia 78298.3671875.\n",
      "Iteration 40, inertia 78293.0625.\n",
      "Iteration 41, inertia 78285.7265625.\n",
      "Iteration 42, inertia 78279.078125.\n",
      "Iteration 43, inertia 78275.6875.\n",
      "Iteration 44, inertia 78273.203125.\n",
      "Iteration 45, inertia 78271.2734375.\n",
      "Iteration 46, inertia 78269.4921875.\n",
      "Iteration 47, inertia 78267.71875.\n",
      "Iteration 48, inertia 78265.9765625.\n",
      "Iteration 49, inertia 78264.2109375.\n",
      "Iteration 50, inertia 78262.171875.\n",
      "Iteration 51, inertia 78260.40625.\n",
      "Iteration 52, inertia 78258.7265625.\n",
      "Iteration 53, inertia 78257.046875.\n",
      "Iteration 54, inertia 78255.1484375.\n",
      "Iteration 55, inertia 78253.265625.\n",
      "Iteration 56, inertia 78252.078125.\n",
      "Iteration 57, inertia 78251.234375.\n",
      "Iteration 58, inertia 78250.390625.\n",
      "Iteration 59, inertia 78249.578125.\n",
      "Iteration 60, inertia 78248.8671875.\n",
      "Iteration 61, inertia 78248.0703125.\n",
      "Iteration 62, inertia 78247.4140625.\n",
      "Iteration 63, inertia 78246.7421875.\n",
      "Iteration 64, inertia 78245.5390625.\n",
      "Iteration 65, inertia 78243.578125.\n",
      "Iteration 66, inertia 78241.125.\n",
      "Iteration 67, inertia 78239.390625.\n",
      "Iteration 68, inertia 78239.015625.\n",
      "Iteration 69, inertia 78238.890625.\n",
      "Iteration 70, inertia 78238.84375.\n",
      "Iteration 71, inertia 78238.734375.\n",
      "Iteration 72, inertia 78238.671875.\n",
      "Iteration 73, inertia 78238.59375.\n",
      "Iteration 74, inertia 78238.5390625.\n",
      "Iteration 75, inertia 78238.5234375.\n",
      "Iteration 76, inertia 78238.484375.\n",
      "Iteration 77, inertia 78238.4609375.\n",
      "Iteration 78, inertia 78238.4296875.\n",
      "Iteration 79, inertia 78238.4140625.\n",
      "Iteration 80, inertia 78238.4140625.\n",
      "Iteration 81, inertia 78238.40625.\n",
      "Iteration 82, inertia 78238.375.\n",
      "Iteration 83, inertia 78238.3515625.\n",
      "Iteration 84, inertia 78238.3359375.\n",
      "Iteration 85, inertia 78238.328125.\n",
      "Iteration 86, inertia 78238.3125.\n",
      "Iteration 87, inertia 78238.3046875.\n",
      "Iteration 88, inertia 78238.3203125.\n",
      "Iteration 89, inertia 78238.3046875.\n",
      "Iteration 90, inertia 78238.3125.\n",
      "Iteration 91, inertia 78238.296875.\n",
      "Converged at iteration 91: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 127991.7421875.\n",
      "Iteration 1, inertia 81832.9140625.\n",
      "Iteration 2, inertia 80238.921875.\n",
      "Iteration 3, inertia 79573.953125.\n",
      "Iteration 4, inertia 79220.3046875.\n",
      "Iteration 5, inertia 78997.53125.\n",
      "Iteration 6, inertia 78830.921875.\n",
      "Iteration 7, inertia 78720.921875.\n",
      "Iteration 8, inertia 78642.046875.\n",
      "Iteration 9, inertia 78578.984375.\n",
      "Iteration 10, inertia 78531.125.\n",
      "Iteration 11, inertia 78492.9921875.\n",
      "Iteration 12, inertia 78464.9453125.\n",
      "Iteration 13, inertia 78446.4765625.\n",
      "Iteration 14, inertia 78432.4375.\n",
      "Iteration 15, inertia 78418.890625.\n",
      "Iteration 16, inertia 78402.9453125.\n",
      "Iteration 17, inertia 78382.8515625.\n",
      "Iteration 18, inertia 78361.1953125.\n",
      "Iteration 19, inertia 78345.4921875.\n",
      "Iteration 20, inertia 78337.78125.\n",
      "Iteration 21, inertia 78332.171875.\n",
      "Iteration 22, inertia 78327.140625.\n",
      "Iteration 23, inertia 78322.4453125.\n",
      "Iteration 24, inertia 78318.125.\n",
      "Iteration 25, inertia 78314.3125.\n",
      "Iteration 26, inertia 78311.2578125.\n",
      "Iteration 27, inertia 78308.6640625.\n",
      "Iteration 28, inertia 78306.3125.\n",
      "Iteration 29, inertia 78304.0234375.\n",
      "Iteration 30, inertia 78301.375.\n",
      "Iteration 31, inertia 78298.953125.\n",
      "Iteration 32, inertia 78295.9921875.\n",
      "Iteration 33, inertia 78292.2890625.\n",
      "Iteration 34, inertia 78287.859375.\n",
      "Iteration 35, inertia 78283.25.\n",
      "Iteration 36, inertia 78279.3125.\n",
      "Iteration 37, inertia 78276.375.\n",
      "Iteration 38, inertia 78273.6953125.\n",
      "Iteration 39, inertia 78271.21875.\n",
      "Iteration 40, inertia 78269.171875.\n",
      "Iteration 41, inertia 78267.671875.\n",
      "Iteration 42, inertia 78266.4296875.\n",
      "Iteration 43, inertia 78265.109375.\n",
      "Iteration 44, inertia 78263.2734375.\n",
      "Iteration 45, inertia 78260.703125.\n",
      "Iteration 46, inertia 78257.6328125.\n",
      "Iteration 47, inertia 78254.8671875.\n",
      "Iteration 48, inertia 78253.1953125.\n",
      "Iteration 49, inertia 78252.34375.\n",
      "Iteration 50, inertia 78251.6953125.\n",
      "Iteration 51, inertia 78251.1484375.\n",
      "Iteration 52, inertia 78250.6484375.\n",
      "Iteration 53, inertia 78250.2421875.\n",
      "Iteration 54, inertia 78249.9375.\n",
      "Iteration 55, inertia 78249.671875.\n",
      "Iteration 56, inertia 78249.453125.\n",
      "Iteration 57, inertia 78249.140625.\n",
      "Iteration 58, inertia 78248.8046875.\n",
      "Iteration 59, inertia 78248.4140625.\n",
      "Iteration 60, inertia 78248.0546875.\n",
      "Iteration 61, inertia 78247.734375.\n",
      "Iteration 62, inertia 78247.46875.\n",
      "Iteration 63, inertia 78247.2109375.\n",
      "Iteration 64, inertia 78246.9921875.\n",
      "Iteration 65, inertia 78246.78125.\n",
      "Iteration 66, inertia 78246.515625.\n",
      "Iteration 67, inertia 78246.1875.\n",
      "Iteration 68, inertia 78245.7265625.\n",
      "Iteration 69, inertia 78245.21875.\n",
      "Iteration 70, inertia 78244.6171875.\n",
      "Iteration 71, inertia 78243.796875.\n",
      "Iteration 72, inertia 78242.6875.\n",
      "Iteration 73, inertia 78241.3125.\n",
      "Iteration 74, inertia 78239.375.\n",
      "Iteration 75, inertia 78236.921875.\n",
      "Iteration 76, inertia 78233.9609375.\n",
      "Iteration 77, inertia 78231.2109375.\n",
      "Iteration 78, inertia 78228.3828125.\n",
      "Iteration 79, inertia 78225.6484375.\n",
      "Iteration 80, inertia 78223.140625.\n",
      "Iteration 81, inertia 78220.9765625.\n",
      "Iteration 82, inertia 78219.734375.\n",
      "Iteration 83, inertia 78219.03125.\n",
      "Iteration 84, inertia 78218.4453125.\n",
      "Iteration 85, inertia 78218.0078125.\n",
      "Iteration 86, inertia 78217.703125.\n",
      "Iteration 87, inertia 78217.4765625.\n",
      "Iteration 88, inertia 78217.296875.\n",
      "Iteration 89, inertia 78217.1796875.\n",
      "Iteration 90, inertia 78217.078125.\n",
      "Iteration 91, inertia 78216.9921875.\n",
      "Iteration 92, inertia 78216.8984375.\n",
      "Iteration 93, inertia 78216.8359375.\n",
      "Iteration 94, inertia 78216.765625.\n",
      "Iteration 95, inertia 78216.703125.\n",
      "Iteration 96, inertia 78216.6484375.\n",
      "Iteration 97, inertia 78216.578125.\n",
      "Iteration 98, inertia 78216.5078125.\n",
      "Iteration 99, inertia 78216.453125.\n",
      "Iteration 100, inertia 78216.390625.\n",
      "Iteration 101, inertia 78216.3359375.\n",
      "Iteration 102, inertia 78216.296875.\n",
      "Iteration 103, inertia 78216.203125.\n",
      "Iteration 104, inertia 78216.1328125.\n",
      "Iteration 105, inertia 78216.03125.\n",
      "Iteration 106, inertia 78215.953125.\n",
      "Iteration 107, inertia 78215.8671875.\n",
      "Iteration 108, inertia 78215.828125.\n",
      "Iteration 109, inertia 78215.8046875.\n",
      "Iteration 110, inertia 78215.7890625.\n",
      "Iteration 111, inertia 78215.7578125.\n",
      "Iteration 112, inertia 78215.7421875.\n",
      "Iteration 113, inertia 78215.7421875.\n",
      "Iteration 114, inertia 78215.7109375.\n",
      "Iteration 115, inertia 78215.6875.\n",
      "Iteration 116, inertia 78215.7109375.\n",
      "Iteration 117, inertia 78215.671875.\n",
      "Iteration 118, inertia 78215.65625.\n",
      "Iteration 119, inertia 78215.6640625.\n",
      "Iteration 120, inertia 78215.6328125.\n",
      "Iteration 121, inertia 78215.640625.\n",
      "Iteration 122, inertia 78215.625.\n",
      "Iteration 123, inertia 78215.6015625.\n",
      "Iteration 124, inertia 78215.6015625.\n",
      "Iteration 125, inertia 78215.5859375.\n",
      "Iteration 126, inertia 78215.5703125.\n",
      "Iteration 127, inertia 78215.5546875.\n",
      "Iteration 128, inertia 78215.5234375.\n",
      "Iteration 129, inertia 78215.53125.\n",
      "Iteration 130, inertia 78215.5078125.\n",
      "Iteration 131, inertia 78215.5.\n",
      "Iteration 132, inertia 78215.484375.\n",
      "Iteration 133, inertia 78215.4765625.\n",
      "Iteration 134, inertia 78215.4921875.\n",
      "Iteration 135, inertia 78215.46875.\n",
      "Iteration 136, inertia 78215.46875.\n",
      "Iteration 137, inertia 78215.453125.\n",
      "Iteration 138, inertia 78215.4609375.\n",
      "Iteration 139, inertia 78215.4453125.\n",
      "Iteration 140, inertia 78215.4296875.\n",
      "Iteration 141, inertia 78215.4453125.\n",
      "Iteration 142, inertia 78215.4296875.\n",
      "Iteration 143, inertia 78215.4375.\n",
      "Iteration 144, inertia 78215.4140625.\n",
      "Iteration 145, inertia 78215.40625.\n",
      "Iteration 146, inertia 78215.421875.\n",
      "Iteration 147, inertia 78215.421875.\n",
      "Iteration 148, inertia 78215.3984375.\n",
      "Iteration 149, inertia 78215.4140625.\n",
      "Iteration 150, inertia 78215.390625.\n",
      "Iteration 151, inertia 78215.390625.\n",
      "Iteration 152, inertia 78215.390625.\n",
      "Iteration 153, inertia 78215.3671875.\n",
      "Iteration 154, inertia 78215.3671875.\n",
      "Iteration 155, inertia 78215.375.\n",
      "Iteration 156, inertia 78215.3515625.\n",
      "Iteration 157, inertia 78215.34375.\n",
      "Iteration 158, inertia 78215.3125.\n",
      "Iteration 159, inertia 78215.3125.\n",
      "Iteration 160, inertia 78215.296875.\n",
      "Iteration 161, inertia 78215.3125.\n",
      "Iteration 162, inertia 78215.3046875.\n",
      "Iteration 163, inertia 78215.265625.\n",
      "Iteration 164, inertia 78215.265625.\n",
      "Iteration 165, inertia 78215.2265625.\n",
      "Iteration 166, inertia 78215.2109375.\n",
      "Iteration 167, inertia 78215.1875.\n",
      "Iteration 168, inertia 78215.15625.\n",
      "Iteration 169, inertia 78215.1484375.\n",
      "Iteration 170, inertia 78215.1484375.\n",
      "Iteration 171, inertia 78215.1171875.\n",
      "Iteration 172, inertia 78215.1171875.\n",
      "Iteration 173, inertia 78215.1015625.\n",
      "Iteration 174, inertia 78215.0703125.\n",
      "Iteration 175, inertia 78215.0546875.\n",
      "Iteration 176, inertia 78215.03125.\n",
      "Iteration 177, inertia 78215.015625.\n",
      "Iteration 178, inertia 78215.015625.\n",
      "Iteration 179, inertia 78214.9921875.\n",
      "Iteration 180, inertia 78214.96875.\n",
      "Iteration 181, inertia 78214.9453125.\n",
      "Iteration 182, inertia 78214.953125.\n",
      "Iteration 183, inertia 78214.9296875.\n",
      "Iteration 184, inertia 78214.9140625.\n",
      "Iteration 185, inertia 78214.890625.\n",
      "Iteration 186, inertia 78214.8671875.\n",
      "Iteration 187, inertia 78214.859375.\n",
      "Iteration 188, inertia 78214.8046875.\n",
      "Iteration 189, inertia 78214.765625.\n",
      "Iteration 190, inertia 78214.7578125.\n",
      "Iteration 191, inertia 78214.7265625.\n",
      "Iteration 192, inertia 78214.6640625.\n",
      "Iteration 193, inertia 78214.5625.\n",
      "Iteration 194, inertia 78214.4609375.\n",
      "Iteration 195, inertia 78214.3515625.\n",
      "Iteration 196, inertia 78214.1640625.\n",
      "Iteration 197, inertia 78214.0234375.\n",
      "Iteration 198, inertia 78213.90625.\n",
      "Iteration 199, inertia 78213.7421875.\n",
      "Iteration 200, inertia 78213.609375.\n",
      "Iteration 201, inertia 78213.40625.\n",
      "Iteration 202, inertia 78213.3046875.\n",
      "Iteration 203, inertia 78213.140625.\n",
      "Iteration 204, inertia 78213.03125.\n",
      "Iteration 205, inertia 78212.8828125.\n",
      "Iteration 206, inertia 78212.7421875.\n",
      "Iteration 207, inertia 78212.59375.\n",
      "Iteration 208, inertia 78212.4921875.\n",
      "Iteration 209, inertia 78212.4296875.\n",
      "Iteration 210, inertia 78212.40625.\n",
      "Iteration 211, inertia 78212.375.\n",
      "Iteration 212, inertia 78212.3828125.\n",
      "Iteration 213, inertia 78212.3203125.\n",
      "Iteration 214, inertia 78212.3359375.\n",
      "Iteration 215, inertia 78212.3125.\n",
      "Iteration 216, inertia 78212.2890625.\n",
      "Iteration 217, inertia 78212.28125.\n",
      "Iteration 218, inertia 78212.265625.\n",
      "Iteration 219, inertia 78212.2578125.\n",
      "Iteration 220, inertia 78212.2265625.\n",
      "Iteration 221, inertia 78212.21875.\n",
      "Iteration 222, inertia 78212.2109375.\n",
      "Iteration 223, inertia 78212.1640625.\n",
      "Iteration 224, inertia 78212.15625.\n",
      "Iteration 225, inertia 78212.1328125.\n",
      "Iteration 226, inertia 78212.109375.\n",
      "Iteration 227, inertia 78212.125.\n",
      "Iteration 228, inertia 78212.0703125.\n",
      "Iteration 229, inertia 78212.046875.\n",
      "Iteration 230, inertia 78212.0078125.\n",
      "Iteration 231, inertia 78211.9609375.\n",
      "Iteration 232, inertia 78211.921875.\n",
      "Iteration 233, inertia 78211.8515625.\n",
      "Iteration 234, inertia 78211.8125.\n",
      "Iteration 235, inertia 78211.7421875.\n",
      "Iteration 236, inertia 78211.71875.\n",
      "Iteration 237, inertia 78211.6875.\n",
      "Iteration 238, inertia 78211.65625.\n",
      "Iteration 239, inertia 78211.6171875.\n",
      "Iteration 240, inertia 78211.59375.\n",
      "Iteration 241, inertia 78211.5546875.\n",
      "Iteration 242, inertia 78211.546875.\n",
      "Iteration 243, inertia 78211.53125.\n",
      "Iteration 244, inertia 78211.5078125.\n",
      "Iteration 245, inertia 78211.5234375.\n",
      "Iteration 246, inertia 78211.515625.\n",
      "Iteration 247, inertia 78211.4921875.\n",
      "Iteration 248, inertia 78211.4921875.\n",
      "Iteration 249, inertia 78211.5.\n",
      "Iteration 250, inertia 78211.4921875.\n",
      "Iteration 251, inertia 78211.484375.\n",
      "Iteration 252, inertia 78211.484375.\n",
      "Iteration 253, inertia 78211.46875.\n",
      "Iteration 254, inertia 78211.46875.\n",
      "Iteration 255, inertia 78211.46875.\n",
      "Iteration 256, inertia 78211.46875.\n",
      "Iteration 257, inertia 78211.453125.\n",
      "Iteration 258, inertia 78211.453125.\n",
      "Iteration 259, inertia 78211.453125.\n",
      "Iteration 260, inertia 78211.4375.\n",
      "Iteration 261, inertia 78211.4296875.\n",
      "Iteration 262, inertia 78211.4453125.\n",
      "Iteration 263, inertia 78211.453125.\n",
      "Iteration 264, inertia 78211.453125.\n",
      "Converged at iteration 264: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 127263.90625.\n",
      "Iteration 1, inertia 82053.0859375.\n",
      "Iteration 2, inertia 80450.5078125.\n",
      "Iteration 3, inertia 79833.8984375.\n",
      "Iteration 4, inertia 79435.578125.\n",
      "Iteration 5, inertia 79188.0546875.\n",
      "Iteration 6, inertia 79030.6328125.\n",
      "Iteration 7, inertia 78916.5625.\n",
      "Iteration 8, inertia 78835.59375.\n",
      "Iteration 9, inertia 78775.6640625.\n",
      "Iteration 10, inertia 78723.5.\n",
      "Iteration 11, inertia 78678.5859375.\n",
      "Iteration 12, inertia 78643.8359375.\n",
      "Iteration 13, inertia 78614.3125.\n",
      "Iteration 14, inertia 78587.4140625.\n",
      "Iteration 15, inertia 78564.3984375.\n",
      "Iteration 16, inertia 78547.2421875.\n",
      "Iteration 17, inertia 78535.2578125.\n",
      "Iteration 18, inertia 78526.703125.\n",
      "Iteration 19, inertia 78520.3671875.\n",
      "Iteration 20, inertia 78515.546875.\n",
      "Iteration 21, inertia 78511.203125.\n",
      "Iteration 22, inertia 78506.6640625.\n",
      "Iteration 23, inertia 78502.4609375.\n",
      "Iteration 24, inertia 78498.1484375.\n",
      "Iteration 25, inertia 78493.015625.\n",
      "Iteration 26, inertia 78487.21875.\n",
      "Iteration 27, inertia 78481.3828125.\n",
      "Iteration 28, inertia 78476.109375.\n",
      "Iteration 29, inertia 78471.6796875.\n",
      "Iteration 30, inertia 78467.5859375.\n",
      "Iteration 31, inertia 78463.78125.\n",
      "Iteration 32, inertia 78460.484375.\n",
      "Iteration 33, inertia 78457.234375.\n",
      "Iteration 34, inertia 78453.390625.\n",
      "Iteration 35, inertia 78448.7578125.\n",
      "Iteration 36, inertia 78443.7890625.\n",
      "Iteration 37, inertia 78439.1171875.\n",
      "Iteration 38, inertia 78434.609375.\n",
      "Iteration 39, inertia 78430.7109375.\n",
      "Iteration 40, inertia 78427.40625.\n",
      "Iteration 41, inertia 78424.6953125.\n",
      "Iteration 42, inertia 78422.859375.\n",
      "Iteration 43, inertia 78421.3046875.\n",
      "Iteration 44, inertia 78419.8515625.\n",
      "Iteration 45, inertia 78418.4375.\n",
      "Iteration 46, inertia 78417.1328125.\n",
      "Iteration 47, inertia 78415.71875.\n",
      "Iteration 48, inertia 78414.4296875.\n",
      "Iteration 49, inertia 78413.3125.\n",
      "Iteration 50, inertia 78412.0703125.\n",
      "Iteration 51, inertia 78410.6953125.\n",
      "Iteration 52, inertia 78409.25.\n",
      "Iteration 53, inertia 78408.0078125.\n",
      "Iteration 54, inertia 78406.9140625.\n",
      "Iteration 55, inertia 78405.9140625.\n",
      "Iteration 56, inertia 78405.1171875.\n",
      "Iteration 57, inertia 78404.4375.\n",
      "Iteration 58, inertia 78403.7578125.\n",
      "Iteration 59, inertia 78403.2734375.\n",
      "Iteration 60, inertia 78402.8828125.\n",
      "Iteration 61, inertia 78402.515625.\n",
      "Iteration 62, inertia 78402.2265625.\n",
      "Iteration 63, inertia 78401.9453125.\n",
      "Iteration 64, inertia 78401.703125.\n",
      "Iteration 65, inertia 78401.578125.\n",
      "Iteration 66, inertia 78401.484375.\n",
      "Iteration 67, inertia 78401.4609375.\n",
      "Iteration 68, inertia 78401.421875.\n",
      "Iteration 69, inertia 78401.3828125.\n",
      "Iteration 70, inertia 78401.3515625.\n",
      "Iteration 71, inertia 78401.34375.\n",
      "Iteration 72, inertia 78401.3046875.\n",
      "Iteration 73, inertia 78401.3046875.\n",
      "Iteration 74, inertia 78401.2421875.\n",
      "Iteration 75, inertia 78401.2265625.\n",
      "Iteration 76, inertia 78401.171875.\n",
      "Iteration 77, inertia 78401.09375.\n",
      "Iteration 78, inertia 78401.046875.\n",
      "Iteration 79, inertia 78400.96875.\n",
      "Iteration 80, inertia 78400.890625.\n",
      "Iteration 81, inertia 78400.7734375.\n",
      "Iteration 82, inertia 78400.71875.\n",
      "Iteration 83, inertia 78400.5859375.\n",
      "Iteration 84, inertia 78400.5234375.\n",
      "Iteration 85, inertia 78400.421875.\n",
      "Iteration 86, inertia 78400.3671875.\n",
      "Iteration 87, inertia 78400.3203125.\n",
      "Iteration 88, inertia 78400.265625.\n",
      "Iteration 89, inertia 78400.1875.\n",
      "Iteration 90, inertia 78400.1015625.\n",
      "Iteration 91, inertia 78399.9921875.\n",
      "Iteration 92, inertia 78399.8359375.\n",
      "Iteration 93, inertia 78399.59375.\n",
      "Iteration 94, inertia 78399.4296875.\n",
      "Iteration 95, inertia 78399.2578125.\n",
      "Iteration 96, inertia 78399.0625.\n",
      "Iteration 97, inertia 78398.8203125.\n",
      "Iteration 98, inertia 78398.6015625.\n",
      "Iteration 99, inertia 78398.34375.\n",
      "Iteration 100, inertia 78398.1640625.\n",
      "Iteration 101, inertia 78397.90625.\n",
      "Iteration 102, inertia 78397.6484375.\n",
      "Iteration 103, inertia 78397.4453125.\n",
      "Iteration 104, inertia 78397.1875.\n",
      "Iteration 105, inertia 78396.734375.\n",
      "Iteration 106, inertia 78395.9921875.\n",
      "Iteration 107, inertia 78395.2421875.\n",
      "Iteration 108, inertia 78394.4765625.\n",
      "Iteration 109, inertia 78393.8515625.\n",
      "Iteration 110, inertia 78393.4921875.\n",
      "Iteration 111, inertia 78393.3046875.\n",
      "Iteration 112, inertia 78393.15625.\n",
      "Iteration 113, inertia 78393.0859375.\n",
      "Iteration 114, inertia 78393.015625.\n",
      "Iteration 115, inertia 78392.9453125.\n",
      "Iteration 116, inertia 78392.84375.\n",
      "Iteration 117, inertia 78392.7734375.\n",
      "Iteration 118, inertia 78392.6953125.\n",
      "Iteration 119, inertia 78392.6484375.\n",
      "Iteration 120, inertia 78392.6015625.\n",
      "Iteration 121, inertia 78392.5625.\n",
      "Iteration 122, inertia 78392.5390625.\n",
      "Iteration 123, inertia 78392.484375.\n",
      "Iteration 124, inertia 78392.46875.\n",
      "Iteration 125, inertia 78392.4140625.\n",
      "Iteration 126, inertia 78392.2734375.\n",
      "Iteration 127, inertia 78392.0234375.\n",
      "Iteration 128, inertia 78391.75.\n",
      "Iteration 129, inertia 78391.4765625.\n",
      "Iteration 130, inertia 78391.1796875.\n",
      "Iteration 131, inertia 78390.8359375.\n",
      "Iteration 132, inertia 78390.5625.\n",
      "Iteration 133, inertia 78390.296875.\n",
      "Iteration 134, inertia 78390.125.\n",
      "Iteration 135, inertia 78390.0078125.\n",
      "Iteration 136, inertia 78389.96875.\n",
      "Iteration 137, inertia 78389.953125.\n",
      "Iteration 138, inertia 78389.953125.\n",
      "Iteration 139, inertia 78389.921875.\n",
      "Iteration 140, inertia 78389.9140625.\n",
      "Iteration 141, inertia 78389.90625.\n",
      "Iteration 142, inertia 78389.9140625.\n",
      "Iteration 143, inertia 78389.90625.\n",
      "Iteration 144, inertia 78389.890625.\n",
      "Converged at iteration 144: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 127697.1171875.\n",
      "Iteration 1, inertia 81729.7109375.\n",
      "Iteration 2, inertia 80034.7890625.\n",
      "Iteration 3, inertia 79499.8125.\n",
      "Iteration 4, inertia 79250.796875.\n",
      "Iteration 5, inertia 79100.890625.\n",
      "Iteration 6, inertia 79000.78125.\n",
      "Iteration 7, inertia 78924.6640625.\n",
      "Iteration 8, inertia 78864.78125.\n",
      "Iteration 9, inertia 78810.453125.\n",
      "Iteration 10, inertia 78765.375.\n",
      "Iteration 11, inertia 78732.5625.\n",
      "Iteration 12, inertia 78700.671875.\n",
      "Iteration 13, inertia 78662.2421875.\n",
      "Iteration 14, inertia 78629.5703125.\n",
      "Iteration 15, inertia 78610.625.\n",
      "Iteration 16, inertia 78597.625.\n",
      "Iteration 17, inertia 78586.125.\n",
      "Iteration 18, inertia 78576.03125.\n",
      "Iteration 19, inertia 78567.3671875.\n",
      "Iteration 20, inertia 78559.515625.\n",
      "Iteration 21, inertia 78553.0234375.\n",
      "Iteration 22, inertia 78547.2734375.\n",
      "Iteration 23, inertia 78541.9453125.\n",
      "Iteration 24, inertia 78537.234375.\n",
      "Iteration 25, inertia 78532.9140625.\n",
      "Iteration 26, inertia 78528.6953125.\n",
      "Iteration 27, inertia 78524.6171875.\n",
      "Iteration 28, inertia 78520.8046875.\n",
      "Iteration 29, inertia 78516.578125.\n",
      "Iteration 30, inertia 78512.2421875.\n",
      "Iteration 31, inertia 78507.0625.\n",
      "Iteration 32, inertia 78499.96875.\n",
      "Iteration 33, inertia 78490.4609375.\n",
      "Iteration 34, inertia 78477.4765625.\n",
      "Iteration 35, inertia 78461.46875.\n",
      "Iteration 36, inertia 78452.4453125.\n",
      "Iteration 37, inertia 78448.546875.\n",
      "Iteration 38, inertia 78444.6875.\n",
      "Iteration 39, inertia 78437.609375.\n",
      "Iteration 40, inertia 78424.3203125.\n",
      "Iteration 41, inertia 78412.109375.\n",
      "Iteration 42, inertia 78408.0625.\n",
      "Iteration 43, inertia 78406.3515625.\n",
      "Iteration 44, inertia 78404.5703125.\n",
      "Iteration 45, inertia 78402.4375.\n",
      "Iteration 46, inertia 78400.0859375.\n",
      "Iteration 47, inertia 78397.8125.\n",
      "Iteration 48, inertia 78395.7109375.\n",
      "Iteration 49, inertia 78393.734375.\n",
      "Iteration 50, inertia 78392.0078125.\n",
      "Iteration 51, inertia 78390.5234375.\n",
      "Iteration 52, inertia 78389.0625.\n",
      "Iteration 53, inertia 78387.8984375.\n",
      "Iteration 54, inertia 78386.890625.\n",
      "Iteration 55, inertia 78385.9375.\n",
      "Iteration 56, inertia 78385.1640625.\n",
      "Iteration 57, inertia 78384.453125.\n",
      "Iteration 58, inertia 78383.84375.\n",
      "Iteration 59, inertia 78383.2578125.\n",
      "Iteration 60, inertia 78382.5546875.\n",
      "Iteration 61, inertia 78381.90625.\n",
      "Iteration 62, inertia 78381.2109375.\n",
      "Iteration 63, inertia 78380.515625.\n",
      "Iteration 64, inertia 78379.8828125.\n",
      "Iteration 65, inertia 78379.1796875.\n",
      "Iteration 66, inertia 78378.3828125.\n",
      "Iteration 67, inertia 78377.5390625.\n",
      "Iteration 68, inertia 78376.671875.\n",
      "Iteration 69, inertia 78375.90625.\n",
      "Iteration 70, inertia 78375.21875.\n",
      "Iteration 71, inertia 78374.5.\n",
      "Iteration 72, inertia 78373.8671875.\n",
      "Iteration 73, inertia 78373.1875.\n",
      "Iteration 74, inertia 78372.6328125.\n",
      "Iteration 75, inertia 78372.1796875.\n",
      "Iteration 76, inertia 78371.8984375.\n",
      "Iteration 77, inertia 78371.703125.\n",
      "Iteration 78, inertia 78371.5703125.\n",
      "Iteration 79, inertia 78371.4765625.\n",
      "Iteration 80, inertia 78371.3984375.\n",
      "Iteration 81, inertia 78371.3046875.\n",
      "Iteration 82, inertia 78371.21875.\n",
      "Iteration 83, inertia 78371.15625.\n",
      "Iteration 84, inertia 78371.0546875.\n",
      "Iteration 85, inertia 78370.9609375.\n",
      "Iteration 86, inertia 78370.8828125.\n",
      "Iteration 87, inertia 78370.8125.\n",
      "Iteration 88, inertia 78370.703125.\n",
      "Iteration 89, inertia 78370.6328125.\n",
      "Iteration 90, inertia 78370.5703125.\n",
      "Iteration 91, inertia 78370.5.\n",
      "Iteration 92, inertia 78370.4453125.\n",
      "Iteration 93, inertia 78370.3671875.\n",
      "Iteration 94, inertia 78370.296875.\n",
      "Iteration 95, inertia 78370.21875.\n",
      "Iteration 96, inertia 78370.140625.\n",
      "Iteration 97, inertia 78370.0703125.\n",
      "Iteration 98, inertia 78369.953125.\n",
      "Iteration 99, inertia 78369.875.\n",
      "Iteration 100, inertia 78369.7734375.\n",
      "Iteration 101, inertia 78369.6640625.\n",
      "Iteration 102, inertia 78369.5546875.\n",
      "Iteration 103, inertia 78369.4765625.\n",
      "Iteration 104, inertia 78369.375.\n",
      "Iteration 105, inertia 78369.21875.\n",
      "Iteration 106, inertia 78369.046875.\n",
      "Iteration 107, inertia 78368.859375.\n",
      "Iteration 108, inertia 78368.703125.\n",
      "Iteration 109, inertia 78368.5234375.\n",
      "Iteration 110, inertia 78368.3671875.\n",
      "Iteration 111, inertia 78368.1796875.\n",
      "Iteration 112, inertia 78368.046875.\n",
      "Iteration 113, inertia 78367.8359375.\n",
      "Iteration 114, inertia 78367.6875.\n",
      "Iteration 115, inertia 78367.515625.\n",
      "Iteration 116, inertia 78367.359375.\n",
      "Iteration 117, inertia 78367.171875.\n",
      "Iteration 118, inertia 78367.0.\n",
      "Iteration 119, inertia 78366.8203125.\n",
      "Iteration 120, inertia 78366.6015625.\n",
      "Iteration 121, inertia 78366.40625.\n",
      "Iteration 122, inertia 78366.25.\n",
      "Iteration 123, inertia 78366.0703125.\n",
      "Iteration 124, inertia 78365.875.\n",
      "Iteration 125, inertia 78365.703125.\n",
      "Iteration 126, inertia 78365.5859375.\n",
      "Iteration 127, inertia 78365.4296875.\n",
      "Iteration 128, inertia 78365.3125.\n",
      "Iteration 129, inertia 78365.203125.\n",
      "Iteration 130, inertia 78365.125.\n",
      "Iteration 131, inertia 78365.046875.\n",
      "Iteration 132, inertia 78364.9140625.\n",
      "Iteration 133, inertia 78364.8203125.\n",
      "Iteration 134, inertia 78364.671875.\n",
      "Iteration 135, inertia 78364.5625.\n",
      "Iteration 136, inertia 78364.4375.\n",
      "Iteration 137, inertia 78364.3203125.\n",
      "Iteration 138, inertia 78364.1875.\n",
      "Iteration 139, inertia 78364.0546875.\n",
      "Iteration 140, inertia 78363.921875.\n",
      "Iteration 141, inertia 78363.796875.\n",
      "Iteration 142, inertia 78363.6484375.\n",
      "Iteration 143, inertia 78363.484375.\n",
      "Iteration 144, inertia 78363.2734375.\n",
      "Iteration 145, inertia 78363.0859375.\n",
      "Iteration 146, inertia 78362.8203125.\n",
      "Iteration 147, inertia 78362.5234375.\n",
      "Iteration 148, inertia 78362.2421875.\n",
      "Iteration 149, inertia 78362.0234375.\n",
      "Iteration 150, inertia 78361.71875.\n",
      "Iteration 151, inertia 78361.4140625.\n",
      "Iteration 152, inertia 78361.1796875.\n",
      "Iteration 153, inertia 78361.0546875.\n",
      "Iteration 154, inertia 78360.8984375.\n",
      "Iteration 155, inertia 78360.828125.\n",
      "Iteration 156, inertia 78360.7265625.\n",
      "Iteration 157, inertia 78360.6328125.\n",
      "Iteration 158, inertia 78360.546875.\n",
      "Iteration 159, inertia 78360.5078125.\n",
      "Iteration 160, inertia 78360.4453125.\n",
      "Iteration 161, inertia 78360.40625.\n",
      "Iteration 162, inertia 78360.3828125.\n",
      "Iteration 163, inertia 78360.3359375.\n",
      "Iteration 164, inertia 78360.2890625.\n",
      "Iteration 165, inertia 78360.25.\n",
      "Iteration 166, inertia 78360.234375.\n",
      "Iteration 167, inertia 78360.21875.\n",
      "Iteration 168, inertia 78360.1953125.\n",
      "Iteration 169, inertia 78360.1796875.\n",
      "Iteration 170, inertia 78360.171875.\n",
      "Iteration 171, inertia 78360.171875.\n",
      "Iteration 172, inertia 78360.1484375.\n",
      "Iteration 173, inertia 78360.15625.\n",
      "Iteration 174, inertia 78360.1484375.\n",
      "Iteration 175, inertia 78360.1484375.\n",
      "Iteration 176, inertia 78360.125.\n",
      "Iteration 177, inertia 78360.1015625.\n",
      "Iteration 178, inertia 78360.1171875.\n",
      "Iteration 179, inertia 78360.109375.\n",
      "Iteration 180, inertia 78360.109375.\n",
      "Iteration 181, inertia 78360.109375.\n",
      "Iteration 182, inertia 78360.09375.\n",
      "Iteration 183, inertia 78360.0703125.\n",
      "Iteration 184, inertia 78360.0859375.\n",
      "Iteration 185, inertia 78360.078125.\n",
      "Iteration 186, inertia 78360.1015625.\n",
      "Converged at iteration 186: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 127416.7421875.\n",
      "Iteration 1, inertia 81812.2265625.\n",
      "Iteration 2, inertia 80314.828125.\n",
      "Iteration 3, inertia 79742.03125.\n",
      "Iteration 4, inertia 79327.65625.\n",
      "Iteration 5, inertia 79071.5078125.\n",
      "Iteration 6, inertia 78895.140625.\n",
      "Iteration 7, inertia 78774.609375.\n",
      "Iteration 8, inertia 78695.3125.\n",
      "Iteration 9, inertia 78638.8046875.\n",
      "Iteration 10, inertia 78597.1015625.\n",
      "Iteration 11, inertia 78564.578125.\n",
      "Iteration 12, inertia 78534.9375.\n",
      "Iteration 13, inertia 78508.390625.\n",
      "Iteration 14, inertia 78486.1796875.\n",
      "Iteration 15, inertia 78469.3515625.\n",
      "Iteration 16, inertia 78455.734375.\n",
      "Iteration 17, inertia 78442.9453125.\n",
      "Iteration 18, inertia 78432.703125.\n",
      "Iteration 19, inertia 78424.4609375.\n",
      "Iteration 20, inertia 78417.734375.\n",
      "Iteration 21, inertia 78411.6875.\n",
      "Iteration 22, inertia 78405.9140625.\n",
      "Iteration 23, inertia 78400.4296875.\n",
      "Iteration 24, inertia 78395.25.\n",
      "Iteration 25, inertia 78390.78125.\n",
      "Iteration 26, inertia 78386.84375.\n",
      "Iteration 27, inertia 78383.140625.\n",
      "Iteration 28, inertia 78379.7734375.\n",
      "Iteration 29, inertia 78376.875.\n",
      "Iteration 30, inertia 78374.390625.\n",
      "Iteration 31, inertia 78372.09375.\n",
      "Iteration 32, inertia 78369.5859375.\n",
      "Iteration 33, inertia 78366.21875.\n",
      "Iteration 34, inertia 78361.796875.\n",
      "Iteration 35, inertia 78357.0234375.\n",
      "Iteration 36, inertia 78354.03125.\n",
      "Iteration 37, inertia 78351.9296875.\n",
      "Iteration 38, inertia 78349.875.\n",
      "Iteration 39, inertia 78348.0625.\n",
      "Iteration 40, inertia 78346.328125.\n",
      "Iteration 41, inertia 78344.4453125.\n",
      "Iteration 42, inertia 78342.2890625.\n",
      "Iteration 43, inertia 78339.796875.\n",
      "Iteration 44, inertia 78337.1328125.\n",
      "Iteration 45, inertia 78333.8515625.\n",
      "Iteration 46, inertia 78330.046875.\n",
      "Iteration 47, inertia 78326.1328125.\n",
      "Iteration 48, inertia 78322.8359375.\n",
      "Iteration 49, inertia 78320.125.\n",
      "Iteration 50, inertia 78317.0234375.\n",
      "Iteration 51, inertia 78313.5703125.\n",
      "Iteration 52, inertia 78310.6171875.\n",
      "Iteration 53, inertia 78308.2734375.\n",
      "Iteration 54, inertia 78306.1328125.\n",
      "Iteration 55, inertia 78304.3671875.\n",
      "Iteration 56, inertia 78302.9140625.\n",
      "Iteration 57, inertia 78301.6953125.\n",
      "Iteration 58, inertia 78300.890625.\n",
      "Iteration 59, inertia 78300.296875.\n",
      "Iteration 60, inertia 78299.734375.\n",
      "Iteration 61, inertia 78299.1796875.\n",
      "Iteration 62, inertia 78298.59375.\n",
      "Iteration 63, inertia 78298.0390625.\n",
      "Iteration 64, inertia 78297.5625.\n",
      "Iteration 65, inertia 78297.078125.\n",
      "Iteration 66, inertia 78296.6640625.\n",
      "Iteration 67, inertia 78296.3203125.\n",
      "Iteration 68, inertia 78296.046875.\n",
      "Iteration 69, inertia 78295.8671875.\n",
      "Iteration 70, inertia 78295.609375.\n",
      "Iteration 71, inertia 78295.4296875.\n",
      "Iteration 72, inertia 78295.2265625.\n",
      "Iteration 73, inertia 78295.078125.\n",
      "Iteration 74, inertia 78294.9609375.\n",
      "Iteration 75, inertia 78294.875.\n",
      "Iteration 76, inertia 78294.8203125.\n",
      "Iteration 77, inertia 78294.7265625.\n",
      "Iteration 78, inertia 78294.671875.\n",
      "Iteration 79, inertia 78294.65625.\n",
      "Iteration 80, inertia 78294.625.\n",
      "Iteration 81, inertia 78294.6015625.\n",
      "Iteration 82, inertia 78294.5859375.\n",
      "Iteration 83, inertia 78294.5703125.\n",
      "Iteration 84, inertia 78294.5234375.\n",
      "Iteration 85, inertia 78294.5234375.\n",
      "Iteration 86, inertia 78294.5078125.\n",
      "Iteration 87, inertia 78294.515625.\n",
      "Iteration 88, inertia 78294.5.\n",
      "Iteration 89, inertia 78294.484375.\n",
      "Iteration 90, inertia 78294.4921875.\n",
      "Iteration 91, inertia 78294.453125.\n",
      "Iteration 92, inertia 78294.4375.\n",
      "Iteration 93, inertia 78294.4453125.\n",
      "Iteration 94, inertia 78294.4296875.\n",
      "Iteration 95, inertia 78294.40625.\n",
      "Iteration 96, inertia 78294.390625.\n",
      "Iteration 97, inertia 78294.375.\n",
      "Iteration 98, inertia 78294.375.\n",
      "Iteration 99, inertia 78294.3515625.\n",
      "Iteration 100, inertia 78294.34375.\n",
      "Iteration 101, inertia 78294.3359375.\n",
      "Iteration 102, inertia 78294.3203125.\n",
      "Iteration 103, inertia 78294.3359375.\n",
      "Iteration 104, inertia 78294.3359375.\n",
      "Iteration 105, inertia 78294.328125.\n",
      "Iteration 106, inertia 78294.3125.\n",
      "Iteration 107, inertia 78294.3203125.\n",
      "Iteration 108, inertia 78294.328125.\n",
      "Iteration 109, inertia 78294.3125.\n",
      "Iteration 110, inertia 78294.3046875.\n",
      "Converged at iteration 110: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=30, max_iter=1000, verbose=1).fit(all_vfeatures)\n",
    "preds = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd625a-49ec-4ee2-a3a9-be5aed13bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_acc(all_clu_label, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7ac05-c81b-4d6d-b9d6-8dded5a49e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./cache/cluster/kmeans-{args.dataset}.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c62d6-a5a2-4de6-94b1-43d8d3cf0c9d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset_name in ['make_entity13', 'imagenet', 'make_entity30', 'make_living17']:\n",
    "    print(f'cluster {dataset_name}')\n",
    "    args.dataset_name = dataset_name\n",
    "    dataset = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_val, seed=0)\n",
    "    loader_val = torch.utils.data.DataLoader(dataset, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "    print('dataset size', len(dataset))\n",
    "    \n",
    "    ### inference\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    all_vfeatures = []\n",
    "    all_clu_label = []\n",
    "    with tqdm(total=len(loader_val)) as pbar:\n",
    "        model.eval()\n",
    "        for idx_batch, batch in enumerate(loader_val):\n",
    "            images, label_voc, label_clu, idx_img = batch\n",
    "            images = images.to(args.device)\n",
    "            with amp_autocast():\n",
    "                with torch.no_grad():\n",
    "                    logits = model.visual.extract_features(images)\n",
    "                    logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                    all_vfeatures.append(logits.cpu().numpy())\n",
    "                    all_clu_label.append(label_clu.numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "    all_vfeatures = np.concatenate(all_vfeatures)\n",
    "    all_clu_label = np.concatenate(all_clu_label)\n",
    "    \n",
    "    \n",
    "    # K = dataset.num_classes\n",
    "    K = 2000\n",
    "    kmeans = MiniBatchKMeans(n_clusters=K, batch_size=2048, \n",
    "                             random_state=0, n_init=10, max_iter=500, verbose=2).fit(all_vfeatures)\n",
    "    preds = kmeans.labels_\n",
    "    print(cluster_acc(all_clu_label, preds))\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=K, random_state=0, n_init=3, max_iter=500, verbose=2).fit(all_vfeatures)\n",
    "    preds = kmeans.labels_\n",
    "    print(cluster_acc(all_clu_label, preds))\n",
    "    np.save(f'./cache/cluster/kmeans-{args.dataset_name}-2k.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7586ee-6282-4a15-8375-bf77fc869308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(vec_count.topk(k=voc_beta*dataset.num_classes).values.cpu().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381a3c8-f1a3-429f-b4b6-13a312c5edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d205833-451c-4539-b20f-227ed64045d9",
   "metadata": {},
   "source": [
    "upperbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57bc5cb-e5af-4528-aabb-a6854c51db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "mask = torch.zeros(classifier.size(0), device=args.device)\n",
    "mapping_classifier = torch.tensor(sorted(set(dataset.labels)), device=args.device)\n",
    "mask = torch.scatter(mask, 0, mapping_classifier, 1)\n",
    "classifier = classifier[mask.bool()]\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "\n",
    "all_pred_voc = torch.gather(mapping_classifier.cpu(), 0, all_pred_voc)\n",
    "\n",
    "print(f'acc={(all_pred_voc == all_gt_voc).float().mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52897615-fcd6-4c26-ac22-b75be99ff3a1",
   "metadata": {},
   "source": [
    "hierarchy accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef9118-91fc-47ad-9de9-a44437a83c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "\n",
    "all_pred_voc = []\n",
    "all_gt_voc = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                all_pred_voc.append(prob.argmax(dim=-1).cpu())\n",
    "                # label_voc = torch.tensor(list(map(lambda x: vocab.mapping_names_idx[x], label_voc)))\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9519a02-6a92-4b2d-b350-05d5f4d83c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_i2gi = vocab.mapping_idx_global_idx\n",
    "isin = lambda x, y: np.array([xx in y for xx in x])\n",
    "all_pred_hier = []\n",
    "for i in range(len(all_gt_voc)):\n",
    "    cond1 = isin(mapping_i2gi[all_gt_voc[i].item()], reduce(lambda x,y: x|y, [parents[p] for p in mapping_i2gi[all_pred_voc[i].item()]])).any()\n",
    "    cond2 = isin(mapping_i2gi[all_pred_voc[i].item()], reduce(lambda x,y: x|y, [parents[p] for p in mapping_i2gi[all_gt_voc[i].item()]])).any()\n",
    "    pred_hier = cond1 | cond2\n",
    "    all_pred_hier.append(pred_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b82639-e18e-4da9-a0b7-36f32dfc184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_pred_hier).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a45ccd-0b81-4faa-833d-27d9ee4e4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_pred_voc==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d1a80-6c76-41d2-97d3-3863a16490c0",
   "metadata": {},
   "source": [
    "KNN performance investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb43e8-525b-4d08-a46d-96f8c19d655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "# classifier = F.normalize(classifier, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deebbc7-f951-40f6-bbe2-19ea353341c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = classifier@classifier.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ab39c-83e4-4414-9282-85f5db9b1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "topk_ind = similarity.topk(k=K+1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5457c3-7439-43d6-88f0-56949b23ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(map(lambda x: list(map(lambda y: classnames[y], x)), topk_ind.cpu().numpy().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040aa32-b9f8-49d0-b48f-14063c0d670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c532a-61b0-48f6-a987-22a327fa77bc",
   "metadata": {},
   "source": [
    "similarity inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c86cca-e05e-435f-894c-1052d2f0841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "all_sim_topk = []\n",
    "all_sim_topk_val = []\n",
    "all_gt_label_voc = []\n",
    "all_features = []\n",
    "with tqdm(total=len(loader_val)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_val):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = logits @ classifier.t()\n",
    "                sim_topk = similarity.topk(k=10)\n",
    "                all_sim_topk.append(sim_topk.indices.cpu())\n",
    "                all_sim_topk_val.append(sim_topk.values.cpu())\n",
    "                all_gt_label_voc.append(label_voc)\n",
    "                all_features.append(logits.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_sim_topk = torch.cat(all_sim_topk, dim=0)\n",
    "all_gt_label_voc = torch.cat(all_gt_label_voc, dim=0)\n",
    "all_sim_topk_val = torch.cat(all_sim_topk_val, dim=0)\n",
    "all_features = np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b28b9-6b27-47d4-a840-1c2dacad56cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea0560-9495-459f-9295-40090979c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu-{args.dataset_name}-train-{arch}.npy', pred_clu)\n",
    "# pred_clu = np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ee39-4248-4c84-95f9-61f6e8baf7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features = all_features.to(args.device)\n",
    "sim = all_features@all_features.t()\n",
    "np.save(f'./knn_ind-{args.dataset_name}-train-{arch}.npy', sim.topk(k=300).indices.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e20167-6022-4ee0-8e95-ddd93dde38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72e71e-2619-4f46-884d-630ce5ca068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "cluster_acc(pred_clu, all_labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d59f21-7e3a-4d8d-9d92-564e0b290224",
   "metadata": {},
   "source": [
    "CLIP clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b1f66-2810-4e10-a66f-1e5e8ef58010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clu = np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bf01e-46bb-4aaa-ae59-260bfc32baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CLIP clustering acc \"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_idx_img = []\n",
    "model.eval()\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    for batch in loader_f:\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images)\n",
    "                features = F.normalize(features, dim=-1).float()\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "        all_features.append(features.detach().cpu())\n",
    "        all_labels.append(label_clu)\n",
    "        all_idx_img.append(idx_img)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "all_idx_img = torch.cat(all_idx_img, dim=0)\n",
    "\n",
    "# cluster_acc(pred_clu, all_labels.numpy())\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(all_labels.unique()), n_init=100, max_iter=1000, random_state=43)\n",
    "pred_clu = kmeans.fit_predict(all_features.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d553326-8716-4ad4-b895-c507c99fb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./pred_clu-{args.dataset_name}-train-clip.npy', pred_clu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f4feb-e99d-4d4c-aa7c-5c4a801f030c",
   "metadata": {},
   "source": [
    "Cluster navigation (depends on clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de786f98-6bb8-4643-b070-de7187cd46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_labels = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cdc6a-707c-45cf-8bda-d76f80bed2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "### per predicted-cluster voting\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "\n",
    "pred_kmeans_t = pred_kmeans\n",
    "# for it in range(10):\n",
    "#     print(f'iteration {it}')\n",
    "\n",
    "# cluster agg\n",
    "all_clu_pred = []\n",
    "for i in range(len(all_gt_voc.unique())):\n",
    "    selected = (pred_kmeans==i)\n",
    "    clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "    all_clu_pred.append(clu_pred)\n",
    "all_clu_pred = torch.stack(all_clu_pred, dim=0)\n",
    "\n",
    "# linear assignment\n",
    "print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "\n",
    "cost_mat = all_clu_pred.cpu().numpy()\n",
    "res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "label_kmeans_voc = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "\n",
    "print('instance label acc::', (label_kmeans_voc==all_gt_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4979ece-f872-406c-ad93-72b2970f0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_labels = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f56c2b-1baf-434d-a4fe-7a35fc495272",
   "metadata": {},
   "outputs": [],
   "source": [
    "### subset vocab\n",
    "col_subset = all_clu_pred.nonzero()[:, 1]\n",
    "col_subset = col_subset.unique().sort().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788bf5fc-fcbb-49df-a2c0-a6155255e9c3",
   "metadata": {},
   "source": [
    "KNN investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52721295-8e5c-4baf-9e70-0434168d1867",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "neighborhood_size = np.arange(5, 500, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    topk_res = similarity.topk(k=K+1)\n",
    "    topk_ind = topk_res.indices[:, 1:]\n",
    "    topk_match = torch.gather(label_match, 1, topk_ind)\n",
    "    topk_acc = topk_match.float().mean(dim=-1).mean()\n",
    "    print(f'K={K} acc={topk_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519d41-07c4-44e5-b649-5b170e3e1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_util_package.graph import compute_consensus_on_features\n",
    "neighborhood_size = np.arange(5, 50, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    _, _, pred_affinity = compute_consensus_on_features(all_features, k=K+1, q=0.8)\n",
    "    acc = ((pred_affinity & label_match).float().sum(1)/(pred_affinity.float().sum(1)+1e-10)).mean()\n",
    "    n_nn = pred_affinity.float().sum(1).mean()\n",
    "    print(f'K={K} acc={acc} n_nn={n_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970ede6-7bd9-408d-a903-30200acf6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_size = np.arange(5, 50, 5)\n",
    "similarity = all_features@all_features.T\n",
    "label_match = all_labels.view(-1, 1)==all_labels.view(1, -1)\n",
    "for K in neighborhood_size:\n",
    "    _, _, pred_affinity = compute_consensus_on_features(all_features, k=K+1, q=0.5)\n",
    "    acc = ((pred_affinity & label_match).float().sum(1)/(pred_affinity.float().sum(1)+1e-10)).mean()\n",
    "    n_nn = pred_affinity.float().sum(1).mean()\n",
    "    print(f'K={K} acc={acc} n_nn={n_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a399a6e-531d-41bd-b16e-c8b0b40f9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" KNN matrix output \"\"\"\n",
    "neighborhood_size = 315\n",
    "similarity = all_features@all_features.T\n",
    "label_match = (all_labels.view(-1, 1)==all_labels.view(1, -1))\n",
    "K = neighborhood_size\n",
    "topk_res = similarity.topk(k=K+1)\n",
    "topk_ind = topk_res.indices\n",
    "\n",
    "torch.save(topk_ind, f'./cache/{args.dataset_name}-clip-knn-{neighborhood_size}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eeb7ed-a204-471f-bdf1-4472658c6669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_normalize = lambda x: x/x[:, 0].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc8f4d-0f9e-4300-a049-7d9da7a6b6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 3\n",
    "weight_normalize(instance_weight)[(instance_pred[:, 0]==all_gt_voc)][:, idx].mean(), \\\n",
    "weight_normalize(instance_weight)[(instance_pred[:, idx]==all_gt_voc)][:, idx].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ed802-7bfe-4264-b0a6-3e2568fdfc09",
   "metadata": {},
   "source": [
    "spatial features reweighting and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6086c-c4f7-40aa-adf7-8456702d768c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" visual reranking computation based on spatial features \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "instance_topk_voclabel_by_scd = all_clu_pred.topk(k=5).indices.index_select(0, record_pred_kmeans_t)\n",
    "all_spatial_label_pred = []\n",
    "all_label_voc = []\n",
    "all_label_match_rerank = []\n",
    "all_label_pred = []\n",
    "all_rerank_pred_voc = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images, return_spatial=True)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "                spatial_similarity = model.logit_scale.exp() * (features @ classifier[instance_topk_voclabel_by_scd[idx_img]].permute(0,2,1))\n",
    "                spatial_label_pred = spatial_similarity[:, 1:, :].topk(k=10, dim=1).values.mean(dim=1).argmax(dim=-1)\n",
    "                all_spatial_label_pred.append(spatial_label_pred.cpu())\n",
    "                all_label_voc.append(label_voc)\n",
    "                \n",
    "                ### global-spatial reranking\n",
    "                # global_label_attn = spatial_similarity[:, 0, :].softmax(dim=-1)\n",
    "                # global_spatial_mixed_sim_after_scaling = (spatial_similarity[:, 0, :].unsqueeze(1)*spatial_similarity[:, 1:, :])/100\n",
    "                # topk_spatial_sim_ind = spatial_similarity[:, 1:, :].topk(k=10, dim=1).indices\n",
    "                # spatial_label_attn = torch.gather(global_spatial_mixed_sim_after_scaling , 1, topk_spatial_sim_ind ).mean(dim=1).softmax(dim=-1)\n",
    "                # ind_increment = torch.arange(idx_img.size(0), device=args.device)\n",
    "                # global_spatial_mixed_sim_argmax = (global_label_attn.pow(0.75)*spatial_label_attn.pow(0.25)).argmax(dim=-1)\n",
    "                # GSRerank_pred_voc = instance_topk_voclabel_by_scd[idx_img][ind_increment, global_spatial_mixed_sim_argmax]\n",
    "                # label_match_rerank = GSRerank_pred_voc==label_voc\n",
    "                ### global-attention based spatial voting\n",
    "                global_spatial_attn = features[:, 0, :].unsqueeze(1) @ features[:, 1:, :].permute(0,2,1)\n",
    "                topk_spatial_ind = global_spatial_attn.topk(k=10).indices\n",
    "                topk_spatial_features = torch.gather(features[:, 1:, :], 1, topk_spatial_ind)\n",
    "                sim_topk_spatial_features = \\\n",
    "                    model.logit_scale.exp() * (topk_spatial_features @ classifier.unsqueeze(0).permute(0,2,1))\n",
    "                \n",
    "                \n",
    "            all_label_match_rerank.append(label_match_rerank.cpu())\n",
    "            all_label_pred.append(global_label_attn.argmax(dim=-1).cpu())\n",
    "            all_rerank_pred_voc.append(GSRerank_pred_voc.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_spatial_label_pred = torch.cat(all_spatial_label_pred, dim=0)\n",
    "all_label_voc = torch.cat(all_label_voc, dim=0)\n",
    "all_label_match_rerank = torch.cat(all_label_match_rerank, dim=0)\n",
    "all_label_pred = torch.cat(all_label_pred, dim=0)\n",
    "all_rerank_pred_voc = torch.cat(all_rerank_pred_voc, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684b3fc-7acd-4294-9af4-b081250a0fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'GSReranked instance Acc:: all_label_match_rerank={all_label_match_rerank.float().mean()}')\n",
    "print(f'SCD:: instance_topk_voclabel_by_scd={(instance_topk_voclabel_by_scd[:, 0]==all_label_voc).float().mean()}')\n",
    "print(f'GSR missing label:: N={len(set(all_gt_voc.unique().cpu().numpy()) - set(all_rerank_pred_voc.unique().cpu().numpy()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f391e4-e326-41dd-9134-a2699e2eba6e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" visual spatial features \n",
    "- KNN difference between CLS and tokens\n",
    "\"\"\"\n",
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "instance_topk_voclabel_by_scd = all_clu_pred.topk(k=5).indices.index_select(0, record_pred_kmeans_t)\n",
    "\n",
    "method = ['cls-spatial-voting', \n",
    "          'cls-spatial-classifier-similarity-inspect',\n",
    "          'cls'][1]\n",
    "all_global_spatial_features = []\n",
    "all_labels_voc_gt = []\n",
    "all_scdknn_classifier_features = []\n",
    "all_entire_spatial_voting = []\n",
    "all_all_voting_voc_ind = []\n",
    "all_pred_voc_label = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images, return_spatial=True)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                if method == 'cls-spatial-voting':\n",
    "                    # scdknn_classifier_features = classifier[instance_topk_voclabel_by_scd[idx_img]]\n",
    "                    cls_spatial_sim = (features[:, 0, :].unsqueeze(1) @ features[:, 1:, :].permute(0,2,1))\n",
    "                    cls_knn_token = cls_spatial_sim.topk(k=10).indices\n",
    "                    token_similarity = model.logit_scale.exp() * (features @ classifier.unsqueeze(0).permute(0,2,1))\n",
    "                    knn_token = token_similarity.topk(k=5).indices\n",
    "                    voting_voc_ind = torch.gather(knn_token, 1, cls_knn_token.permute(0,2,1).repeat(1,1,5)).flatten(1)\n",
    "                    all_voting_voc_ind = []\n",
    "                    for i in range(voting_voc_ind.size(0)):\n",
    "                        val, ind = voting_voc_ind[i].unique(return_counts=True)\n",
    "                        all_voting_voc_ind.append(val[ind.topk(k=5).indices].cpu())\n",
    "                    all_voting_voc_ind = torch.stack(all_voting_voc_ind, dim=0)\n",
    "                    all_all_voting_voc_ind.append(all_voting_voc_ind)\n",
    "                elif method == 'cls-spatial-classifier-similarity-inspect': ### corrected, consider projection head\n",
    "                    n_similar_token = 20\n",
    "                    n_vote = 5\n",
    "                    token_similarity = model.logit_scale.exp() * (features @ classifier.unsqueeze(0).permute(0,2,1)) ### B x L+1 x V\n",
    "                    knn_token = token_similarity.topk(k=n_vote).indices ### B x L+1 x n_vote\n",
    "                    cls_spatial_sim = (features[:, 0, :].unsqueeze(1) @ features[:, 1:, :].permute(0,2,1))\n",
    "                    cls_knn_token = cls_spatial_sim.topk(k=n_similar_token).indices + 1 ### B x 1 x n_similar_token\n",
    "                    # knn_token.gather(1, cls_knn_token)\n",
    "                    all_voting_voc_ind = []\n",
    "                    for i in range(idx_img.size(0)):\n",
    "                        val, count = knn_token[i, cls_knn_token[i, 0, :], :].flatten().unique(return_counts=True) ### n_similar_token x n_vote\n",
    "                        all_voting_voc_ind.append(val[count.topk(k=n_vote).indices].cpu())\n",
    "                    all_voting_voc_ind = torch.stack(all_voting_voc_ind, dim=0)\n",
    "                    all_all_voting_voc_ind.append(all_voting_voc_ind)\n",
    "                    \n",
    "                    all_pred_voc_label.append((features[:, 0, :]@classifier.t()).argmax(dim=-1).cpu())\n",
    "                    \n",
    "                elif method == 'cls':\n",
    "                    similarity = features[:, 0, :]@classifier.t()\n",
    "                    all_pred_voc_label.append(similarity.argmax(dim=-1).cpu())\n",
    "                # entire_spatial_voting = (features[:, 0:, :] @ classifier.unsqueeze(0).permute(0,2,1)).topk(k=5).indices\n",
    "                # torch.scatter_add(p, 1, entire_spatial_voting.flatten(1), torch.ones_like(entire_spatial_voting.flatten(1)).float()).argmax(dim=-1)\n",
    "#             all_global_spatial_features.append(features.cpu())\n",
    "            all_labels_voc_gt.append(label_voc)\n",
    "#             all_scdknn_classifier_features.append(scdknn_classifier_features.cpu().numpy())\n",
    "#             # all_entire_spatial_voting.append(entire_spatial_voting.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "# all_global_spatial_features = torch.cat(all_global_spatial_features, dim=0)\n",
    "all_labels_voc_gt = torch.cat(all_labels_voc_gt, dim=0)\n",
    "# all_scdknn_classifier_features = np.concatenate(all_scdknn_classifier_features)\n",
    "# # all_entire_spatial_voting = torch.cat(all_entire_spatial_voting, dim=0)\n",
    "all_all_voting_voc_ind = torch.cat(all_all_voting_voc_ind, dim=0)\n",
    "all_pred_voc_label = torch.cat(all_pred_voc_label, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e331be-480d-4c28-b77c-11acc0524cc0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" offline clustering for visual spatial features \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "cluster_kmeans = {}\n",
    "cluster_spectral = {}\n",
    "n_clusters = 10\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='precomputed')\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images, return_spatial=True)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "        batch_size = features.size(0)\n",
    "        for i in range(batch_size):\n",
    "            self_sim = features[i]@features[i].t()\n",
    "            pred_spectral = spectral.fit_predict(self_sim.cpu().numpy())\n",
    "            pred_kmeans = kmeans.fit_predict(features[i].cpu().numpy())\n",
    "        cluster_kmeans[idx_img[i]] = pred_kmeans\n",
    "        cluster_spectral[idx_img[i]] = pred_spectral\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780607e-52bb-43cc-88e4-91f1bccdbb8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870300b9-1da8-432a-a39c-a61999920035",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" clustering performance comparison \"\"\"\n",
    "n_clusters = 10\n",
    "idx = np.random.randint(low=0, high=510, size=1)\n",
    "spectral = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', n_init=30)\n",
    "begin = time.time()\n",
    "pred_spectral = spectral.fit_predict(self_sim[idx].cpu().numpy())\n",
    "end = time.time()\n",
    "print(f'time={end-begin}')\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "begin = time.time()\n",
    "pred_kmeans = kmeans.fit_predict(features[idx].cpu().numpy())\n",
    "end = time.time()\n",
    "print(f'time={end-begin}')\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0)\n",
    "tsne_features_tr = tsne.fit_transform(features[idx].cpu().numpy())\n",
    "\n",
    "plt.figure(dpi=64)\n",
    "plt.scatter(x=tsne_features_tr[:, 0], y=tsne_features_tr[:, 1], c=pred_spectral, s=5, alpha=0.6) ### SPATIAL\n",
    "plt.scatter(x=tsne_features_tr[pred_spectral==pred_spectral[0], 0], y=tsne_features_tr[pred_spectral==pred_spectral[0], 1], c='r', s=5) ### CLS\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=64)\n",
    "plt.scatter(x=tsne_features_tr[:, 0], y=tsne_features_tr[:, 1], c=pred_kmeans, s=5, alpha=0.6) ### SPATIAL\n",
    "plt.scatter(x=tsne_features_tr[pred_kmeans==pred_kmeans[0], 0], y=tsne_features_tr[pred_kmeans==pred_kmeans[0], 1], c='r', s=5) ### CLS\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eec163-3209-4dc2-8877-9a046f9f54e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "self_sim.mean(dim=[1,2]).mean(), self_sim.std(dim=[1,2]).mean(), self_sim_classifier.mean(dim=[0,1]), self_sim_classifier.std(dim=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257ce01-470a-4fca-ad9d-575e2933d6af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = torch.zeros([512, classifier.size(0)], device=args.device)\n",
    "torch.scatter_add(p, 1, entire_spatial_voting.flatten(1), torch.ones_like(entire_spatial_voting.flatten(1)).float()).argmax(dim=-1), entire_spatial_voting.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10308782-81f6-4b32-ab51-6866c79676e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### dimensionality reduction with TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "np.random.seed(2)\n",
    "idx = np.random.randint(low=0, high=len(all_global_spatial_features), size=[1])[0]\n",
    "image_features = all_global_spatial_features[idx]\n",
    "knn_classifier_features = torch.from_numpy(all_scdknn_classifier_features[idx])\n",
    "Nimg = image_features.size(0)\n",
    "tsne_features_input = torch.cat([image_features, knn_classifier_features], dim=0)\n",
    "tsne = TSNE(n_components=2, verbose=2)\n",
    "tsne_features_tr = tsne.fit_transform(tsne_features_input.numpy())\n",
    "image_features_tr = tsne_features_tr[:Nimg]\n",
    "knn_classifier_features_tr = tsne_features_tr[Nimg:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb99d0-1961-450a-b23d-9d01776d773c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=128)\n",
    "plt.scatter(x=image_features_tr[1:, 0], y=image_features_tr[1:, 1], c='g', s=5, alpha=0.6) ### SPATIAL\n",
    "plt.scatter(x=image_features_tr[0, 0], y=image_features_tr[0, 1], c='r', s=5) ### CLS\n",
    "plt.scatter(x=knn_classifier_features_tr[:, 0], y=knn_classifier_features_tr[:, 1], c='b', s=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e0118-2026-4af4-865a-797ab80e1dd7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "idx = np.random.randint(low=0, high=len(all_global_spatial_features), size=[1])[0]\n",
    "image_features = all_global_spatial_features[idx]\n",
    "knn_classifier_features = torch.from_numpy(all_scdknn_classifier_features[idx])\n",
    "\n",
    "Nimg = image_features.size(0)\n",
    "N2 = knn_classifier_features.size(0)\n",
    "all_knn_classifier_features = classifier[(image_features.to(args.device) @ classifier.t()).topk(k=5).indices.flatten().unique()].cpu()\n",
    "tsne_features_input = torch.cat([image_features, knn_classifier_features, all_knn_classifier_features], dim=0)\n",
    "tsne = TSNE(n_components=2, verbose=2)\n",
    "tsne_features_tr = tsne.fit_transform(tsne_features_input.numpy())\n",
    "image_features_tr = tsne_features_tr[:Nimg]\n",
    "knn_classifier_features_tr = tsne_features_tr[Nimg:Nimg+N2]\n",
    "all_knn_classifier_features_tr = tsne_features_tr[Nimg+N2:]\n",
    "\n",
    "plt.figure(dpi=128)\n",
    "plt.scatter(x=image_features_tr[1:, 0], y=image_features_tr[1:, 1], c='g', s=5, alpha=0.6) ### SPATIAL\n",
    "plt.scatter(x=image_features_tr[0, 0], y=image_features_tr[0, 1], c='r', s=5) ### CLS\n",
    "plt.scatter(x=all_knn_classifier_features_tr[:, 0], y=all_knn_classifier_features_tr[:, 1], c='y', s=5) ### spatial KNN\n",
    "plt.scatter(x=knn_classifier_features_tr[:, 0], y=knn_classifier_features_tr[:, 1], c='b', s=5) ### cluster KNN\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5ba19-3fd9-4329-86aa-8b38eaab0c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatial_knn, spatial_knn_counts = (image_features.to(args.device) @ classifier.t()).topk(k=5).indices[:, 0].unique(return_counts=True)\n",
    "(spatial_knn==all_labels_voc_gt[idx]).nonzero(), spatial_knn_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f2346-76ba-4b23-9ad5-75e81ff32394",
   "metadata": {},
   "source": [
    "SCD with shrinked vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e6634-c549-416e-89ff-34041414068b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" SCD with shrinked vocab \"\"\"\n",
    "classifier_selected = None\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                ### mapping @selected to vocab ind\n",
    "                B, C = prob_topk_ind.shape\n",
    "                prob_topk_ind = classifier_selected[prob_topk_ind.view(-1)].view(B, C)\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "# pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino-dino_stage1.npy'))\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d409fc-0177-4e54-b270-7f7d0f452870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_all_clu_pred = all_clu_pred\n",
    "# len(set_gt - set(final_all_clu_pred.topk(k=2).indices.flatten().unique().numpy().tolist()))\n",
    "\n",
    "# len(set_gt - set(all_clu_pred.topk(k=3).indices.flatten().unique().numpy().tolist()))\n",
    "\n",
    "# select_correct = (cluster_ind_voc.cpu()==all_gt_voc)\n",
    "\n",
    "# all_topk_val = torch.from_numpy(all_topk_val)#[select_correct]\n",
    "# prob_all_topk_val = torch.cat([all_topk_val, 1-all_topk_val.sum(dim=-1, keepdim=True)], dim=-1)\n",
    "\n",
    "# ent = - (prob_all_topk_val * (prob_all_topk_val+1e-30).log()).sum(dim=-1)\n",
    "\n",
    "# # import seaborn as sns\n",
    "# # sns.distplot(prob_all_topk_val[select_correct, 0], bins=100)\n",
    "# # sns.distplot(prob_all_topk_val[~select_correct, 0], bins=100)\n",
    "# # sns.scatterplot(x=prob_all_topk_val[:, 0], y=select_correct.float(), s=3, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9395f6-9497-4a0c-90aa-e85c2f26e81d",
   "metadata": {},
   "source": [
    "collect variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfb1e2-f2da-4787-b872-d08922d43bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "all_topk_val = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                topk_res = prob.topk(k=prob_k, dim=-1)\n",
    "                prob_topk_ind = topk_res.indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_topk_val.append(topk_res.values.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_topk_val = np.concatenate(all_topk_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc17b14-894c-4cab-bf09-1cd6e4636a09",
   "metadata": {},
   "source": [
    "confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc944ec3-21e1-4577-a721-d969514552e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# classifier = get_classifier(args)\n",
    "# classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "# args.num_voc = classifier.size(0)\n",
    "# amp_autocast = torch.cuda.amp.autocast\n",
    "# ### collect variables\n",
    "# prob_k = 5\n",
    "# all_topk_voc = []\n",
    "# all_gt_voc = []\n",
    "# all_label_clu = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     if hasattr(model, 'eval'):\n",
    "#         model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "#         images = images.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = 100 * logits @ classifier.t()\n",
    "#                 prob = similarity.softmax(-1)\n",
    "#                 prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "#                 all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "#                 all_gt_voc.append(label_voc)\n",
    "#                 all_label_clu.append(label_clu)\n",
    "#         pbar.update(1)\n",
    "\n",
    "# all_topk_voc = np.concatenate(all_topk_voc)\n",
    "# all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "# all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "use_confidence = True\n",
    "th_confidence = 0.5\n",
    "pred_kmeans = torch.from_numpy(np.load(f'/home/sheng/OSZSL/ipynb/pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "\n",
    "if use_confidence:\n",
    "    # ### SSL feature extraction\n",
    "    # ssl_prototypes = torch.zeros([pred_kmeans.unique().size(0), 768], device=args.device, dtype=torch.float64) ### C x D\n",
    "    # ssl_counter = torch.zeros(pred_kmeans.unique().size(0))\n",
    "    # with tqdm(total=len(loader_f)) as pbar:\n",
    "    #     modelf.eval()\n",
    "    #     modelf.to(args.device)\n",
    "    #     for idx_batch, batch in enumerate(loader_f):\n",
    "    #         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "    #         images = images.to(args.device)\n",
    "    #         with torch.no_grad():\n",
    "    #             features = modelf(images.float())\n",
    "    #             features = F.normalize(features, dim=-1)\n",
    "    #             for p in range(idx_img.size(0)):\n",
    "    #                 ssl_prototypes[pred_kmeans[p].long()] += features.to(torch.float64)[p]\n",
    "    #             # ssl_prototypes = torch.scatter_add(ssl_prototypes, 0, pred_kmeans[idx_img.long()].to(args.device).long(), features.to(torch.float64))\n",
    "    #             counter_voc_ind, counter_val = pred_kmeans[idx_img].unique(return_counts=True)\n",
    "    #             ssl_counter[counter_voc_ind.long()] += counter_val\n",
    "    #         pbar.update(1)\n",
    "    # ssl_prototypes = ssl_prototypes/ssl_counter.to(args.device).unsqueeze(-1)\n",
    "    # ssl_prototypes = F.normalize(ssl_prototypes, dim=-1)\n",
    "    # ### select confident instances\n",
    "    # all_prob = []\n",
    "    # all_sim = []\n",
    "    # with tqdm(total=len(loader_f)) as pbar:\n",
    "    #     modelf.eval()\n",
    "    #     modelf.to(args.device)\n",
    "    #     for idx_batch, batch in enumerate(loader_f):\n",
    "    #         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "    #         images = images.to(args.device)\n",
    "    #         with torch.no_grad():\n",
    "    #             features = modelf(images)\n",
    "    #             features = F.normalize(features, dim=-1)\n",
    "    #             sim = features@ssl_prototypes.float().t()\n",
    "    #             prob = (sim/1.0).amax(dim=-1)\n",
    "    #             all_prob.append(prob.cpu())\n",
    "    #             all_sim.append(sim.cpu())\n",
    "    #         pbar.update(1)\n",
    "    # all_prob = torch.cat(all_prob, dim=0)\n",
    "    # all_sim = torch.cat(all_sim, dim=0)\n",
    "    ### confidence thresholding\n",
    "    q = np.quantile(all_prob.numpy(), q=0.5)\n",
    "    selected = (all_prob>q)\n",
    "    ### computing\n",
    "    pred_kmeans_t = pred_kmeans[selected]\n",
    "    for t in range(3):\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc[selected], voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc[selected])\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, \n",
    "                                                                  all_prob=None, instance_selected=selected)\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu[selected].numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "else:\n",
    "    pred_kmeans_t = pred_kmeans\n",
    "    for t in range(3):\n",
    "        all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "        label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "        pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "        set_pred = set(res_ass[1].tolist())\n",
    "        set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "        print('missing label::', len(set_gt - set_pred))\n",
    "        print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cac48-7b09-47ac-b5f6-d67a19966a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" inspect cluster topk assigned classes \"\"\"\n",
    "# topk_cluster_label = all_clu_pred.topk(k=5).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc40ee62-0996-46bf-8e3b-40d9b0fc1997",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # pred_kmeans_t = pred_kmeans\n",
    "# # for t in range(5):\n",
    "# #     all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t, all_topk_voc)\n",
    "# #     label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "# #     pred_kmeans_t = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args, all_prob=None)\n",
    "# #     set_pred = set(res_ass[1].tolist())\n",
    "# #     set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "# #     print('missing label::', len(set_gt - set_pred))\n",
    "# #     print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "\n",
    "\n",
    "# \"\"\" get confident prediction \"\"\"\n",
    "# th = 0.5\n",
    "# amp_autocast = torch.cuda.amp.autocast\n",
    "# label_voc_kmeans_t = label_voc_kmeans_t.to(args.device)\n",
    "# cluster_ind = []\n",
    "# selected_ind = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     if hasattr(model, 'eval'):\n",
    "#         model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "#         images = images.to(device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = 100 * logits @ classifier.t()\n",
    "#                 prob = similarity[:, label_voc_kmeans_t].softmax(dim=-1)\n",
    "#                 selected = (prob.amax(dim=-1)>th)\n",
    "#                 selected_ind.append(selected.cpu())\n",
    "#         pbar.update(1)\n",
    "# selected_ind = torch.cat(selected_ind, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# # precision = cluster_acc(y_true=all_label_clu[selected_ind].numpy(), y_pred=pred_kmeans_t[selected_ind].numpy())\n",
    "# # recall = selected_ind.mean()\n",
    "# print(f'confidence selection precision={precision} recall={recall}')\n",
    "\n",
    "# # np.save(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy', pred_kmeans_t.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688f3cb-d754-4c42-bf39-242029dbd35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\" \n",
    "# 1. inverse entropy of prototype\n",
    "\n",
    "# 2. top1 sim of proto-image\n",
    "\n",
    "# 3. top1 sim of image-proto\n",
    "\n",
    "# \"\"\"\n",
    "# # candidate_ind = res_ass[1].unique()\n",
    "# # cls_proto_similarity = torch.zeros([len(dataset_f), candidate_ind.size()])\n",
    "# all_sim_proto_image_pred = []\n",
    "# all_sim_proto_image_gt = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch\n",
    "#         images = images.to(args.device)\n",
    "#         label_voc = label_voc.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "#                 similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "#                 prob = similarity.softmax(dim=-1)\n",
    "#                 all_sim_proto_image_pred.append(similarity[:, prob.argmax(dim=-1)].cpu())\n",
    "#                 all_sim_proto_image_gt.append(similarity[:, label_voc].cpu())\n",
    "#         pbar.update(1)\n",
    "        \n",
    "# all_sim_proto_image_pred = torch.cat(all_sim_proto_image_pred, dim=0)\n",
    "# all_sim_proto_image_gt = torch.cat(all_sim_proto_image_gt, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05625353-4773-4ed7-951c-d8974dfb3b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_sim_proto_image_pred = torch.cat(all_sim_proto_image_pred, dim=0)\n",
    "# all_sim_proto_image_gt = torch.cat(all_sim_proto_image_gt, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "# label_match = all_label_clu.view(-1, 1)@all_label_clu.view(1, -1)\n",
    "# pred_match_init = pred_kmeans.view(-1, 1)@pred_kmeans.view(1, -1)\n",
    "# pred_match = pred_kmeans_t.view(-1, 1)@pred_kmeans_t.view(1, -1)\n",
    "\n",
    "# pred_consensus = (pred_match_init==pred_match) \n",
    "# ((pred_consensus & label_match).float().sum(dim=-1) / (pred_consensus.sum(dim=-1)+1e-20)).mean()\n",
    "\n",
    "# (pred_consensus & label_match).float().sum(dim=-1).bool().float().mean()\n",
    "\n",
    "# all_clu_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d94cc9-d78b-4cc7-b347-e73d7fc03602",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_topk_voc.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "\n",
    "all_topk_voc = np.concatenate(all_topk_voc)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "# ### MCMF\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "all_clu_pred = agg_by_pred_cluster(args, pred_kmeans.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "class_topk_assignment = MCMF_assign_labels(all_clu_pred.cpu().numpy(), K=2)\n",
    "\n",
    "# ### collect variables\n",
    "# prob_k = 5\n",
    "# all_mcmf_rerank_pred = []\n",
    "# all_gt_voc = []\n",
    "# all_label_clu = []\n",
    "# with tqdm(total=len(loader_f)) as pbar:\n",
    "#     if hasattr(model, 'eval'):\n",
    "#         model.eval()\n",
    "#     for idx_batch, batch in enumerate(loader_f):\n",
    "#         images, label_voc, label_clu, idx_img = batch[:4]\n",
    "#         images = images.to(args.device)\n",
    "#         with amp_autocast():\n",
    "#             with torch.no_grad():\n",
    "#                 logits = model.visual(images)\n",
    "#                 logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "#                 valid_classifier_ind = class_topk_assignment[pred_kmeans[idx_img].long()].to(args.device)\n",
    "#                 bb, kk = valid_classifier_ind.size()\n",
    "#                 valid_classifier = classifier[valid_classifier_ind.flatten()].view(bb, kk, -1).permute(0,2,1)\n",
    "                \n",
    "#                 similarity = 100 * logits.unsqueeze(1) @ valid_classifier\n",
    "#                 prob = similarity.softmax(-1)\n",
    "                \n",
    "#                 all_mcmf_rerank_pred.append(valid_classifier_ind[prob.argmax(dim=-1)].cpu().numpy())\n",
    "#                 all_gt_voc.append(label_voc)\n",
    "#                 all_label_clu.append(label_clu)\n",
    "#         pbar.update(1)\n",
    "\n",
    "# all_mcmf_rerank_pred = np.concatenate(all_mcmf_rerank_pred)\n",
    "# all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "# all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "        \n",
    "# instance_assignment_pred = torch.zeros(all_mcmf_rerank_pred.shape[0])\n",
    "# for c in pred_kmeans.unique():\n",
    "#     select = (pred_kmeans==c)\n",
    "#     unique_ind, unique_count = torch.from_numpy(all_mcmf_rerank_pred[select]).unique(return_counts=True)\n",
    "#     instance_assignment_pred[select] = unique_ind[unique_count.argsort()[-1]].item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb5d88-4184-408c-9363-ac2e0d74144f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### class-wise assignment to instance prediction\n",
    "instance_assignment_pred = torch.zeros(pred_kmeans.size(0), class_topk_assignment.size(1)).to(class_topk_assignment.device).long()\n",
    "for c in pred_kmeans.unique():\n",
    "    select = (pred_kmeans==c)\n",
    "    instance_assignment_pred[select] = class_topk_assignment[c].view(-1, class_topk_assignment.size(1))\n",
    "\n",
    "all_mcmf_instance_pred = []\n",
    "all_gt_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                valid_classifier_ind = instance_assignment_pred[idx_img].long().to(args.device)\n",
    "                bb, kk = valid_classifier_ind.size()\n",
    "                valid_classifier = classifier[valid_classifier_ind.flatten()].view(bb, kk, -1).permute(0,2,1)\n",
    "                \n",
    "                similarity = 100 * logits.unsqueeze(1) @ valid_classifier\n",
    "                prob = similarity.softmax(-1)\n",
    "                \n",
    "                all_mcmf_instance_pred.append(valid_classifier_ind[torch.arange(valid_classifier_ind.size(0)), \n",
    "                                                                   prob.argmax(dim=-1).squeeze(-1)].cpu().numpy())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "    \n",
    "all_mcmf_instance_pred = np.concatenate(all_mcmf_instance_pred)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "# pred_kmeans_t = pred_kmeans\n",
    "\n",
    "# history_set_pred = []\n",
    "# for t in range(3):\n",
    "#     record_pred_kmeans_t = pred_kmeans_t\n",
    "#     all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_topk_voc, voc_size=args.num_voc)\n",
    "#     label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_voc)\n",
    "#     pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "#     set_pred = set(res_ass[1].tolist())\n",
    "#     set_gt = set(all_gt_voc.unique().numpy().tolist())\n",
    "#     print('missing label::', len(set_gt - set_pred))\n",
    "#     print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "#     history_set_pred.append(set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06cace-457b-4845-bfbe-78954de01973",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_assignment_pred = torch.zeros(pred_kmeans.size(0)).to(pred_kmeans.device).long()\n",
    "cluster_assignment_argmax = all_clu_pred.argmax(dim=-1)\n",
    "for c in pred_kmeans.unique():\n",
    "    select = (pred_kmeans==c)\n",
    "    instance_assignment_pred[select] = cluster_assignment_argmax[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf2c04-7a24-4da7-9bb5-4a1771cb3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_assignment_pred = torch.zeros(pred_kmeans.size(0), class_topk_assignment.size(1)).to(class_topk_assignment.device).long()\n",
    "for c in pred_kmeans.unique():\n",
    "    select = (pred_kmeans==c)\n",
    "    instance_assignment_pred[select] = class_topk_assignment[c].view(-1, class_topk_assignment.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddf3a3-5c40-411a-94b0-0f8fd1033079",
   "metadata": {},
   "outputs": [],
   "source": [
    "((instance_assignment_pred[:, 0]==all_gt_voc) | (instance_assignment_pred[:, 1]==all_gt_voc)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff5b8b-7e5a-4501-813e-a8b8f49b9b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.from_numpy(all_mcmf_instance_pred)==all_gt_voc).float().mean(), (instance_assignment_pred==all_gt_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3dfd7-b123-48ed-8b03-f0412b0fba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "(instance_assignment_pred==all_gt_voc).float().mean(), len(set(all_gt_voc.unique().numpy()) - set(class_topk_assignment.unique().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43394174-62af-486b-9343-d8b2a4ff98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_topk_assignment.flatten().unique().long(), all_gt_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9f42b-9961-4a24-b172-265501467003",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(all_mcmf_rerank_pred.squeeze(1), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2b726-9910-44ba-bd22-f9232e038cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isin(all_gt_voc.unique(), instance_assignment_pred.unique().long()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa41c0-83b6-4343-96ec-26b7a5abff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ind, unique_count = torch.from_numpy(all_mcmf_rerank_pred[select]).unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc8434-c573-4e2b-8ff8-90f6cb4cac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ind[unique_count.argsort()[-1]].item(), unique_count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8830f-3235-4f83-a063-a332d6236112",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56630fb-9c21-4b54-a78f-06c271abc438",
   "metadata": {},
   "source": [
    "test for MCMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196a123-fccb-4f9a-bc97-88427099915d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "class_topk_assignment = MCMF_assign_labels(all_clu_pred.cpu().numpy(), K=K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9e34a-29d8-4855-998d-c1a06d283db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" overlap with SCD linear assignment prediction \n",
    "NOTE: MCMF is not ordered prediction\n",
    "\"\"\"\n",
    "for i in range(K):\n",
    "    overlap = (all_clu_pred.argmax(dim=-1).cpu()==class_topk_assignment[:, i]).sum()\n",
    "    print(overlap.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f584cab-0721-4b67-9223-2097b76a8cea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('missing label:', len(set_gt - set(class_topk_assignment.unique().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4082f41-7789-4577-b505-2eb33b6788b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" reranking with voting similarity \"\"\"\n",
    "class_topk_assignment_ordered = torch.gather(class_topk_assignment, 1, torch.gather(all_clu_pred, 1, class_topk_assignment).argsort(descending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab171407-10f8-4d1a-b1bd-7068d4ac5556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(class_topk_assignment_ordered[record_pred_kmeans_t][:, 1]==all_gt_voc).float().mean() #, (class_topk_assignment_ordered[record_pred_kmeans_t][:, 0]==cluster_ind_voc.cpu()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde93f02-13a3-498c-ba18-67de692b8e3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "dataset_r = get_datasets_oszsl(args, vocab, is_train=True, transform=None, seed=1)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))\n",
    "    \n",
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3641aa10-d6ea-4434-b4b3-57058a9e1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d793092-23ab-497e-9759-714c91e55e51",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### collect variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac099527-5211-4e10-a50f-1ebc926834c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" topk prediction from CLIP \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "use_norm = True\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True) if use_norm else classifier\n",
    "\n",
    "# initial topK prediction from CLIP\n",
    "prob_k = 5\n",
    "all_topk_voc = []\n",
    "all_gt_voc = []\n",
    "all_prob = []\n",
    "all_max_ind = []\n",
    "all_topk_vocinds = []\n",
    "all_label_clu = []\n",
    "all_topk_vals = []\n",
    "all_topk_inds = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True) if use_norm else logits\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                pred_topk_scattered = torch.scatter(torch.zeros([images.size(0), classifier.size(0)], \n",
    "                                                                device=args.device), 1, prob_topk_ind, 1)\n",
    "                all_topk_voc.append(pred_topk_scattered.cpu())\n",
    "                all_gt_voc.append(label_voc)\n",
    "                all_label_clu.append(label_clu)\n",
    "                all_max_ind.append(prob.argmax(dim=-1).cpu())\n",
    "                all_topk_vocinds.append(prob.topk(k=10, dim=-1).indices.cpu())\n",
    "                \n",
    "                batch_topk_res = prob.topk(k=20, dim=-1)\n",
    "                all_topk_vals.append(batch_topk_res.values.cpu())\n",
    "                all_topk_inds.append(batch_topk_res.indices.cpu())\n",
    "        pbar.update(1)\n",
    "\n",
    "# all_prob = torch.cat(all_prob, dim=0)\n",
    "all_topk_voc = torch.cat(all_topk_voc, dim=0)\n",
    "all_gt_voc = torch.cat(all_gt_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "all_max_ind = torch.cat(all_max_ind, dim=0)\n",
    "all_topk_vocinds = torch.cat(all_topk_vocinds, dim=0)\n",
    "all_topk_vals = torch.cat(all_topk_vals, dim=0)\n",
    "all_topk_inds = torch.cat(all_topk_inds, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387b4a31-0d72-45d8-bcdd-97c69944fecb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### text proto inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159430a-4362-40b9-9181-1c08c1de63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KNN proto analysis\n",
    "text_sim = classifier[all_gt_voc.unique(), :]@classifier.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5c99a-edbd-485e-9082-ab819e27db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_topk = text_sim.topk(k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a41ba7-f29d-4697-ae19-399d222cc970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(np.array([vocab.mapping_idx_names[t.item()] for t in text_topk.indices[:, :].flatten().cpu()]).reshape(text_sim.size(0), -1).tolist(), compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f20703-1fc0-4791-9242-591d6a7b10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "\n",
    "sns.distplot(all_topk_vals[:, 0].cpu().numpy(), bins=200)\n",
    "sns.distplot(all_topk_vals[:, 1].cpu().numpy(), bins=200)\n",
    "sns.distplot(all_topk_vals[:, 2].cpu().numpy(), bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075cda2-8944-48de-b2f5-8e9517de5565",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = (all_topk_vals[:, 0]>0.7)\n",
    "'top1 acc', (all_gt_voc[selected] == all_max_ind[selected]).float().mean(), \\\n",
    "'selected percentile', selected.float().mean(), \\\n",
    "'class diversity', len(set(all_gt_voc.unique().numpy()) - set(all_max_ind[selected].unique().numpy())), \\\n",
    "'topk inclusion', torch.stack([all_topk_vocinds[selected, i]==all_gt_voc[selected] for i in range(all_topk_vocinds.size(1))], dim=1).float().sum(dim=-1).bool().float().mean(), \\\n",
    "'selected sample pred voc size', len(all_max_ind[selected].unique()), \\\n",
    "'average selected instance number per class', selected.sum()/len(all_gt_voc.unique()),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e444ff3-7837-4e21-a5eb-d21c6111fd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" initial clip assignment \"\"\"\n",
    "list(filter(lambda x: x[1]<100, [(i.item(), (all_max_ind==i).sum().item()) for i in all_gt_voc.unique()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15937caa-ab71-4efd-b334-634f64485d73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### text to image entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9165942-c523-4821-9ddf-da7e57c3e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "### get all label and predicted label\n",
    "all_label_voc = []\n",
    "all_pred_voc = []\n",
    "all_label_clu = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                pred_voc = prob.argmax(dim=-1)\n",
    "        all_label_voc.append(label_voc)\n",
    "        all_pred_voc.append(pred_voc.cpu())\n",
    "        all_label_clu.append(label_clu)\n",
    "        pbar.update(1)\n",
    "all_label_voc = torch.cat(all_label_voc, dim=0)\n",
    "all_pred_voc = torch.cat(all_pred_voc, dim=0)\n",
    "all_label_clu = torch.cat(all_label_clu, dim=0)\n",
    "\n",
    "                \n",
    "### compute entropy\n",
    "set_all_label_voc = all_label_voc.unique()\n",
    "set_all_pred_voc = all_pred_voc.unique()\n",
    "selected_classifier_ind = torch.cat([set_all_label_voc, set_all_pred_voc], dim=0).unique()\n",
    "all_similarity = []\n",
    "all_selected_sim = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        label_voc = label_voc.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = model.logit_scale.exp() * logits @ classifier.t()\n",
    "                all_selected_sim.append(similarity[:, selected_classifier_ind].cpu())\n",
    "                all_similarity.append(similarity.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "        \n",
    "all_selected_sim = torch.cat(all_selected_sim, dim=0)\n",
    "all_similarity = np.concatenate(all_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ded316-eb1c-43f6-93a0-b65682197242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans_t = torch.from_numpy(np.load(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy'))\n",
    "classwise_all_selected_sim = []\n",
    "for c in pred_kmeans_t.unique():\n",
    "    subset = (pred_kmeans_t==c)\n",
    "    classwise_all_selected_sim.append(all_selected_sim[subset, :].mean(dim=0).cpu())\n",
    "classwise_all_selected_sim = torch.stack(classwise_all_selected_sim, dim=0)\n",
    "p = classwise_all_selected_sim.float().softmax(dim=0)\n",
    "ent = (-p*(p+1e-10).log2()).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446d82a-1e89-4e68-a08f-2924614cc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "softmax(all_similarity, axis=0)\n",
    "pred_kmeans_t = torch.from_numpy(np.load(f'./pred_clu_clip-{args.dataset_name}-train-{arch}.npy'))\n",
    "classwise_all_sim = []\n",
    "for c in pred_kmeans_t.unique():\n",
    "    subset = (pred_kmeans_t==c)\n",
    "    classwise_all_sim.append(all_selected_sim[subset, :].mean(dim=0).cpu())\n",
    "classwise_all_sim = torch.stack(classwise_all_sim, dim=0)\n",
    "p_all = classwise_all_sim.float().softmax(dim=0)\n",
    "ent_all = (-p_all*(p_all+1e-10).log2()).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f95cb-d240-4f8e-bff3-89d1ee2b0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = torch.nan_to_num(ent)\n",
    "ent_gt = ent[torch.isin(selected_classifier_ind, set_all_label_voc)]\n",
    "ent_pred = ent[torch.isin(selected_classifier_ind, set_all_pred_voc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2bd40-54aa-4cd3-8b70-9818908b212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=128)\n",
    "sns.distplot(ent_gt.numpy(), bins=100)\n",
    "sns.distplot(ent_pred.numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5420f-f0cd-4283-b407-23b711156895",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ent_gt>2).sum(), (ent_gt>3).sum(), \\\n",
    "(ent_pred<2).sum(), (ent_pred<3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423501f-0554-4cfc-8457-5d5b2deb434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ent_gt>2).sum(), (ent_gt>3).sum(), \\\n",
    "(ent_pred<2).sum(), (ent_pred<3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b0a19-6e75-46b1-bc28-c977acbaca11",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### class-wise distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907b0c5-c4ad-47e4-b212-871cf13a6ae9",
   "metadata": {},
   "source": [
    "collect variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb4816-1225-43a5-a842-f42334f28c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" collect variables \"\"\"\n",
    "classifier = get_classifier(args)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# initial topK prediction from CLIP\n",
    "prob_k = 5\n",
    "all_gt_label_voc = []\n",
    "all_gt_label_clu = []\n",
    "all_inst_topk_ind_voc = []\n",
    "all_inst_topk_val_voc = []\n",
    "all_inst_max_pred = []\n",
    "all_img_idx = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk = prob.topk(k=prob_k, dim=-1)\n",
    "                \n",
    "                all_gt_label_voc.append(label_voc)\n",
    "                all_gt_label_clu.append(label_clu)\n",
    "                all_inst_topk_ind_voc.append(prob_topk.indices[:, :prob_k+1].cpu())\n",
    "                all_inst_topk_val_voc.append(prob_topk.values[:, :prob_k+1].cpu())\n",
    "                all_inst_max_pred.append(prob.argmax(dim=-1).cpu())\n",
    "                all_img_idx.append(idx_img)\n",
    "                \n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "all_gt_label_voc = torch.cat(all_gt_label_voc, dim=0)\n",
    "all_gt_label_clu = torch.cat(all_gt_label_clu, dim=0)\n",
    "all_inst_topk_ind_voc = torch.cat(all_inst_topk_ind_voc, dim=0)\n",
    "all_inst_topk_val_voc = torch.cat(all_inst_topk_val_voc, dim=0)\n",
    "all_inst_max_pred = torch.cat(all_inst_max_pred, dim=0)\n",
    "all_img_idx = torch.cat(all_img_idx, dim=0)\n",
    "\n",
    "# res = torch.load(f'./cache-{args.dataset_name}.pth')\n",
    "# all_clu_pred = res['all_clu_pred']\n",
    "# label_voc_kmeans = res['label_voc_kmeans']\n",
    "# pred_kmeans_t = res['pred_kmeans_t']\n",
    "# cluster_ind_voc = res['cluster_ind_voc']\n",
    "# record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "# all_gt_voc = res['all_gt_voc']\n",
    "# all_label_clu = res['all_label_clu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0c7c3-a560-4944-be35-4984d7286aac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.num_voc = classifier.size(0)\n",
    "\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "history_mapping_assignment_clu = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_inst_topk_ind_voc, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_label_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_label_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    history_mapping_assignment_clu.append(pred_kmeans_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f3f70-fa95-4d12-8a68-db76c60f3f72",
   "metadata": {},
   "source": [
    "##### class-wise feaature space with KNN prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7bb374-07de-4539-8b90-a1c2f1c419f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "c = np.random.choice(all_gt_label_clu.unique().numpy())\n",
    "select = (all_gt_label_clu==c.item())\n",
    "\n",
    "all_class_features = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                features = model.visual(images)\n",
    "                features = features/features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                # if select[idx_img].sum().item()==0:\n",
    "                #     pbar.update(1)\n",
    "                #     continue\n",
    "                all_class_features.append(features.cpu().numpy())\n",
    "                \n",
    "        pbar.update(1)\n",
    "\n",
    "all_class_features = np.concatenate(all_class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6367e97-4145-40a5-b4c1-cac6d43e0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### randomly select a class of features \n",
    "np.random.seed(5)\n",
    "c = np.random.choice(all_gt_label_clu.unique().numpy())\n",
    "select = (all_gt_label_clu==c.item())\n",
    "selected_all_class_features = torch.from_numpy(all_class_features)[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8553e-8c3f-4b8c-811f-d66ab399638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute scd confusing classifier\n",
    "i = torch.arange(select.size(0))[select]\n",
    "for x in history_mapping_assignment_clu[:-1]:\n",
    "    i = x[i]\n",
    "ind, counts = i.unique(return_counts=True)\n",
    "all_confusing_classifier_ind = all_clu_pred[ind[counts.argmax()]].topk(k=3).indices\n",
    "\n",
    "confusing_classifier = classifier[all_confusing_classifier_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299acae-d0bb-4b15-8169-b2b28c2d2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "### distances among confusing classifiers\n",
    "triu_confusing_classifier = (confusing_classifier@confusing_classifier.t()).triu(1)\n",
    "dist_confusing_classifier = triu_confusing_classifier[triu_confusing_classifier.nonzero(as_tuple=True)]\n",
    "print(dist_confusing_classifier.mean(), dist_confusing_classifier.max(), dist_confusing_classifier.min())\n",
    "\n",
    "plt.figure()\n",
    "sns.distplot(dist_confusing_classifier.cpu().numpy(), bins=10)\n",
    "plt.show()\n",
    "\n",
    "### distances among gt classifier and confusing classifiers\n",
    "gt_voc_label = all_gt_label_voc[select].unique()[0].item()\n",
    "classifier[gt_voc_label].view(1, -1)@confusing_classifier.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0c3dd-d4d5-43ed-a22d-2718d9f1ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_voc_label in all_confusing_classifier_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39606dc-cf07-4b8c-996c-70f8f9f85e93",
   "metadata": {},
   "source": [
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c855b4-e5a0-489f-8de1-1b443c5426a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.choice(all_gt_label_clu.unique().numpy())\n",
    "select = (all_gt_label_clu==c.item())\n",
    "selected_all_class_features = all_class_features[select.numpy()]\n",
    "\n",
    "### compute scd confusing classifier\n",
    "i = torch.arange(select.size(0))[select]\n",
    "for x in history_mapping_assignment_clu[:-1]:\n",
    "    i = x[i]\n",
    "ind, counts = i.unique(return_counts=True)\n",
    "all_confusing_classifier_ind = list(set(all_clu_pred[ind[counts.argmax()]].topk(k=5).indices.flatten().numpy()) )\n",
    "                                    # | set(all_inst_topk_ind_voc[select].unique().numpy()))\n",
    "\n",
    "all_confusing_classifier = classifier[torch.tensor(all_confusing_classifier_ind), :].cpu().numpy()\n",
    "all_features_vis = np.concatenate([selected_all_class_features, all_confusing_classifier, classifier[c].view(1, -1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9f2ca-ec79-4687-b78f-4bf020dc5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, \n",
    "            n_iter=1000, \n",
    "            # perplexity=10,\n",
    "            method='exact',\n",
    "           )\n",
    "tr_all_features_vis = tsne.fit_transform(all_features_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f19e5b-a25c-4ba8-b292-f5cd06956856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(dpi=128)\n",
    "plt.scatter(x=tr_all_features_vis[:selected_all_class_features.shape[0], 0], y=tr_all_features_vis[:selected_all_class_features.shape[0], 1], s=5, c='b', alpha=0.6)\n",
    "plt.scatter(x=tr_all_features_vis[selected_all_class_features.shape[0]:-1, 0], y=tr_all_features_vis[selected_all_class_features.shape[0]:-1, 1], s=5, c='g', alpha=0.8)\n",
    "plt.scatter(x=tr_all_features_vis[-1, 0], y=tr_all_features_vis[-1, 1], s=8, c='r', alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fdeec-8a34-4438-b8d8-e49fbe8a6843",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### spatial region visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220fb90-11d5-42c5-aa86-fc28ab55c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordnet_utils import *\n",
    "from PIL import Image\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea03a00-fc6f-43c0-9aef-35e9153ed7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. collect variables\n",
    "upper bound visualization test\n",
    "\"\"\"\n",
    "args.num_voc = classifier.size(0)\n",
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-vit_dino.npy'))\n",
    "all_clu_pred = agg_by_pred_cluster(args, pred_kmeans.numpy(), all_inst_topk_ind_voc, voc_size=args.num_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6ad22-bf9a-42f9-9af6-bb752ac2fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cluster to instance topk\n",
    "cluster_topk_ind = all_clu_pred.topk(k=5).indices\n",
    "inst_topk_ind = cluster_topk_ind[pred_kmeans.long(), ...]\n",
    "### sample one cluster \n",
    "sampled_cluster_idx = torch.randint(low=0, high=all_gt_label_clu.max(), size=[1])\n",
    "inst_select = all_gt_label_clu==sampled_cluster_idx.item()\n",
    "\n",
    "sampled_img_idx = np.random.choice(inst_select.nonzero().flatten().numpy(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb46779-a632-4c67-a04e-9d1d378f9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9\n",
    "model.eval()\n",
    "with amp_autocast():\n",
    "    with torch.no_grad():\n",
    "        image, label_voc, label_clu, _ = dataset_f[sampled_img_idx[idx]]\n",
    "        image = image.to(args.device)\n",
    "        \n",
    "        candidate_voc_labels = inst_topk_ind[sampled_img_idx[idx]]\n",
    "        candidate_voc_synsets = [ mapping_vocidx_to_synsets(x.item(), vocab)[0].name() for x in candidate_voc_labels ]\n",
    "        gt_class_label = mapping_vocidx_to_synsets(label_voc, vocab)[0].name()\n",
    "        \n",
    "        features = model.visual(image.unsqueeze(0), return_spatial=True)\n",
    "        features = F.normalize(features, dim=-1)\n",
    "        \n",
    "        spatial_sim = 100 * features[0, 1:, :]@classifier[candidate_voc_labels].t()\n",
    "        spatial_softmax = spatial_sim.softmax(dim=-1)\n",
    "        spatial_ind = spatial_softmax.argmax(dim=-1).reshape(14, 14)\n",
    "        spatial_score = spatial_softmax.amax(dim=-1).reshape(14, 14)\n",
    "        \n",
    "        spatial_sim_entire = 100 * features[0, 1:, :]@classifier.t()\n",
    "        spatial_softmax_entire = spatial_sim_entire.softmax(dim=-1)\n",
    "        spatial_sim_entire = spatial_sim_entire[:, candidate_voc_labels]\n",
    "        spatial_ind_entire = spatial_softmax_entire.argmax(dim=-1).reshape(14, 14)\n",
    "        spatial_score_entire = spatial_softmax_entire.amax(dim=-1).reshape(14, 14)\n",
    "        \n",
    "        sim_self = 10 * features[0, 0, :]@features[0, 1:, :].t()\n",
    "        sim_self = sim_self.softmax(dim=-1).reshape(14, 14)\n",
    "        \n",
    "        candidate_voc_topk_labels = (100 * features[0, 0, :]@classifier.t()).topk(k=10).indices\n",
    "        candidate_voc_topk_synsets = [ mapping_vocidx_to_synsets(x.item(), vocab)[0].name() for x in candidate_voc_topk_labels ]\n",
    "        \n",
    "        features = model.visual(image.unsqueeze(0), return_spatial=True, with_proj=False)\n",
    "        features = F.normalize(features, dim=-1)\n",
    "        \n",
    "        image, label_voc, label_clu, _ = dataset_r[sampled_img_idx[idx]]\n",
    "        sim_self = 10 * features[0, 0, :]@features[0, 1:, :].t()\n",
    "        sim_self = sim_self.softmax(dim=-1).reshape(14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d16c8-f23a-4ef2-900b-fdb41006bbdc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "spatial_ind, spatial_score, spatial_ind_entire, spatial_score_entire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e305481-005d-4d73-914b-011aa97fa716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint([gt_class_label, candidate_voc_synsets, candidate_voc_topk_synsets], compact=True)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, dpi=128)\n",
    "ax[0,0].imshow(np.array(image.resize([256,256])))\n",
    "ax[0,0].axis(False)\n",
    "ax[0,1].imshow(spatial_score.cpu().numpy())\n",
    "ax[0,1].axis(False)\n",
    "ax[0,2].imshow(spatial_ind.cpu().numpy())\n",
    "ax[0,2].axis(False)\n",
    "ax[0,3].imshow(spatial_score.cpu().numpy()>0.6)\n",
    "ax[0,3].axis(False)\n",
    "\n",
    "ax[1,0].imshow(np.array(image.resize([256,256])))\n",
    "ax[1,0].axis(False)\n",
    "ax[1,1].imshow(spatial_score_entire.cpu().numpy())\n",
    "ax[1,1].axis(False)\n",
    "ax[1,2].imshow(spatial_ind_entire.cpu().numpy()==gt_voc_label)\n",
    "ax[1,2].axis(False)\n",
    "ax[1,3].imshow(spatial_score_entire.cpu().numpy()>0.5)\n",
    "ax[1,3].axis(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(sim_self.cpu().numpy()>sim_self.flatten().topk(k=10).values[-1].item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d0b01-e3dc-4ebd-a3ac-5b18633a9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ mapping_vocidx_to_synsets(x.item(), vocab)[0].definition() for x in candidate_voc_labels ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77a81a-7092-4ea4-b5fe-a436208081b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = (features[0, 1:, :]@features[0, 1:, :].t())[(sim_self.cpu().numpy()>sim_self.flatten().topk(k=10).values[-1].item()).flatten()]\n",
    "for i in range(s.size(0)):\n",
    "    plt.imshow(s[i].reshape(14, 14).cpu().numpy()>s[i].topk(k=20).values[-1].item())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9919f-7380-4aa5-9927-a49607e70ffa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Linguistic Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d4eea-51fe-48e4-a9d1-d6e8177699cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordnet_utils import *\n",
    "from PIL import Image\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fef3e7-1463-4e39-af54-480565eaceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mutex_prompt(args, model, pairs):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        class_embedding: tensor([P x D])\n",
    "    \"\"\"\n",
    "    with open('../templates_small_mutex.json', 'rb') as f:\n",
    "        data = json.load(f)['imagenet']\n",
    "        \n",
    "    all_prompts = []\n",
    "    for p in pairs:\n",
    "        prompts = []\n",
    "        for r in data:\n",
    "            prompts.append(r.format(p[0], p[1]))\n",
    "        all_prompts.append(prompts)\n",
    "    all_prompts = np.array(all_prompts)\n",
    "    n_pairs, n_templates = all_prompts.shape\n",
    "    \n",
    "    # model, preprocess = clip.load(args.arch)\n",
    "    # model.to(args.device).eval()\n",
    "    \n",
    "    texts = tokenize(all_prompts.ravel()).to(args.device) # tokenize\n",
    "    class_embeddings = model.encode_text(texts) # embed with text encoder\n",
    "    class_embeddings = class_embeddings.view(n_pairs, n_templates, -1)\n",
    "    class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "    class_embedding = class_embeddings.mean(dim=1)\n",
    "    class_embedding /= class_embedding.norm(dim=-1, keepdim=True)\n",
    "    return class_embedding, all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ae1b5-a2d6-474e-a02b-ffef04608e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(args.arch)\n",
    "model.to(args.device).eval()\n",
    "\n",
    "res = torch.load(f'./cache-{args.dataset_name}.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23e684-58a2-4d06-a7fc-a017446cb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_topk_pred_voc_ind = all_clu_pred.topk(k=5).indices\n",
    "class_topk_class_names = np.array([ mapping_vocidx_to_synsets(x, vocab)[0].name().split('.')[0] \n",
    "                                   for x in class_topk_pred_voc_ind.flatten().numpy() ]).reshape(-1, 5)\n",
    "class_mutex_templates = []\n",
    "for row in class_topk_class_names:\n",
    "    class_mutex_templates.append([[x, y] for i, x in enumerate(row) for j, y in enumerate(row) if i!=j])\n",
    "\n",
    "idx_class = 12\n",
    "class_embedding_pair, prompts_pair = build_mutex_prompt(args, model, class_mutex_templates[idx_class])\n",
    "classifier = get_classifier(args)\n",
    "class_embedding_single = classifier[class_topk_pred_voc_ind[idx_class]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5df626-13ac-4b93-9390-2f1152d9be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_sim = class_embedding_single@class_embedding_single.t()\n",
    "sim_pair = class_embedding_pair.float() @ class_embedding_pair.float().t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd382f8-5568-4978-b3e8-e6585c1d102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        p1 = [i//4, i%4]\n",
    "        p2 = [j//4, j%4]\n",
    "        if (p1[0]==p2[1]) and (p1[1]==p2[0]) and (p1[0]!=p1[1]) and (p2[0]!=p2[1]):\n",
    "            res.append(sim_pair[i, j].item())\n",
    "print(np.mean(res))\n",
    "print(self_sim.triu(1)[self_sim.triu(1).nonzero(as_tuple=True)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bba420-e9b1-4e25-be6d-c2407d9b5d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa441492-f2e6-46db-8c94-d0b7542634c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b6d8b-9bdd-4034-94a5-16d2fdfa4806",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "dataset_r = get_datasets_oszsl(args, vocab, is_train=True, transform=None, seed=1)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))\n",
    "    \n",
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872be6e4-28f3-422b-b7d0-d90c8f554dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect features and labels\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "all_gt_label_voc = []\n",
    "all_gt_label_clu = []\n",
    "all_img_idx = []\n",
    "all_features = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                all_gt_label_voc.append(label_voc)\n",
    "                all_gt_label_clu.append(label_clu)\n",
    "                all_img_idx.append(idx_img)\n",
    "                all_features.append(logits.cpu().numpy())\n",
    "                \n",
    "        pbar.update(1)\n",
    "\n",
    "all_gt_label_voc = torch.cat(all_gt_label_voc, dim=0)\n",
    "all_gt_label_clu = torch.cat(all_gt_label_clu, dim=0)\n",
    "all_img_idx = torch.cat(all_img_idx, dim=0)\n",
    "all_features = np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189dc601-24ca-4ac9-9136-7e8a5f37dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_store = {\n",
    "    'all_gt_label_voc': all_gt_label_voc.cpu(),\n",
    "    'all_gt_label_clu': all_gt_label_clu.cpu(),\n",
    "    'all_img_idx': all_img_idx.cpu(),\n",
    "    'all_features': all_features,\n",
    "}\n",
    "torch.save(clip_store, f'./cache/clip_store-{args.dataset_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1114234-f9e4-4274-858f-dfb0f62aa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_name = 'make_entity13'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9dbb29-97f8-4457-95de-d8260d896a28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Classifier Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052655e-3e69-464b-afe0-0caa99b82767",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset_name = 'make_nonliving26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ed488-fab0-4b78-ab5c-78c054069bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f)\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)\n",
    "dataset_r = get_datasets_oszsl(args, vocab, is_train=True, transform=None, seed=1)\n",
    "\n",
    "if subset == 'train':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-{arch}.npy'))\n",
    "elif subset == 'val':\n",
    "    pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-val-{arch}.npy'))\n",
    "    \n",
    "model, preprocess = clip.load(args.arch)\n",
    "if args.clip_checkpoint:\n",
    "    model.load_state_dict({k[len('model.'):]:v for k, v in torch.load(args.clip_checkpoint, map_location='cpu')['model'].items()}, strict=False)\n",
    "model.to(args.device).eval()\n",
    "\n",
    "classifier = get_classifier(args)\n",
    "classifier = F.normalize(classifier.float(), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769921b6-b0ae-497d-a178-33c612569d78",
   "metadata": {},
   "source": [
    "collect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae32f8-a5ec-4687-8423-296b8ef95c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_store = torch.load(f'./cache/clip_store-{args.dataset_name}.pth')\n",
    "all_gt_label_voc = clip_store['all_gt_label_voc']\n",
    "all_gt_label_clu = clip_store['all_gt_label_clu']\n",
    "all_img_idx = clip_store['all_img_idx']\n",
    "all_features = clip_store['all_features']\n",
    "\n",
    "\n",
    "### collect features and labels\n",
    "all_knn_classifier_ind = []\n",
    "all_knn_classifier_val = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        _, label_voc, label_clu, idx_img = batch\n",
    "        batch_features = torch.from_numpy(all_features[idx_img.numpy()])\n",
    "        batch_features = F.normalize(batch_features.float(), dim=-1).to(args.device)\n",
    "        sim = batch_features@classifier.t()\n",
    "        sim_topk = sim.topk(k=5)\n",
    "        all_knn_classifier_ind.append(sim_topk.indices.cpu())\n",
    "        all_knn_classifier_val.append(sim_topk.values.cpu())\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "all_knn_classifier_ind = torch.cat(all_knn_classifier_ind, dim=0)\n",
    "all_knn_classifier_val = torch.cat(all_knn_classifier_val, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbaced4-5d9f-41c0-bdfe-a505669394a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*all_knn_classifier_val).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2d647-d200-41f7-a1d6-3f58bf3e8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(all_knn_classifier_ind, f'./cache/clip_img_knn-{args.dataset_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487d13c-0071-428a-9a48-55d903bed11e",
   "metadata": {},
   "source": [
    "#### Classifier study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fdb79-2d76-4965-b730-92335270fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_knn_classifier_ind = torch.load(f'./cache/clip_img_knn-{args.dataset_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cca80d-b55c-472c-a607-bbbaaba2eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_knn_classifier_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b1aac-aa7c-4a7e-b635-d168a4ec5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind, val = all_knn_classifier_ind.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a4c1c-c23c-4c0e-9275-d00a3a567bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load SCD results\n",
    "res = torch.load(f'./cache-{args.dataset_name}.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24460e4a-1b0b-444a-8351-549de3080b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-clip.npy'))\n",
    "args.num_voc = classifier.size(0)\n",
    "all_clu_pred = agg_by_pred_cluster(args, pred_kmeans.numpy(), all_knn_classifier_ind, voc_size=args.num_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee0f9c-f1d5-4e7f-bba5-f2f8e1d7edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pred_kmeans.size(0)\n",
    "K = all_knn_classifier_ind.size(1)\n",
    "instance_assigned_pred = torch.zeros([N, K]).long()\n",
    "for c in pred_kmeans.unique():\n",
    "    select = (pred_kmeans==c)\n",
    "    instance_assigned_pred[select] = all_clu_pred[c].topk(k=5).indices\n",
    "\n",
    "print('acc', (instance_assigned_pred[:, 0]==all_gt_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f15c4a-bddb-44ce-97d4-b0215a71b063",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_select_idx = 2\n",
    "class_select = (pred_kmeans==class_select_idx)\n",
    "study_candidates = all_clu_pred.topk(k=3).indices[class_select_idx]\n",
    "ind, val = all_gt_voc[class_select].unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b9656-c95c-433d-8066-8da67604f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordnet_utils import *\n",
    "\n",
    "print([mapping_vocidx_to_synsets(c.item(), vocab)[0].name().split('.')[0] for c in study_candidates])\n",
    "print([mapping_vocidx_to_synsets(c.item(), vocab)[0].definition() for c in study_candidates])\n",
    "\n",
    "# descriptions = \\\n",
    "# [\n",
    "#     \"A photo of an aircraft_carrier. It has a long, flat deck, with a large superstructure at the back, and a tall tower at the front. It is usually painted grey, and has multiple aircrafts parked on the deck.\",\n",
    "#     \"A photo of a parking_meter. A metal box with a coin slot, a digital display, and a lever or button to activate the timer.\",\n",
    "#     \"A photo of an ambulance. A vehicle typically characterized by a bright red or orange color, a siren, and a medical cross symbol.\",\n",
    "# ]\n",
    "descriptions = \\\n",
    "[\n",
    "    \"A photo of a Winnebago. The Winnebago language is characterized by a distinct set of phonemes and a unique set of grammatical structures.\",\n",
    "    \"A photo of an ambulance. A vehicle typically characterized by a bright red or orange color, a siren, and a medical cross symbol.\",\n",
    "    \"A photo of police_van. A large, box-shaped vehicle with a distinctive black and white paint job and a barred window in the back.\"\n",
    "]\n",
    "descriptions = \\\n",
    "[\n",
    "    \"A photo of a gobiesox. Small, slender fish with a laterally compressed body and two separate dorsal fins.\",\n",
    "    \"A photo of a sock. A foot covering that is typically made of cloth, reaching from the ankle to the knee.\",\n",
    "    \"A photo of a athletic_sock. A sock typically made of a lightweight, breathable material with a reinforced heel and toe for added durability.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e07785-bb8e-46b9-948a-b133e44d3413",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "study_classifier = tokenize(descriptions, truncate=True).to(args.device)\n",
    "study_classifier = model.encode_text(study_classifier)\n",
    "study_classifier = study_classifier/study_classifier.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8242b1-613c-43d9-853d-1bd90c56d530",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_store = torch.load(f'./cache/clip_store-{args.dataset_name}.pth')\n",
    "all_gt_label_voc = clip_store['all_gt_label_voc']\n",
    "all_gt_label_clu = clip_store['all_gt_label_clu']\n",
    "all_img_idx = clip_store['all_img_idx']\n",
    "all_features = clip_store['all_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96014516-c647-4578-a043-fd6412c4c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_features = torch.from_numpy(all_features[class_select]).to(args.device)\n",
    "class_label = all_gt_label_voc[class_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf12165-30f2-40fe-949c-e0eeec2b815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind, val = class_label.unique(return_counts=True)\n",
    "mapping_vocidx_to_synsets(ind[val.argmax(dim=-1)].item(), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58536a4-8e31-476a-94c7-094da5d3e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(study_candidates[(class_features.float() @ study_classifier.float().t()).argmax(dim=-1)]==class_label).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062ab4d-c06a-42ba-88ba-af8376ee81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(class_label==study_candidates[2]).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e81c1-70f6-48cd-9d98-04b8b9da769f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a0e6b-bf4b-495e-998b-33284b693507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordnet_utils import *\n",
    "\n",
    "def compute_knn_batch(tensor, k=5, exclude_self=False, batch_size=1024, device='cpu'):\n",
    "    n_batch = int(np.ceil(tensor.size(0)/batch_size))\n",
    "    all_topk_ind = []\n",
    "    all_topk_val = []\n",
    "    for b in range(n_batch):\n",
    "        start = b*batch_size\n",
    "        end = min((b+1)*batch_size, tensor.size(0))\n",
    "        batch_tensor = tensor[start:end, :].to(device)\n",
    "        batch_sim = batch_tensor@tensor.t()\n",
    "        batch_sim_topk = batch_sim.topk(k=k)\n",
    "        if exclude_self:\n",
    "            all_topk_ind.append(batch_sim_topk.indices[:, 1:k+1].cpu())\n",
    "            all_topk_val.append(batch_sim_topk.values[:, 1:k+1].cpu())\n",
    "        else:\n",
    "            all_topk_ind.append(batch_sim_topk.indices[:, :k].cpu())\n",
    "            all_topk_val.append(batch_sim_topk.values[:, :k].cpu())\n",
    "    all_topk_ind = torch.cat(all_topk_ind, dim=0)\n",
    "    all_topk_val = torch.cat(all_topk_val, dim=0)\n",
    "    return all_topk_ind, all_topk_val\n",
    "\n",
    "\n",
    "def compute_similarity_with_augmented_classifier(features, candidate_names, \n",
    "                                                 class_name_key_mapping, all_augmented_classifier, \n",
    "                                                 method='ensemble', agg_func=None, return_type='max', **kwargs):\n",
    "    \"\"\" only support single instance \n",
    "    Args:\n",
    "        features: tensor([D])\n",
    "        candidate_names: list([])\n",
    "        class_name_key_mapping: {`class_name`: [`synsets`]}\n",
    "        all_augmented_classifier: {`synset`: tensor([M x D])}\n",
    "    \"\"\"\n",
    "    res_similarity = {}\n",
    "    for c in candidate_names:\n",
    "        res_similarity.setdefault(c, [])\n",
    "        synsets = class_name_key_mapping[c]\n",
    "        for synset in synsets:\n",
    "            if method == 'ensemble':\n",
    "                single_ensembled_classifier = all_augmented_classifier[synset].to(features.device).float().mean(dim=0)\n",
    "                sim = 100 * features.view(1, -1) @ single_ensembled_classifier.view(1, -1).t()\n",
    "                res_similarity[c].append(sim.item())\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "    if agg_func is not None:\n",
    "        for k, v in res_similarity.items():\n",
    "            res_similarity[k] = agg_func(v)\n",
    "        if return_type=='max':\n",
    "            max_k = max(res_similarity, key=lambda x: res_similarity[x])\n",
    "            return max_k\n",
    "        elif return_type=='topk':\n",
    "            top_k = heapq.nlargest(kwargs['k'], res_similarity, key=res_similarity.get)\n",
    "            return top_k\n",
    "    return res_similarity\n",
    "    \n",
    "    \n",
    "def agg_by_pred_cluster(args, pred_kmeans, all_topk_voc, voc_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_kmeans: np.array([N])\n",
    "        all_topk_voc: np.array([N x K])\n",
    "        voc_size: int\n",
    "    Returns:\n",
    "        all_clu_pred: tensor([C x V])\n",
    "    \"\"\"\n",
    "    print('agg_by_pred_cluster')\n",
    "    all_clu_pred = []\n",
    "    n_count = []\n",
    "    for i in np.unique(pred_kmeans):\n",
    "        selected = (pred_kmeans==i)\n",
    "        n_count.append( selected.sum().item() )\n",
    "        counter_voc_ind, counter_val = np.unique((all_topk_voc[selected]).ravel(), return_counts=True)\n",
    "        # counter_val = counter_val/(n_count+1e-20) # L1 norm\n",
    "        clu_pred = torch.zeros(args.num_voc) # cluster-wise prob\n",
    "        clu_pred[torch.from_numpy(counter_voc_ind).long()] = torch.from_numpy(counter_val).float()\n",
    "        # clu_pred = F.normalize(all_topk_voc[selected].sum(dim=0), dim=-1, p=1)\n",
    "        all_clu_pred.append(clu_pred)\n",
    "    all_clu_pred = torch.stack(all_clu_pred, dim=0).cpu()\n",
    "    n_count = torch.tensor(n_count).cpu()\n",
    "    \n",
    "    # all_clu_pred = setdiff_assignment(all_clu_pred)\n",
    "    \n",
    "    all_clu_pred = all_clu_pred/(n_count.view(-1, 1) + 1e-20)\n",
    "    \n",
    "    print('is mutex assignment::', all_clu_pred.argmax(dim=-1).size(0)==all_clu_pred.argmax(dim=-1).unique().size(0))\n",
    "    print('assignment collision num::', len(list(filter(lambda x: x>1, Counter(all_clu_pred.argmax(dim=-1).numpy()).values()))))\n",
    "    return all_clu_pred\n",
    "\n",
    "def linear_assign(all_clu_pred, pred_kmeans, all_gt_voc):\n",
    "    print('linear_assign')\n",
    "    cost_mat = all_clu_pred.cpu().numpy()\n",
    "    print(f'assignment shape={cost_mat.shape}')\n",
    "    res_ass = linear_assignment(cost_mat.max() - cost_mat)\n",
    "    label_voc_kmeans = torch.tensor([res_ass[1][x.item()] for x in pred_kmeans])\n",
    "    print('instance label acc::', (label_voc_kmeans==all_gt_voc).float().mean().item())\n",
    "    return label_voc_kmeans, res_ass\n",
    "\n",
    "def reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, device, all_prob=None, \n",
    "                             instance_selected=None, \n",
    "                             classifier_selected=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        classifier_selected: tensor([C2])\n",
    "    \"\"\"\n",
    "    print('reassign_by_pred_cluster')\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    label_voc_kmeans = label_voc_kmeans.to(device)\n",
    "    if all_prob is None:\n",
    "        cluster_ind = []\n",
    "        with tqdm(total=len(loader_f)) as pbar:\n",
    "            if hasattr(model, 'eval'):\n",
    "                model.eval()\n",
    "            for idx_batch, batch in enumerate(loader_f):\n",
    "                images, label_voc, label_clu, idx_img = batch[:4]\n",
    "                images = images.to(device)\n",
    "                if (instance_selected is not None) and ((~instance_selected[idx_img]).all()):\n",
    "                    continue\n",
    "                with amp_autocast():\n",
    "                    with torch.no_grad():\n",
    "                        if (instance_selected is not None):\n",
    "                            logits = model.visual(images[instance_selected[idx_img]])\n",
    "                        else:\n",
    "                            logits = model.visual(images)\n",
    "                            \n",
    "                        logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                        \n",
    "                        if classifier_selected is not None:\n",
    "                            similarity = 100 * logits @ classifier[classifier_selected].t()\n",
    "                            prob = classifier_selected[similarity.softmax(-1)]\n",
    "                            cluster_ind.append(prob.cpu().argmax(dim=-1))\n",
    "                        else:\n",
    "                            similarity = 100 * logits @ classifier.t()\n",
    "                            prob = similarity.softmax(-1)\n",
    "                            cluster_ind.append(prob[:, label_voc_kmeans].cpu().argmax(dim=-1))\n",
    "                pbar.update(1)\n",
    "        cluster_ind = torch.cat(cluster_ind, dim=0)\n",
    "    else:\n",
    "        all_prob = all_prob[:, label_voc_kmeans]\n",
    "        cluster_ind = all_prob.argmax(dim=-1)\n",
    "        \n",
    "    if classifier_selected is not None:\n",
    "        cluster_ind_voc = classifier_selected[cluster_ind]\n",
    "    else:\n",
    "        cluster_ind_voc = label_voc_kmeans[cluster_ind]\n",
    "    mapping_ind = dict(zip(cluster_ind.unique().numpy(), torch.arange(cluster_ind.unique().size(0)).numpy()))\n",
    "    cluster_ind = torch.tensor([mapping_ind[x.item()] for x in cluster_ind])\n",
    "    return cluster_ind, cluster_ind_voc\n",
    "\n",
    "\n",
    "def row_wise_isin(a, b):\n",
    "    n, k = b.size()\n",
    "    results = []\n",
    "    for i in range(1, k+1):\n",
    "        res = torch.zeros_like(a).bool()\n",
    "        for j in range(i):\n",
    "            res = res | (a==b[:, j])\n",
    "        results.append(res)\n",
    "    results = torch.stack(results, dim=1).cpu()\n",
    "    return results\n",
    "\n",
    "import openai\n",
    "def request_gpt(prompt, model_name='text-davinci-003', max_tokens=400, temperature=0.01, best_of=1):\n",
    "    openai.api_key = \"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\"\n",
    "    while 1:\n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "              model=model_name,\n",
    "              prompt=prompt,\n",
    "              temperature=temperature,\n",
    "              max_tokens=max_tokens,\n",
    "              top_p=1,\n",
    "                best_of=best_of,\n",
    "              frequency_penalty=0,\n",
    "              presence_penalty=0,\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f'e={e}')\n",
    "            continue\n",
    "    return response\n",
    "\n",
    "def get_prompt_candidate_discrimination(candidates, attributes='color and shape'):\n",
    "    candidate_string = ''\n",
    "    candidates = list(map(lambda word:\"'\"+word+\"'\", candidates))\n",
    "    candidate_string = ', '.join(candidates[:-1]) + ', and ' + candidates[-1]\n",
    "    ### 42\n",
    "    prompt = f\"Precisely describe discriminative visual features of each word in {candidate_string}. Describe the color and texture. In two bullet points, each uses the template \\\"{'name'}: {'description'}\\\".\"\n",
    "    return prompt\n",
    "\n",
    "def get_prompt_candidate_discrimination_v2(candidates, attributes='color and shape'):\n",
    "    candidate_string = ''\n",
    "    candidates = list(map(lambda word:\"'\"+word+\"'\", candidates))\n",
    "    candidate_string = ', '.join(candidates[:-1]) + ', and ' + candidates[-1]\n",
    "    prompt = f\"Precisely distinguish discriminative visual features (e.g., {attributes}) of each category in {candidate_string}. Each category is elaborated in separate sentence with template \\\"category_name: description\\\". Do not use comparative degree.\"\n",
    "    return prompt\n",
    "\n",
    "def get_prompt_candidate_discrimination_v3(candidates, attributes='color and shape'):\n",
    "    candidate_string = ''\n",
    "    candidates = list(map(lambda word:\"'\"+word+\"'\", candidates))\n",
    "    candidate_string = ', '.join(candidates[:-1]) + ', and ' + candidates[-1]\n",
    "    prompt = f\"There are five categories: {candidate_string}. Closely and Precisely mention all discriminative visual differences between each category and others. Each category is described in a caption with template \\\"category_name: description\\\". \"\n",
    "    return prompt\n",
    "\n",
    "def get_prompt_candidate_discrimination_v4(candidates, attributes='color and shape'):\n",
    "    candidate_string = ''\n",
    "    candidates = list(map(lambda word:\"'\"+word+\"'\", candidates))\n",
    "    candidate_string = ', '.join(candidates[:-1]) + ', and ' + candidates[-1]\n",
    "    ### 44.8\n",
    "    # prompt = f\"Please generate visual descriptions based on the following five category nouns (taken from WordNet): {candidate_string}. For each category, provide a separate sentence that highlights representative visual features that distinguish it from the others. Be precise and concise in your descriptions, using vivid language to help bring each category to life. Please make sure to emphasize the visual differences between the categories, and describe each category in a way that highlights how it differs from the others visually.\"\n",
    "    prompt = f\"Please generate visual descriptions based on the following three category nouns (taken from WordNet): {candidate_string}. For each category, sequentially provide a separate sentence that highlights representative visual features that distinguish it from the others. Be precise and concise in your descriptions, using vivid language to help bring each category to life. Please make sure to emphasize the visual differences between the categories, and describe each category in a way that highlights how it differs from the others. Write in three lines, use template \\\"{'name'}: {'description'}\\\".\"\n",
    "    return prompt\n",
    "\n",
    "def get_prompt_candidate_discrimination_pair(candidates, attributes, index, enable_def=False):\n",
    "    pair = [ candidates[i] for i in index ]\n",
    "    candidate_string = ''\n",
    "    candidate_string = '\\'' + '\\' and \\''.join(pair) + '\\''\n",
    "    prompt = f\"Given two category nouns (taken from WordNet): {candidate_string}. For each category, please generate a separate phrase that highlights representative visual features that distinguish it from the others. Be precise and concise in your descriptions, using vivid language to help bring each category to life. Please make sure to only emphasize the visual differences between the categories, and describe each category in a way that highlights how it differs from the others in appearance. Write in two lines, use template \\\"name: description\\\".\"\n",
    "    if enable_def:\n",
    "        pair = [ candidates[i] for i in index ]\n",
    "        candidate_string = ''\n",
    "        candidate_string = '\\'' + '\\' and \\''.join(pair) + '\\''\n",
    "        prompt = f\"Given two category nouns (taken from WordNet): {candidate_string}. For each category, please generate a separate phrase that highlights representative visual features that distinguish it from the others. Be precise and concise in your descriptions, using vivid language to help bring each category to life. Please make sure to only emphasize the visual differences between the categories, and describe each category in a way that highlights how it differs from the others in appearance. Write in two lines, use template \\\"name: description\\\".\"\n",
    "    return prompt\n",
    "\n",
    "def get_prompt_candidate_discrimination_pair_caption(candidates, attributes, index, enable_def=False):\n",
    "    pair = [ candidates[i] for i in index ]\n",
    "    random.shuffle(pair)\n",
    "    candidate_string = ''\n",
    "    candidate_string = '\\'' + '\\' and \\''.join(pair) + '\\''\n",
    "    prompt = f\"Given two category nouns (taken from WordNet): {candidate_string}. For each category, imagine given one photo, please generate a list of frequently co-occurred related words in image caption dataset in descending order. Write in one line for each category. Only mention the word that the other do not have.\"\n",
    "    return prompt\n",
    "\n",
    "def build_classifier_from_prompt_response(args, model, response):\n",
    "    with open('../templates_small_description.json', 'rb') as f:\n",
    "        templates_small = json.load(f)['imagenet']\n",
    "    all_prompts = []\n",
    "    for r in response:\n",
    "        name, description = r.split(': ')[0].lower(), ': '.join(r.split(': ')[1:])\n",
    "        filled_templates_small = [t.format(name, description) for t in templates_small]\n",
    "        all_prompts.append(filled_templates_small)\n",
    "    all_aug_classifiers = []\n",
    "    for prompt in all_prompts:\n",
    "        aug_classifier = tokenize(prompt, truncate=True).to(args.device)\n",
    "        with torch.no_grad():\n",
    "            aug_classifier = model.encode_text(aug_classifier)\n",
    "        aug_classifier = aug_classifier/aug_classifier.norm(dim=-1, keepdim=True)\n",
    "        aug_classifier = aug_classifier.mean(dim=0) ### ensembling\n",
    "        aug_classifier = aug_classifier/aug_classifier.norm(dim=-1, keepdim=True)\n",
    "        all_aug_classifiers.append(aug_classifier)\n",
    "    all_aug_classifiers = torch.stack(all_aug_classifiers, dim=0).to(args.device)\n",
    "    return all_aug_classifiers\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c3799-6d65-4dc9-906a-a23e0787498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from my_util_package.evaluation import cluster_acc\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "subset = ['train', 'val'][0]\n",
    "modelf = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "arch = 'vit_dino'\n",
    "\n",
    "\"\"\" load dataset \"\"\"\n",
    "transform_f = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "if subset == 'train':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=True, transform=transform_f, seed=1)\n",
    "elif subset == 'val':\n",
    "    dataset_f = get_datasets_oszsl(args, vocab, is_train=False, transform=transform_f, seed=1)\n",
    "args.nb_classes = dataset_f.num_classes\n",
    "loader_f = torch.utils.data.DataLoader(dataset_f, num_workers=8, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0a864-1216-410d-83c3-8442ba58cc8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline Classifier Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e010fc-ca2a-46cb-aca6-24b7fc442b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from MUST \"\"\"\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> torch.LongTensor:\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n",
    "    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n",
    "    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n",
    "    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n",
    "\n",
    "    for i, tokens in enumerate(all_tokens):\n",
    "        if len(tokens) > context_length:\n",
    "            if truncate:\n",
    "                tokens = tokens[:context_length]\n",
    "                tokens[-1] = eot_token\n",
    "            else:\n",
    "                raise RuntimeError(f\"Input {texts[i]} is too long for context length {context_length}\")\n",
    "        result[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921a7ce-ded2-4d02-ae7f-a744f4d548c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load prompts and templates\n",
    "output_fpath = './cache/parsed-wn-gpt3-d-2023_02_26.json'\n",
    "with open(output_fpath, 'rb') as f:\n",
    "    all_parse_results = pickle.load(f)\n",
    "\n",
    "with open('../templates_small.json', 'rb') as f:\n",
    "    templates_small = json.load(f)['imagenet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc89912-bcfc-4ffe-a485-ca87a038a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "### synset and prompts\n",
    "text_inputs = {}\n",
    "for k, v in all_parse_results.items():\n",
    "    text_inputs[k] = [t.format(k.split('.')[0]) + f' {v}' for t in templates_small]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6d0b5-74a7-424a-a8a1-9d980d3b6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### class name to synsets mapping\n",
    "class_name_key_mapping = {}\n",
    "for k in text_inputs:\n",
    "    class_name_key_mapping.setdefault(k.split('.')[0], [])\n",
    "    class_name_key_mapping[k.split('.')[0]].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4de6a-0da0-4545-b34f-8f14815fafce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract prompt embeddings\n",
    "all_augmented_classifier = {}\n",
    "with tqdm(total=len(text_inputs)) as pbar:\n",
    "    for k, v in text_inputs.items():\n",
    "        aug_classifier = tokenize(v, truncate=True).to(args.device)\n",
    "        with torch.no_grad():\n",
    "            aug_classifier = model.encode_text(aug_classifier)\n",
    "        aug_classifier = aug_classifier/aug_classifier.norm(dim=-1, keepdim=True)\n",
    "        all_augmented_classifier[k] = aug_classifier.cpu()\n",
    "        \n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba26ff-f2fb-41c2-aedc-1364b7057d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_classifier = {\n",
    "    'all_augmented_classifier': all_augmented_classifier,\n",
    "    'class_name_key_mapping': class_name_key_mapping,\n",
    "}\n",
    "torch.save(data_augmented_classifier, './cache/all_aug_prompts_embed-wn-gpt3-d-2023_02_26.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61f051-f0e9-4f31-8b0c-8ba7fc2472a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### classifier statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981158e-b75a-43d4-aa6d-4f36fbf625dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load SCD results\n",
    "res = torch.load(f'./cache-{args.dataset_name}.pth')\n",
    "all_clu_pred = res['all_clu_pred']\n",
    "label_voc_kmeans = res['label_voc_kmeans']\n",
    "pred_kmeans_t = res['pred_kmeans_t']\n",
    "cluster_ind_voc = res['cluster_ind_voc']\n",
    "record_pred_kmeans_t = res['record_pred_kmeans_t']\n",
    "all_gt_voc = res['all_gt_voc']\n",
    "all_label_clu = res['all_label_clu']\n",
    "\n",
    "clip_store = torch.load(f'./cache/clip_store-{args.dataset_name}.pth')\n",
    "all_gt_label_voc = clip_store['all_gt_label_voc']\n",
    "all_gt_label_clu = clip_store['all_gt_label_clu']\n",
    "all_img_idx = clip_store['all_img_idx']\n",
    "all_features = clip_store['all_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd164501-06dc-44b1-92e6-c1be86cea9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85009376-e8d3-4797-a7db-3ed4cef66322",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_all_topk_ind, classifier_all_topk_val = compute_knn_batch(classifier.to(args.device), \n",
    "                                                                     k=5, exclude_self=True, batch_size=512, \n",
    "                                                                     device=args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25305a39-9818-4fe0-a9c0-41fa3cda2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_all_topk_val.mean(dim=-1).mean(), classifier_all_topk_val.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ad7bc-e993-4838-a9e1-966fabb9b9a2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in all_gt_label_voc.unique():\n",
    "    select = (all_gt_label_voc==c)\n",
    "    subset_features = torch.from_numpy(all_features[select]).to(args.device)\n",
    "    subset_features = subset_features/subset_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    target_classifier = classifier[c].to(args.device).view(1, -1)\n",
    "    target_classifier = target_classifier/target_classifier.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    sim_intra_img_text = subset_features@target_classifier.t()\n",
    "    print(sim_intra_img_text.mean())\n",
    "    \n",
    "    sim_intra = subset_features@subset_features.t()\n",
    "    mask = torch.ones_like(sim_intra)\n",
    "    mask = torch.scatter(mask, 1, torch.arange(mask.size(0), device=args.device).view(-1, 1), 0).bool()\n",
    "    intra_topk_ind, intra_topk_val = compute_knn_batch(subset_features.to(args.device), \n",
    "                                                       k=5, exclude_self=True, batch_size=512, \n",
    "                                                       device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5f084-6291-4662-a65d-28e17f8eacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_topk_ind, intra_topk_val = compute_knn_batch(torch.from_numpy(all_features).to(args.device), \n",
    "                                                   k=5, exclude_self=True, batch_size=512, \n",
    "                                                   device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44788174-d405-47f9-9a6d-2de1db63abf7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in all_gt_label_voc.unique():\n",
    "    select = (all_gt_label_voc==c)\n",
    "    class_features = torch.from_numpy(all_features)[select].to(args.device)\n",
    "    sim = class_features@class_features.t()\n",
    "    mask = torch.ones_like(sim)\n",
    "    mask = torch.scatter(mask, 1, torch.arange(mask.size(0), device=args.device).view(-1, 1), 0).bool()\n",
    "    print(sim[mask].mean(dim=-1).mean())\n",
    "    # intra_topk_ind, intra_topk_val = compute_knn_batch(torch.from_numpy(all_features)[select].to(args.device), \n",
    "    #                                                    k=5, exclude_self=True, batch_size=512, \n",
    "    #                                                    device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34dfca8-c0a5-4bd7-9cd5-5579c80972ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_topk_val.mean(dim=-1).mean(), intra_topk_val.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49eb0d-8e63-40bc-94d3-18d1677cec09",
   "metadata": {},
   "source": [
    "### Method test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bc0d97-0b29-418b-9bbe-947077fe1c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmented_classifier = torch.load('./cache/all_aug_prompts_embed-wn-gpt3-d-2023_02_26.pth')\n",
    "all_augmented_classifier = data_augmented_classifier['all_augmented_classifier']\n",
    "class_name_key_mapping = data_augmented_classifier['class_name_key_mapping']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c6430-8f89-45cc-8c30-f75caf92b879",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### naive method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710c214-a68f-4c55-81b4-ddb2fd02724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = get_classifier(args)\n",
    "classifier = classifier/classifier.norm(dim=-1, keepdim=True)\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_instance_voc_topk_ind = []\n",
    "all_gt_label_voc = []\n",
    "all_gt_label_clu = []\n",
    "all_features = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_instance_voc_topk_ind.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_label_voc.append(label_voc)\n",
    "                all_gt_label_clu.append(label_clu)\n",
    "                all_features.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_instance_voc_topk_ind = np.concatenate(all_instance_voc_topk_ind)\n",
    "all_gt_label_voc = torch.cat(all_gt_label_voc, dim=0)\n",
    "all_gt_label_clu = torch.cat(all_gt_label_clu, dim=0)\n",
    "all_features = np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01cb0cb-8ea6-430d-ae2b-5d5e68d8c8cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_kmeans = torch.from_numpy(np.load(f'./pred_clu-{args.dataset_name}-train-clip.npy'))\n",
    "# all_clu_pred = agg_by_pred_cluster(args, pred_kmeans.numpy(), all_instance_voc_topk_ind, voc_size=args.num_voc)\n",
    "\n",
    "pred_kmeans_t = pred_kmeans\n",
    "history_set_pred = []\n",
    "for t in range(3):\n",
    "    record_pred_kmeans_t = pred_kmeans_t\n",
    "    all_clu_pred = agg_by_pred_cluster(args, pred_kmeans_t.numpy(), all_instance_voc_topk_ind, voc_size=args.num_voc)\n",
    "    label_voc_kmeans, res_ass = linear_assign(all_clu_pred, pred_kmeans_t, all_gt_label_voc)\n",
    "    pred_kmeans_t, cluster_ind_voc = reassign_by_pred_cluster(label_voc_kmeans, loader_f, model, classifier, args.device, all_prob=None)\n",
    "    set_pred = set(res_ass[1].tolist())\n",
    "    set_gt = set(all_gt_label_voc.unique().numpy().tolist())\n",
    "    print('missing label::', len(set_gt - set_pred))\n",
    "    print('cluster acc', cluster_acc(y_true=all_gt_label_clu.numpy(), y_pred=pred_kmeans_t.numpy()))\n",
    "    history_set_pred.append(set_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6616691-c6ee-4165-a0f4-71b5bed8a9be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for attributes in [\n",
    "    # 'texture and shape',\n",
    "    # 'shape and texture',\n",
    "    # 'color and shape',\n",
    "    # 'shape and color',\n",
    "    # 'components and color',\n",
    "    # 'color and components',\n",
    "    # 'components and texture',\n",
    "    # 'texture and components',\n",
    "    # 'components and shape',\n",
    "    # 'shape and components',\n",
    "    # 'texture and color',\n",
    "    'color and texture',\n",
    "    # 'components, shape, and color',\n",
    "    # 'shape, color, and components',\n",
    "    # 'color, components, and shape',\n",
    "    # 'components, color, and shape',\n",
    "    # 'shape, components, and color',\n",
    "    # 'color, shape, and components',\n",
    "]:\n",
    "    topK = 3\n",
    "    print(attributes)\n",
    "    cluster_topk_voc_ind = all_clu_pred.topk(k=topK).indices.cpu()\n",
    "    class_prediction = []\n",
    "    record_response = []\n",
    "    all_prompt_response = []\n",
    "    all_aug_classifiers = []\n",
    "    with tqdm(total=len(cluster_topk_voc_ind)) as pbar:\n",
    "        for idx, row in enumerate(cluster_topk_voc_ind):\n",
    "            candidates = [mapping_vocidx_to_synsets(x, vocab)[0].name().split('.')[0] for x in row.numpy()[:topK]]\n",
    "            # candidates_def = [mapping_vocidx_to_synsets(x, vocab)[0].name().split('.')[0] for x in row.numpy()[:topK]]\n",
    "            while 1:\n",
    "                try:\n",
    "                    prompt = get_prompt_candidate_discrimination_v4(candidates, attributes)\n",
    "                    response = request_gpt(prompt, model_name='text-davinci-003')\n",
    "                    response = list(filter(lambda x: len(x), response['choices'][0]['text'].lstrip('\\n').split('\\n')))\n",
    "                    # response = record_response[idx]\n",
    "                    aug_classifiers = build_classifier_from_prompt_response(args, model, response) ### K x D\n",
    "                    assert aug_classifiers.size(0)==topK\n",
    "                    all_prompt_response.append(response)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(response)\n",
    "            \n",
    "            subset_features = torch.from_numpy(all_features[record_pred_kmeans_t==idx]).to(args.device)\n",
    "            sim = subset_features.float() @ aug_classifiers.float().t()\n",
    "            ind, count = sim.argmax(dim=-1).unique(return_counts=True)\n",
    "            class_prediction.append(cluster_topk_voc_ind[idx, ind[count.argmax()]].item())\n",
    "            record_response.append(response)\n",
    "            all_aug_classifiers.append(aug_classifiers)\n",
    "            pbar.update(1)\n",
    "    class_prediction = torch.tensor(class_prediction)\n",
    "    all_aug_classifiers = torch.cat(all_aug_classifiers, dim=0)\n",
    "\n",
    "    N = pred_kmeans.size(0)\n",
    "    instance_assigned_pred = torch.zeros(N).long()\n",
    "    for c in record_pred_kmeans_t.unique():\n",
    "        select = (record_pred_kmeans_t==c)\n",
    "        instance_assigned_pred[select] = class_prediction[c]\n",
    "    print('acc', (instance_assigned_pred==all_gt_label_voc).float().mean().item())\n",
    "    print('conflict', len(all_gt_label_voc.unique()) - len(instance_assigned_pred.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c9bbc-b930-4709-ac72-ff316056a493",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "topK = 3\n",
    "cluster_topk_voc_ind = all_clu_pred.topk(k=topK).indices.cpu()\n",
    "class_prediction = []\n",
    "record_response = []\n",
    "all_prompt_response = []\n",
    "all_aug_classifiers = []\n",
    "with tqdm(total=len(cluster_topk_voc_ind)) as pbar:\n",
    "    for idx, row in enumerate(cluster_topk_voc_ind):\n",
    "        ### parse candidate class names\n",
    "        candidates = [mapping_vocidx_to_synsets(x, vocab)[0].name().split('.')[0] for x in row.numpy()[:topK]]\n",
    "        ### get subset features\n",
    "        subset_features = torch.from_numpy(all_features[record_pred_kmeans_t==idx]).to(args.device)\n",
    "        ### candidate index flag\n",
    "        curr_candidate_idx_1 = 0 ### head\n",
    "        curr_candidate_idx_2 = 1 ### tail\n",
    "        ### record\n",
    "        pair_prompts = []\n",
    "        while curr_candidate_idx_2<topK:\n",
    "            ### get pair prompts\n",
    "            prompt = get_prompt_candidate_discrimination_pair_caption(candidates, attributes, \n",
    "                                                              index=[curr_candidate_idx_1, curr_candidate_idx_2],\n",
    "                                                             )\n",
    "            ### record\n",
    "            pred_ind = []\n",
    "            pair_repeat_prompts = []\n",
    "            ### repeat\n",
    "            for _ in range(3):\n",
    "                while 1:\n",
    "                    try:\n",
    "                        response = request_gpt(prompt, model_name='text-davinci-003', max_tokens=200, temperature=0.7, best_of=1)\n",
    "                        response = list(filter(lambda x: len(x), response['choices'][0]['text'].lstrip('\\n').split('\\n')))\n",
    "                        aug_classifiers = build_classifier_from_prompt_response(args, model, response) ### K x D\n",
    "                        ### constraint\n",
    "                        assert aug_classifiers.size(0)==2\n",
    "                        ### record\n",
    "                        pair_repeat_prompts.append(response)\n",
    "                        all_aug_classifiers.append(aug_classifiers)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(response)\n",
    "                sim = subset_features.float() @ aug_classifiers.float().t()\n",
    "                ind, count = sim.argmax(dim=-1).unique(return_counts=True)\n",
    "                pred_ind.append(ind[count.argmax()].item())\n",
    "            ind, count = torch.tensor(pred_ind).unique(return_counts=True)\n",
    "            curr_candidate_idx_1 = ind[count.argmax()] ### winner\n",
    "            curr_candidate_idx_2 = curr_candidate_idx_2 + 1\n",
    "            pair_prompts.append(pair_repeat_prompts)\n",
    "        ### results\n",
    "        class_prediction.append(cluster_topk_voc_ind[idx, curr_candidate_idx_1].item())\n",
    "        all_prompt_response.append(pair_prompts)\n",
    "        pbar.update(1)\n",
    "\n",
    "class_prediction = torch.tensor(class_prediction)\n",
    "all_aug_classifiers = torch.cat(all_aug_classifiers, dim=0)\n",
    "\n",
    "N = pred_kmeans.size(0)\n",
    "instance_assigned_pred = torch.zeros(N).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    instance_assigned_pred[select] = class_prediction[c]\n",
    "print('acc', (instance_assigned_pred==all_gt_label_voc).float().mean().item())\n",
    "print('conflict', len(all_gt_label_voc.unique()) - len(instance_assigned_pred.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b161c6d-5e08-4ff1-bad3-eb181ef223da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save({'all_prompt_response': all_prompt_response}, f'./cache/all_prompt_response-{args.dataset_name}-pair.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae5a98-dce3-4218-9b40-1ec1bf55eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### upperbound performance of cluster-wise assignment\n",
    "N = pred_kmeans.size(0)\n",
    "instance_assigned_pred = torch.zeros(N).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    ind_gt, count_gt = all_gt_label_voc[record_pred_kmeans_t==c].unique(return_counts=True) \n",
    "    instance_assigned_pred[select] = ind_gt[count_gt.argmax()]\n",
    "print('acc', (instance_assigned_pred==all_gt_label_voc).float().mean().item())\n",
    "print('conflict', len(all_gt_label_voc.unique()) - len(instance_assigned_pred.unique()))\n",
    "### class recall performance of SCD topK predictions\n",
    "recall = []\n",
    "all_gtlbl = []\n",
    "for idx in range(len(cluster_topk_voc_ind)):\n",
    "    ind_gt, count_gt = all_gt_label_voc[record_pred_kmeans_t==idx].unique(return_counts=True) \n",
    "    gtlbl = ind_gt[count_gt.argmax()]\n",
    "    recall.append(torch.isin(gtlbl, cluster_topk_voc_ind[idx, :3]).item())\n",
    "    all_gtlbl.append(gtlbl)\n",
    "\n",
    "recall = torch.tensor(recall)\n",
    "all_gtlbl = torch.tensor(all_gtlbl)\n",
    "recall.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9ba57-99c0-4454-b59c-5d7cb72dabc3",
   "metadata": {},
   "source": [
    "entropy partition experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe30e24-2aba-4c6c-bade-cfd76666aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pred_kmeans.size(0)\n",
    "instance_assigned_pred = torch.zeros(N).long()\n",
    "# normalize_sum = lambda x: x/x.sum(dim=-1, keepdim=True)\n",
    "entropy = lambda p, a=1: -((a*p)*((a*p)+1e-20).log()).sum()\n",
    "record_true = []\n",
    "record_false = []\n",
    "cluster_topk_voc_val = (1 * all_clu_pred.topk(k=topK).values.cpu()).softmax(-1)\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    ind_gt, count_gt = all_gt_label_voc[record_pred_kmeans_t==c].unique(return_counts=True) \n",
    "    if (ind_gt[count_gt.argmax()]==cluster_topk_voc_ind[c][0]).item():\n",
    "        record_true.append(cluster_topk_voc_val[c][0])\n",
    "        # record_true.append(entropy(cluster_topk_voc_val[c]))\n",
    "    else:\n",
    "        record_false.append(cluster_topk_voc_val[c][0])\n",
    "        # record_false.append(entropy(cluster_topk_voc_val[c]))\n",
    "    # break\n",
    "    \n",
    "plt.figure()\n",
    "sns.distplot(torch.tensor(record_true).numpy(), bins=100)\n",
    "sns.distplot(torch.tensor(record_false).numpy(), bins=100)\n",
    "plt.legend(['true', 'false'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec8ff0-6354-4893-aec8-48e73bce1e12",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.distplot(torch.tensor(record_true).numpy(), bins=100)\n",
    "sns.distplot(torch.tensor(record_false).numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50233f07-b299-483f-ae58-56348d589103",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(cluster_topk_voc_val[:, 0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7f3e6-a244-4e24-8ca0-204032c7be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./all_prompt_response_v4-{args.dataset_name}.pkl', 'wb') as f:\n",
    "#     pickle.dump(all_prompt_response, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb788e-a3f1-4710-96fa-53b1cf668822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./all_prompt_response_v4-{args.dataset_name}.pkl', 'rb') as f:\n",
    "#     all_prompt_response = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7759d-ee9f-4ebc-b990-77fb0a72a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pred_idx_list = []\n",
    "for idx in range(len(cluster_topk_voc_ind)):\n",
    "    ind_gt, count_gt = all_gt_label_voc[record_pred_kmeans_t==idx].unique(return_counts=True) \n",
    "    correct_pred = (ind_gt[count_gt.argmax()] == class_prediction[idx]).item()\n",
    "    if not correct_pred:\n",
    "        false_pred_idx_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82cfcd9-04e8-4b78-93e5-5843f2f47106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "idx = np.random.choice(false_pred_idx_list)\n",
    "# idx = idx + 1\n",
    "response = all_prompt_response[idx]\n",
    "aug_classifiers = build_classifier_from_prompt_response(args, model, response)\n",
    "subset_features = torch.from_numpy(all_features[record_pred_kmeans_t==idx]).to(args.device)\n",
    "sim = subset_features.float() @ aug_classifiers.float().t()\n",
    "ind, count = sim.argmax(dim=-1).unique(return_counts=True)\n",
    "ind_gt, count_gt = all_gt_label_voc[record_pred_kmeans_t==idx].unique(return_counts=True) \n",
    "\n",
    "print(all_prompt_response[idx])\n",
    "print(f'ind={ind}, count={count}')\n",
    "print(f'cand={cluster_topk_voc_ind[idx]}, prev_pred={class_prediction[idx]}')\n",
    "print(f'pred={cluster_topk_voc_ind[idx,ind[count.argmax()]]}, gt={ind_gt[count_gt.argmax()]}')\n",
    "print('synset=', mapping_vocidx_to_synsets(ind_gt[count_gt.argmax()].item(), vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd72f04-a8a6-437d-8d14-3047a16728f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    'components, shape, and color',\n",
    "    'shape, color, and components',\n",
    "    'color, components, and shape',\n",
    "    'components, color, and shape',\n",
    "    'shape, components, and color',\n",
    "    'color, shape, and components',\n",
    "]\n",
    "print(attributes)\n",
    "cluster_topk_voc_ind = all_clu_pred.topk(k=5).indices.cpu()\n",
    "class_prediction = []\n",
    "record_response = []\n",
    "all_prompt_response = []\n",
    "with tqdm(total=len(cluster_topk_voc_ind)) as pbar:\n",
    "    for idx, row in enumerate(cluster_topk_voc_ind):\n",
    "        candidates = [mapping_vocidx_to_synsets(x, vocab)[0].name().split('.')[0] for x in row.numpy()[:5]]\n",
    "        ensembled_response = []\n",
    "        ensembled_classifier = []\n",
    "        for a in attributes:\n",
    "            prompt = get_prompt_candidate_discrimination(candidates, attributes)\n",
    "            response = request_gpt(prompt, model_name='text-davinci-003')\n",
    "            response = response['choices'][0]['text'].lstrip('\\n\\n').split('\\n\\n')\n",
    "            aug_classifiers = build_classifier_from_prompt_response(args, model, response) ### K x D\n",
    "            ensembled_classifier.append(aug_classifiers)\n",
    "            ensembled_response.append(response)\n",
    "        all_prompt_response.append(ensembled_response)\n",
    "        \n",
    "        ### similarity average ensemble\n",
    "        subset_features = torch.from_numpy(all_features[record_pred_kmeans_t==idx]).float()#.to(args.device).float()\n",
    "        a_c = aug_classifiers.float().t().cpu()\n",
    "        ensembled_sim = []\n",
    "        for aug_classifiers in ensembled_classifier:\n",
    "            sim = 100 * subset_features @ a_c\n",
    "            ensembled_sim.append(sim)\n",
    "        ensembled_sim = torch.stack(ensembled_sim, dim=0).mean(dim=0) ### average\n",
    "        ind, count = ensembled_sim.argmax(dim=-1).unique(return_counts=True)\n",
    "        class_prediction.append(cluster_topk_voc_ind[idx, ind[count.argmax()]].item())\n",
    "        record_response.append(response)\n",
    "        pbar.update(1)\n",
    "class_prediction = torch.tensor(class_prediction)\n",
    "\n",
    "N = pred_kmeans.size(0)\n",
    "instance_assigned_pred = torch.zeros(N).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    instance_assigned_pred[select] = class_prediction[c]\n",
    "print('acc', (instance_assigned_pred==all_gt_label_voc).float().mean().item())\n",
    "print('conflict', len(all_gt_label_voc.unique()) - len(instance_assigned_pred.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d990eb6-1b8d-4e9b-b430-7747915e9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f./cache/request/equest/ensmbled_prompts-{args.dataset_name}.pkl', 'wb') as f:\n",
    "    pickle.dump(all_prompt_response, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d87ae-867b-492a-831a-96278284c467",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_prompt_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44eee86-26fe-4714-a0ca-313dd4b57a1b",
   "metadata": {},
   "source": [
    "#### iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01df0d-958d-4ccd-befd-f057ab1dbe70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6300e4-132b-4f12-a676-a6ea37857ac0",
   "metadata": {},
   "source": [
    "#### misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b089e8-6c33-4ee0-8165-41c2f24c9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pred_kmeans.size(0)\n",
    "instance_assigned_pred = torch.zeros(N).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    instance_assigned_pred[select] = class_prediction[c]\n",
    "print('acc', (instance_assigned_pred==all_gt_label_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402145b0-b7f5-4bfb-b4ec-c2be49f66007",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pred_kmeans.size(0)\n",
    "instance_assigned_pred_scd = torch.zeros(N).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    instance_assigned_pred_scd[select] = all_clu_pred[c].argmax(dim=-1)\n",
    "print('acc', (instance_assigned_pred_scd==all_gt_label_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2654442-2239-42e5-9395-eb2429f9404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fn = (instance_assigned_pred!=all_gt_label_voc) & (instance_assigned_pred_scd==all_gt_label_voc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e9cda-a276-45e1-98c1-b58da5a4272b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('flip rate', (class_prediction == all_clu_pred.argmax(dim=-1)).float().mean())\n",
    "scd_names = np.array([mapping_vocidx_to_synsets(x.item(), vocab)[0].name().split('.')[0] for x in all_clu_pred.argmax(dim=-1)])\n",
    "updated_names = np.array([mapping_vocidx_to_synsets(x.item(), vocab)[0].name().split('.')[0] for x in class_prediction])\n",
    "\n",
    "np.array(record_response)[class_prediction!=all_clu_pred.argmax(dim=-1)].tolist(), \\\n",
    "updated_names[(scd_names!=updated_names)], scd_names[(scd_names!=updated_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd471dc-dcea-488b-9028-3d60d3fc4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('acc', (all_clu_pred.argmax(dim=-1)==all_gt_label_voc).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8be80-212f-4494-ad71-e1a64827b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = pred_kmeans.size(0)\n",
    "K = prob_k\n",
    "instance_assigned_pred = torch.zeros([N, K]).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    instance_assigned_pred[select] = all_clu_pred[c].topk(k=5).indices\n",
    "\n",
    "print('acc', (instance_assigned_pred[:, 0]==all_gt_label_voc).float().mean().item())\n",
    "print('acc instance topk', (torch.from_numpy(all_instance_voc_topk_ind)[:, 0]==all_gt_label_voc).float().mean().item())\n",
    "retrieved_labels = instance_assigned_pred[instance_assigned_pred[:, 0]==all_gt_label_voc][:, 0].unique().numpy()\n",
    "pred_labels = instance_assigned_pred[:, 0].unique().numpy()\n",
    "gt_labels = all_gt_label_voc.unique().numpy()\n",
    "print(f'missing label of retrieval:: {len(set(gt_labels) - set(retrieved_labels))}')\n",
    "print(f'missing label of predict:: {len(set(gt_labels) - set(pred_labels))}')\n",
    "for k in range(1, K):\n",
    "    retrieved_labels_topk = instance_assigned_pred[instance_assigned_pred[:, 0]==all_gt_label_voc][:, :k].flatten().unique().numpy()\n",
    "    print(f'missing label of retieval at k={k}:: {len(set(gt_labels) - set(retrieved_labels_topk))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64614ea-8231-4ca1-aaf3-04cae70f776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "isin_instance_topk = row_wise_isin(all_gt_label_voc, torch.from_numpy(all_instance_voc_topk_ind))\n",
    "\n",
    "isin_instance_topk.float().mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37ffd9-10cf-4b87-b9f7-89cb3471951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "isin_instance_topk = row_wise_isin(all_gt_label_voc, instance_assigned_pred)\n",
    "isin_instance_topk.float().mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb3425-be2f-497f-8647-69bb95942c93",
   "metadata": {},
   "source": [
    "instance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae00b29-38ba-446f-aefe-532782fbb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instance_voc_topk_ind\n",
    "\n",
    "candidate_names = \n",
    "compute_similarity_with_augmented_classifier(features, candidate_names, \n",
    "                                             class_name_key_mapping, all_augmented_classifier, \n",
    "                                             method='ensemble', agg_func=max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f6783-ef4e-45ed-b623-c27992cc4d84",
   "metadata": {},
   "source": [
    "cluster based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c8500-d3c7-4a61-a87e-9332efcc9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = record_pred_kmeans_t.size(0)\n",
    "K = prob_k\n",
    "instance_assigned_pred = torch.zeros([N, K]).long()\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    select = (record_pred_kmeans_t==c)\n",
    "    instance_assigned_pred[select] = all_clu_pred[c].topk(k=10).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b380c5-a65a-4f05-9983-5195b6e9a1fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "all_sample_ind = []\n",
    "with tqdm(total=all_features.shape[0]) as pbar:\n",
    "    for i in range(all_features.shape[0]):\n",
    "        img_feature = torch.from_numpy(all_features[i])#.to(args.device)\n",
    "        candidate_names = [mapping_vocidx_to_synsets(x.item(), vocab)[0].name().split('.')[0] \n",
    "                           for x in instance_assigned_pred[i, :3]]\n",
    "        max_k = \\\n",
    "            compute_similarity_with_augmented_classifier(img_feature, candidate_names, \n",
    "                                                         class_name_key_mapping, all_augmented_classifier, \n",
    "                                                         method='ensemble', agg_func=np.mean, return_indices=True)\n",
    "        idx_max = candidate_names.index(max_k)\n",
    "        \n",
    "        all_sample_ind.append(idx_max)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "all_sample_ind = torch.tensor(all_sample_ind)\n",
    "baseline_instance_pred = instance_assigned_pred.gather(1, all_sample_ind.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c3f47-69b1-4e45-99ec-e5053c6233bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(baseline_instance_pred.flatten() == all_gt_label_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7c61a-0dc3-4e57-bd73-131e028a32ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(instance_assigned_pred[:, 0] == all_gt_label_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe399f5-e538-4d0a-9b5a-d9e2b3028338",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_names, max_k, mapping_vocidx_to_synsets(all_gt_label_voc[i].item(), vocab), \\\n",
    "baseline_instance_pred[-1], all_gt_label_voc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92846901-17b1-4da1-9f38-c35ec002db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_instance_pred = baseline_instance_pred.flatten()\n",
    "baseline_instance_pred_clu = torch.zeros_like(baseline_instance_pred)\n",
    "for c in record_pred_kmeans_t.unique():\n",
    "    val, count = baseline_instance_pred[record_pred_kmeans_t==c].unique(return_counts=True)\n",
    "    baseline_instance_pred_clu[record_pred_kmeans_t==c] = val[count.argmax(dim=-1)].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ceb35-6d5d-4d5e-9875-2808b74894ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(baseline_instance_pred_clu == all_gt_label_voc).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa10391-ea9d-40f1-acf0-f47e1fdf56ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### reranked KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39be754-2a0d-42d5-a8ed-81a642a88350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute top-20 predictions \"\"\"\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_instance_voc_topk_ind = []\n",
    "all_gt_label_voc = []\n",
    "all_gt_label_clu = []\n",
    "all_features = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_instance_voc_topk_ind.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_label_voc.append(label_voc)\n",
    "                all_gt_label_clu.append(label_clu)\n",
    "                all_features.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_instance_voc_topk_ind = np.concatenate(all_instance_voc_topk_ind)\n",
    "all_gt_label_voc = torch.cat(all_gt_label_voc, dim=0)\n",
    "all_gt_label_clu = torch.cat(all_gt_label_clu, dim=0)\n",
    "all_features = np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca48837-88e5-439a-b4a0-e4c693a127a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "N = all_instance_voc_topk_ind.shape[0]\n",
    "all_instance_voc_topk_ind_rerank = torch.zeros(N, K).long()\n",
    "with tqdm(total=N) as pbar:\n",
    "    for i in range(all_features.shape[0]):\n",
    "        feature = torch.from_numpy(all_features[i])\n",
    "        candidate_names = [ mapping_vocidx_to_synsets(x.item(), vocab)[0].name().split('.')[0] for x in all_instance_voc_topk_ind[i, :3] ]\n",
    "        topk_candidate_voc_ind = \\\n",
    "        compute_similarity_with_augmented_classifier(feature, candidate_names, \n",
    "                                                     class_name_key_mapping, all_augmented_classifier, \n",
    "                                                     method='ensemble', agg_func=max, return_type='topk', k=K)\n",
    "        all_instance_voc_topk_ind_rerank[i] = torch.tensor([ vocab.mapping_names_idx[x] for x in topk_candidate_voc_ind ])\n",
    "        pbar.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6049d-fdb0-495f-8b23-24720c278dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.permutation(N)[0]\n",
    "feature = torch.from_numpy(all_features[i])\n",
    "candidate_names = [ mapping_vocidx_to_synsets(x.item(), vocab)[0].name().split('.')[0] for x in all_instance_voc_topk_ind[i] ]\n",
    "topk_candidate_voc_ind = \\\n",
    "compute_similarity_with_augmented_classifier(feature, candidate_names, \n",
    "                                             class_name_key_mapping, all_augmented_classifier, \n",
    "                                             method='ensemble', agg_func=max, return_type='topk', k=5)\n",
    "all_instance_voc_topk_ind_rerank[i] = torch.tensor([ vocab.mapping_names_idx[x] for x in topk_candidate_voc_ind ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac0558-2e4a-42d9-8ff6-2f0066ff7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_instance_voc_topk_ind_rerank[:, 0]==all_gt_label_voc).float().mean(), \\\n",
    "(torch.from_numpy(all_instance_voc_topk_ind)[:, 0]==all_gt_label_voc).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970564f-36dd-4efa-9c57-d91e09681d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, d = compute_similarity_with_augmented_classifier(torch.rand(512), ['cat', 'dog', 'frog', 'shirt', 'man', 'swarm', 'liquid'], \n",
    "                                                 class_name_key_mapping, all_augmented_classifier, \n",
    "                                                 method='ensemble', agg_func=max, return_type='topk', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3821e-c7b7-4764-8a61-a40698ed997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fpath = './cache/parsed-wn-gpt3-d-2023_02_26.json'\n",
    "with open(output_fpath, 'rb') as f:\n",
    "    all_parse_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a2b64-3e17-4025-9542-46213959d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_vocidx_to_synsets(all_gt_label_voc[i].item(), vocab)[0].name(), \\\n",
    "topk_candidate_voc_ind, \\\n",
    "[mapping_vocidx_to_synsets(x, vocab)[0].name() for x in all_instance_voc_topk_ind[i][:5]], \\\n",
    "[ all_parse_results[mapping_vocidx_to_synsets(x, vocab)[0].name()] for x in all_instance_voc_topk_ind[i][:5] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fb4f2-518b-461d-b14b-784b2354fb96",
   "metadata": {},
   "source": [
    "#### basic observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196adeb7-7e2d-4061-927f-0fad8c484f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" compute top-20 predictions \"\"\"\n",
    "args.num_voc = classifier.size(0)\n",
    "amp_autocast = torch.cuda.amp.autocast\n",
    "### collect variables\n",
    "prob_k = 5\n",
    "all_instance_voc_topk_ind = []\n",
    "all_gt_label_voc = []\n",
    "all_gt_label_clu = []\n",
    "all_features = []\n",
    "with tqdm(total=len(loader_f)) as pbar:\n",
    "    if hasattr(model, 'eval'):\n",
    "        model.eval()\n",
    "    for idx_batch, batch in enumerate(loader_f):\n",
    "        images, label_voc, label_clu, idx_img = batch[:4]\n",
    "        images = images.to(args.device)\n",
    "        with amp_autocast():\n",
    "            with torch.no_grad():\n",
    "                logits = model.visual(images)\n",
    "                logits = logits/logits.norm(dim=-1, keepdim=True)\n",
    "                similarity = 100 * logits @ classifier.t()\n",
    "                prob = similarity.softmax(-1)\n",
    "                prob_topk_ind = prob.topk(k=prob_k, dim=-1).indices\n",
    "                all_instance_voc_topk_ind.append(prob_topk_ind.cpu().numpy())\n",
    "                all_gt_label_voc.append(label_voc)\n",
    "                all_gt_label_clu.append(label_clu)\n",
    "                all_features.append(logits.cpu().numpy())\n",
    "        pbar.update(1)\n",
    "\n",
    "all_instance_voc_topk_ind = np.concatenate(all_instance_voc_topk_ind)\n",
    "all_gt_label_voc = torch.cat(all_gt_label_voc, dim=0)\n",
    "all_gt_label_clu = torch.cat(all_gt_label_clu, dim=0)\n",
    "all_features = np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ffe8b-8734-4424-8fdb-8409a242f7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea3ba74-5b6e-4a46-9b4d-6fef55fe7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('/home/sheng/dataset/LAION/part-00000-5b54c5d5-bbcf-484d-a2ce-0d6f73df1a36-c000.snappy.parquet', engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcd",
   "language": "python",
   "name": "gcd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
